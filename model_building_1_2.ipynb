{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DoMyyZHXMVQ6"
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bl08pfOPMadW"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/train.csv')\n",
    "test = pd.read_csv('/content/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rMR1Rz6SMbFG",
    "outputId": "cee43e49-9a07-4feb-b51a-cba1fab9e3f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The train data size after dropping Id feature is : (1460, 80) \n",
      "The test data size after dropping Id feature is : (1459, 79) \n"
     ]
    }
   ],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "MmG6SEyTMdgm",
    "outputId": "26757dd0-2c31-4270-840d-f0555cdc898a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data size is : (2919, 79)\n",
      "Shape all_data: (2919, 78)\n",
      "\n",
      "Skew in numerical features: \n",
      "\n",
      "There are 59 skewed numerical features to Box Cox transform\n",
      "(2919, 221)\n"
     ]
    }
   ],
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "target = train[\"SalePrice\"]\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))\n",
    "\n",
    "# The Data cleaning process, can be find in the Data Cleaning File\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))       \n",
    "print('Shape all_data: {}'.format(all_data.shape))\n",
    "\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "  all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jNbeXp48M0o2",
    "outputId": "92bc1e7f-3603-4012-f6a9-d793139cfd71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 221)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "WxxYXP76M58x",
    "outputId": "930ec546-d8c5-44e7-dfa8-a2b9c8aec89c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Build the NN network\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rEyWRUFlOcXc"
   },
   "outputs": [],
   "source": [
    "# set a check point \n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jl9jTWFLOhRy",
    "outputId": "85c036fe-0c67-4903-f141-c48a6f5e6c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1168/1168 [==============================] - 5s 4ms/step - loss: 2.5317 - mean_absolute_error: 2.5317 - val_loss: 0.5674 - val_mean_absolute_error: 0.5674\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56739, saving model to Weights-001--0.56739.hdf5\n",
      "Epoch 2/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.3153 - mean_absolute_error: 0.3153 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56739 to 0.24882, saving model to Weights-002--0.24882.hdf5\n",
      "Epoch 3/500\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 0.4452 - mean_absolute_error: 0.4452 - val_loss: 0.6268 - val_mean_absolute_error: 0.6268\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.24882\n",
      "Epoch 4/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.3342 - mean_absolute_error: 0.3342 - val_loss: 0.3401 - val_mean_absolute_error: 0.3401\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24882\n",
      "Epoch 5/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.2100 - mean_absolute_error: 0.2100 - val_loss: 0.4440 - val_mean_absolute_error: 0.4440\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24882\n",
      "Epoch 6/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.2280 - mean_absolute_error: 0.2280 - val_loss: 0.2385 - val_mean_absolute_error: 0.2385\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24882 to 0.23846, saving model to Weights-006--0.23846.hdf5\n",
      "Epoch 7/500\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.3684 - mean_absolute_error: 0.3684 - val_loss: 0.2924 - val_mean_absolute_error: 0.2924\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23846\n",
      "Epoch 8/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.2472 - mean_absolute_error: 0.2472 - val_loss: 0.1988 - val_mean_absolute_error: 0.1988\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23846 to 0.19876, saving model to Weights-008--0.19876.hdf5\n",
      "Epoch 9/500\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.1938 - mean_absolute_error: 0.1938 - val_loss: 0.1499 - val_mean_absolute_error: 0.1499\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19876 to 0.14995, saving model to Weights-009--0.14995.hdf5\n",
      "Epoch 10/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.1630 - mean_absolute_error: 0.1630 - val_loss: 0.1692 - val_mean_absolute_error: 0.1692\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14995\n",
      "Epoch 11/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.2017 - mean_absolute_error: 0.2017 - val_loss: 0.2665 - val_mean_absolute_error: 0.2665\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14995\n",
      "Epoch 12/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.1960 - mean_absolute_error: 0.1960 - val_loss: 0.1771 - val_mean_absolute_error: 0.1771\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14995\n",
      "Epoch 13/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.1612 - mean_absolute_error: 0.1612 - val_loss: 0.3044 - val_mean_absolute_error: 0.3044\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14995\n",
      "Epoch 14/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.1657 - mean_absolute_error: 0.1657 - val_loss: 0.1514 - val_mean_absolute_error: 0.1514\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14995\n",
      "Epoch 15/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.1665 - mean_absolute_error: 0.1665 - val_loss: 0.1707 - val_mean_absolute_error: 0.1707\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14995\n",
      "Epoch 16/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.1679 - mean_absolute_error: 0.1679 - val_loss: 0.3178 - val_mean_absolute_error: 0.3178\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14995\n",
      "Epoch 17/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.2519 - mean_absolute_error: 0.2519 - val_loss: 0.3613 - val_mean_absolute_error: 0.3613\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14995\n",
      "Epoch 18/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.4533 - mean_absolute_error: 0.4533 - val_loss: 0.4743 - val_mean_absolute_error: 0.4743\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14995\n",
      "Epoch 19/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.2378 - mean_absolute_error: 0.2378 - val_loss: 0.4769 - val_mean_absolute_error: 0.4769\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14995\n",
      "Epoch 20/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.3985 - mean_absolute_error: 0.3985 - val_loss: 0.3209 - val_mean_absolute_error: 0.3209\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.14995\n",
      "Epoch 21/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.3587 - mean_absolute_error: 0.3587 - val_loss: 0.5394 - val_mean_absolute_error: 0.5394\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14995\n",
      "Epoch 22/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.2991 - mean_absolute_error: 0.2991 - val_loss: 0.1624 - val_mean_absolute_error: 0.1624\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.14995\n",
      "Epoch 23/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.1800 - mean_absolute_error: 0.1800 - val_loss: 0.3038 - val_mean_absolute_error: 0.3038\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14995\n",
      "Epoch 24/500\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 0.2899 - mean_absolute_error: 0.2899 - val_loss: 0.2286 - val_mean_absolute_error: 0.2286\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.14995\n",
      "Epoch 25/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.2219 - mean_absolute_error: 0.2219 - val_loss: 0.1620 - val_mean_absolute_error: 0.1620\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.14995\n",
      "Epoch 26/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1592 - mean_absolute_error: 0.1592 - val_loss: 0.2102 - val_mean_absolute_error: 0.2102\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.14995\n",
      "Epoch 27/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.1830 - mean_absolute_error: 0.1830 - val_loss: 0.1424 - val_mean_absolute_error: 0.1424\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.14995 to 0.14238, saving model to Weights-027--0.14238.hdf5\n",
      "Epoch 28/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.1610 - mean_absolute_error: 0.1610 - val_loss: 0.1425 - val_mean_absolute_error: 0.1425\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.14238\n",
      "Epoch 29/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.1623 - mean_absolute_error: 0.1623 - val_loss: 0.2358 - val_mean_absolute_error: 0.2358\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.14238\n",
      "Epoch 30/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.1673 - mean_absolute_error: 0.1673 - val_loss: 0.1349 - val_mean_absolute_error: 0.1349\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.14238 to 0.13487, saving model to Weights-030--0.13487.hdf5\n",
      "Epoch 31/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.1313 - mean_absolute_error: 0.1313 - val_loss: 0.3328 - val_mean_absolute_error: 0.3328\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13487\n",
      "Epoch 32/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1902 - mean_absolute_error: 0.1902 - val_loss: 0.2609 - val_mean_absolute_error: 0.2609\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13487\n",
      "Epoch 33/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.1532 - mean_absolute_error: 0.1532 - val_loss: 0.2351 - val_mean_absolute_error: 0.2351\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13487\n",
      "Epoch 34/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 0.1853 - mean_absolute_error: 0.1853 - val_loss: 0.4081 - val_mean_absolute_error: 0.4081\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13487\n",
      "Epoch 35/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.1813 - val_mean_absolute_error: 0.1813\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13487\n",
      "Epoch 36/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.1188 - val_mean_absolute_error: 0.1188\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.13487 to 0.11885, saving model to Weights-036--0.11885.hdf5\n",
      "Epoch 37/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.1637 - mean_absolute_error: 0.1637 - val_loss: 0.2291 - val_mean_absolute_error: 0.2291\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11885\n",
      "Epoch 38/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.1366 - mean_absolute_error: 0.1366 - val_loss: 0.1983 - val_mean_absolute_error: 0.1983\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11885\n",
      "Epoch 39/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.2654 - val_mean_absolute_error: 0.2654\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11885\n",
      "Epoch 40/500\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 0.1159 - mean_absolute_error: 0.1159 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11885\n",
      "Epoch 41/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.1471 - mean_absolute_error: 0.1471 - val_loss: 0.1358 - val_mean_absolute_error: 0.1358\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11885\n",
      "Epoch 42/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1740 - mean_absolute_error: 0.1740 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11885\n",
      "Epoch 43/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.1381 - mean_absolute_error: 0.1381 - val_loss: 0.1567 - val_mean_absolute_error: 0.1567\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11885\n",
      "Epoch 44/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.1856 - mean_absolute_error: 0.1856 - val_loss: 0.1610 - val_mean_absolute_error: 0.1610\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11885\n",
      "Epoch 45/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.1566 - mean_absolute_error: 0.1566 - val_loss: 0.1407 - val_mean_absolute_error: 0.1407\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11885\n",
      "Epoch 46/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.1480 - mean_absolute_error: 0.1480 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11885\n",
      "Epoch 47/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.1615 - mean_absolute_error: 0.1615 - val_loss: 0.1154 - val_mean_absolute_error: 0.1154\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.11885 to 0.11540, saving model to Weights-047--0.11540.hdf5\n",
      "Epoch 48/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.1751 - mean_absolute_error: 0.1751 - val_loss: 0.1423 - val_mean_absolute_error: 0.1423\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11540\n",
      "Epoch 49/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.1276 - mean_absolute_error: 0.1276 - val_loss: 0.1847 - val_mean_absolute_error: 0.1847\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11540\n",
      "Epoch 50/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.2363 - mean_absolute_error: 0.2363 - val_loss: 0.3091 - val_mean_absolute_error: 0.3091\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11540\n",
      "Epoch 51/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.2232 - mean_absolute_error: 0.2232 - val_loss: 0.1174 - val_mean_absolute_error: 0.1174\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11540\n",
      "Epoch 52/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1461 - mean_absolute_error: 0.1461 - val_loss: 0.2353 - val_mean_absolute_error: 0.2353\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11540\n",
      "Epoch 53/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.1498 - mean_absolute_error: 0.1498 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11540\n",
      "Epoch 54/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0970 - mean_absolute_error: 0.0970 - val_loss: 0.1146 - val_mean_absolute_error: 0.1146\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.11540 to 0.11460, saving model to Weights-054--0.11460.hdf5\n",
      "Epoch 55/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1349 - mean_absolute_error: 0.1349 - val_loss: 0.1557 - val_mean_absolute_error: 0.1557\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11460\n",
      "Epoch 56/500\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 0.1475 - mean_absolute_error: 0.1475 - val_loss: 0.1470 - val_mean_absolute_error: 0.1470\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11460\n",
      "Epoch 57/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.2911 - mean_absolute_error: 0.2911 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11460\n",
      "Epoch 58/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.1683 - mean_absolute_error: 0.1683 - val_loss: 0.1576 - val_mean_absolute_error: 0.1576\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11460\n",
      "Epoch 59/500\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 0.1676 - mean_absolute_error: 0.1676 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11460\n",
      "Epoch 60/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.1241 - mean_absolute_error: 0.1241 - val_loss: 0.1150 - val_mean_absolute_error: 0.1150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11460\n",
      "Epoch 61/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.1982 - mean_absolute_error: 0.1982 - val_loss: 0.1304 - val_mean_absolute_error: 0.1304\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11460\n",
      "Epoch 62/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.2271 - mean_absolute_error: 0.2271 - val_loss: 0.2788 - val_mean_absolute_error: 0.2788\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11460\n",
      "Epoch 63/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1785 - mean_absolute_error: 0.1785 - val_loss: 0.1129 - val_mean_absolute_error: 0.1129\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.11460 to 0.11292, saving model to Weights-063--0.11292.hdf5\n",
      "Epoch 64/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.11292\n",
      "Epoch 65/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1121 - mean_absolute_error: 0.1121 - val_loss: 0.1666 - val_mean_absolute_error: 0.1666\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.11292\n",
      "Epoch 66/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.1140 - mean_absolute_error: 0.1140 - val_loss: 0.1431 - val_mean_absolute_error: 0.1431\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11292\n",
      "Epoch 67/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.1353 - mean_absolute_error: 0.1353 - val_loss: 0.2100 - val_mean_absolute_error: 0.2100\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11292\n",
      "Epoch 68/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.1537 - mean_absolute_error: 0.1537 - val_loss: 0.5437 - val_mean_absolute_error: 0.5437\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11292\n",
      "Epoch 69/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 0.1561 - mean_absolute_error: 0.1561 - val_loss: 0.1744 - val_mean_absolute_error: 0.1744\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11292\n",
      "Epoch 70/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.1464 - mean_absolute_error: 0.1464 - val_loss: 0.1149 - val_mean_absolute_error: 0.1149\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11292\n",
      "Epoch 71/500\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1363 - val_mean_absolute_error: 0.1363\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.11292\n",
      "Epoch 72/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.2573 - mean_absolute_error: 0.2573 - val_loss: 0.4289 - val_mean_absolute_error: 0.4289\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11292\n",
      "Epoch 73/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1932 - mean_absolute_error: 0.1932 - val_loss: 0.2365 - val_mean_absolute_error: 0.2365\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.11292\n",
      "Epoch 74/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1264 - mean_absolute_error: 0.1264 - val_loss: 0.1287 - val_mean_absolute_error: 0.1287\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.11292\n",
      "Epoch 75/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.1294 - mean_absolute_error: 0.1294 - val_loss: 0.1659 - val_mean_absolute_error: 0.1659\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.11292\n",
      "Epoch 76/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.1464 - mean_absolute_error: 0.1464 - val_loss: 0.1038 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.11292 to 0.10380, saving model to Weights-076--0.10380.hdf5\n",
      "Epoch 77/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0873 - mean_absolute_error: 0.0873 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.10380 to 0.10359, saving model to Weights-077--0.10359.hdf5\n",
      "Epoch 78/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.0956 - mean_absolute_error: 0.0956 - val_loss: 0.1457 - val_mean_absolute_error: 0.1457\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10359\n",
      "Epoch 79/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.1129 - mean_absolute_error: 0.1129 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10359 to 0.10013, saving model to Weights-079--0.10013.hdf5\n",
      "Epoch 80/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1347 - mean_absolute_error: 0.1347 - val_loss: 0.1739 - val_mean_absolute_error: 0.1739\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10013\n",
      "Epoch 81/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.1655 - mean_absolute_error: 0.1655 - val_loss: 0.1178 - val_mean_absolute_error: 0.1178\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10013\n",
      "Epoch 82/500\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 0.2631 - mean_absolute_error: 0.2631 - val_loss: 0.4057 - val_mean_absolute_error: 0.4057\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10013\n",
      "Epoch 83/500\n",
      "1168/1168 [==============================] - 0s 234us/step - loss: 0.1457 - mean_absolute_error: 0.1457 - val_loss: 0.2043 - val_mean_absolute_error: 0.2043\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10013\n",
      "Epoch 84/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.1516 - mean_absolute_error: 0.1516 - val_loss: 0.0997 - val_mean_absolute_error: 0.0997\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.10013 to 0.09972, saving model to Weights-084--0.09972.hdf5\n",
      "Epoch 85/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.1317 - val_mean_absolute_error: 0.1317\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09972\n",
      "Epoch 86/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.1565 - mean_absolute_error: 0.1565 - val_loss: 0.1453 - val_mean_absolute_error: 0.1453\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09972\n",
      "Epoch 87/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.1252 - mean_absolute_error: 0.1252 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09972\n",
      "Epoch 88/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.1810 - val_mean_absolute_error: 0.1810\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.09972\n",
      "Epoch 89/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.1223 - mean_absolute_error: 0.1223 - val_loss: 0.2194 - val_mean_absolute_error: 0.2194\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.09972\n",
      "Epoch 90/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1386 - mean_absolute_error: 0.1386 - val_loss: 0.1196 - val_mean_absolute_error: 0.1196\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.09972\n",
      "Epoch 91/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1195 - mean_absolute_error: 0.1195 - val_loss: 0.1679 - val_mean_absolute_error: 0.1679\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.09972\n",
      "Epoch 92/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2804 - val_mean_absolute_error: 0.2804\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.09972\n",
      "Epoch 93/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.1260 - mean_absolute_error: 0.1260 - val_loss: 0.1500 - val_mean_absolute_error: 0.1500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.09972\n",
      "Epoch 94/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1974 - mean_absolute_error: 0.1974 - val_loss: 0.1051 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.09972\n",
      "Epoch 95/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.1317 - mean_absolute_error: 0.1317 - val_loss: 0.3957 - val_mean_absolute_error: 0.3957\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.09972\n",
      "Epoch 96/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.2719 - mean_absolute_error: 0.2719 - val_loss: 0.1554 - val_mean_absolute_error: 0.1554\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.09972\n",
      "Epoch 97/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.1301 - mean_absolute_error: 0.1301 - val_loss: 0.2055 - val_mean_absolute_error: 0.2055\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.09972\n",
      "Epoch 98/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 0.2274 - mean_absolute_error: 0.2274 - val_loss: 0.1568 - val_mean_absolute_error: 0.1568\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.09972\n",
      "Epoch 99/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.2383 - mean_absolute_error: 0.2383 - val_loss: 0.2340 - val_mean_absolute_error: 0.2340\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.09972\n",
      "Epoch 100/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1602 - mean_absolute_error: 0.1602 - val_loss: 0.1514 - val_mean_absolute_error: 0.1514\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.09972\n",
      "Epoch 101/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1118 - mean_absolute_error: 0.1118 - val_loss: 0.1240 - val_mean_absolute_error: 0.1240\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.09972\n",
      "Epoch 102/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.1360 - mean_absolute_error: 0.1360 - val_loss: 0.2027 - val_mean_absolute_error: 0.2027\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.09972\n",
      "Epoch 103/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.1135 - mean_absolute_error: 0.1135 - val_loss: 0.1063 - val_mean_absolute_error: 0.1063\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.09972\n",
      "Epoch 104/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.1248 - mean_absolute_error: 0.1248 - val_loss: 0.1528 - val_mean_absolute_error: 0.1528\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.09972\n",
      "Epoch 105/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.1964 - mean_absolute_error: 0.1964 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.09972\n",
      "Epoch 106/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.1530 - mean_absolute_error: 0.1530 - val_loss: 0.1594 - val_mean_absolute_error: 0.1594\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.09972\n",
      "Epoch 107/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.1386 - mean_absolute_error: 0.1386 - val_loss: 0.1122 - val_mean_absolute_error: 0.1122\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.09972\n",
      "Epoch 108/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.1317 - mean_absolute_error: 0.1317 - val_loss: 0.1016 - val_mean_absolute_error: 0.1016\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.09972\n",
      "Epoch 109/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.1365 - mean_absolute_error: 0.1365 - val_loss: 0.1469 - val_mean_absolute_error: 0.1469\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.09972\n",
      "Epoch 110/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.1161 - val_mean_absolute_error: 0.1161\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.09972\n",
      "Epoch 111/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.0896 - mean_absolute_error: 0.0896 - val_loss: 0.0999 - val_mean_absolute_error: 0.0999\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.09972\n",
      "Epoch 112/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1004 - mean_absolute_error: 0.1004 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.09972\n",
      "Epoch 113/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.0915 - mean_absolute_error: 0.0915 - val_loss: 0.1069 - val_mean_absolute_error: 0.1069\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.09972\n",
      "Epoch 114/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.1184 - mean_absolute_error: 0.1184 - val_loss: 0.1660 - val_mean_absolute_error: 0.1660\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.09972\n",
      "Epoch 115/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.1341 - mean_absolute_error: 0.1341 - val_loss: 0.1716 - val_mean_absolute_error: 0.1716\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.09972\n",
      "Epoch 116/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.1147 - mean_absolute_error: 0.1147 - val_loss: 0.1208 - val_mean_absolute_error: 0.1208\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.09972\n",
      "Epoch 117/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.1523 - mean_absolute_error: 0.1523 - val_loss: 0.0989 - val_mean_absolute_error: 0.0989\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09972 to 0.09885, saving model to Weights-117--0.09885.hdf5\n",
      "Epoch 118/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1046 - mean_absolute_error: 0.1046 - val_loss: 0.0932 - val_mean_absolute_error: 0.0932\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09885 to 0.09319, saving model to Weights-118--0.09319.hdf5\n",
      "Epoch 119/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 0.0792 - mean_absolute_error: 0.0792 - val_loss: 0.1124 - val_mean_absolute_error: 0.1124\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.09319\n",
      "Epoch 120/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.1323 - mean_absolute_error: 0.1323 - val_loss: 0.1718 - val_mean_absolute_error: 0.1718\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.09319\n",
      "Epoch 121/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.2002 - mean_absolute_error: 0.2002 - val_loss: 0.0973 - val_mean_absolute_error: 0.0973\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.09319\n",
      "Epoch 122/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0994 - mean_absolute_error: 0.0994 - val_loss: 0.2340 - val_mean_absolute_error: 0.2340\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.09319\n",
      "Epoch 123/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1170 - mean_absolute_error: 0.1170 - val_loss: 0.1563 - val_mean_absolute_error: 0.1563\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.09319\n",
      "Epoch 124/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1172 - mean_absolute_error: 0.1172 - val_loss: 0.0966 - val_mean_absolute_error: 0.0966\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.09319\n",
      "Epoch 125/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.1325 - mean_absolute_error: 0.1325 - val_loss: 0.1465 - val_mean_absolute_error: 0.1465\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.09319\n",
      "Epoch 126/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0959 - mean_absolute_error: 0.0959 - val_loss: 0.0955 - val_mean_absolute_error: 0.0955\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.09319\n",
      "Epoch 127/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 0.1128 - mean_absolute_error: 0.1128 - val_loss: 0.1090 - val_mean_absolute_error: 0.1090\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.09319\n",
      "Epoch 128/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0801 - mean_absolute_error: 0.0801 - val_loss: 0.1733 - val_mean_absolute_error: 0.1733\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.09319\n",
      "Epoch 129/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.1030 - mean_absolute_error: 0.1030 - val_loss: 0.1123 - val_mean_absolute_error: 0.1123\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.09319\n",
      "Epoch 130/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.1446 - mean_absolute_error: 0.1446 - val_loss: 0.1702 - val_mean_absolute_error: 0.1702\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.09319\n",
      "Epoch 131/500\n",
      "1168/1168 [==============================] - 0s 225us/step - loss: 0.1116 - mean_absolute_error: 0.1116 - val_loss: 0.1870 - val_mean_absolute_error: 0.1870\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.09319\n",
      "Epoch 132/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.1508 - mean_absolute_error: 0.1508 - val_loss: 0.1961 - val_mean_absolute_error: 0.1961\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.09319\n",
      "Epoch 133/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.1005 - mean_absolute_error: 0.1005 - val_loss: 0.0971 - val_mean_absolute_error: 0.0971\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.09319\n",
      "Epoch 134/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.1147 - mean_absolute_error: 0.1147 - val_loss: 0.1345 - val_mean_absolute_error: 0.1345\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.09319\n",
      "Epoch 135/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.1293 - mean_absolute_error: 0.1293 - val_loss: 0.1343 - val_mean_absolute_error: 0.1343\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.09319\n",
      "Epoch 136/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1361 - val_mean_absolute_error: 0.1361\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.09319\n",
      "Epoch 137/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1459 - val_mean_absolute_error: 0.1459\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.09319\n",
      "Epoch 138/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0890 - mean_absolute_error: 0.0890 - val_loss: 0.1973 - val_mean_absolute_error: 0.1973\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.09319\n",
      "Epoch 139/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.1071 - mean_absolute_error: 0.1071 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.09319\n",
      "Epoch 140/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.0878 - mean_absolute_error: 0.0878 - val_loss: 0.1068 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.09319\n",
      "Epoch 141/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.1648 - mean_absolute_error: 0.1648 - val_loss: 0.4280 - val_mean_absolute_error: 0.4280\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.09319\n",
      "Epoch 142/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.2581 - mean_absolute_error: 0.2581 - val_loss: 0.1503 - val_mean_absolute_error: 0.1503\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.09319\n",
      "Epoch 143/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.09319\n",
      "Epoch 144/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.1467 - mean_absolute_error: 0.1467 - val_loss: 0.2084 - val_mean_absolute_error: 0.2084\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.09319\n",
      "Epoch 145/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1288 - mean_absolute_error: 0.1288 - val_loss: 0.1516 - val_mean_absolute_error: 0.1516\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.09319\n",
      "Epoch 146/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1298 - val_mean_absolute_error: 0.1298\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.09319\n",
      "Epoch 147/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.1075 - mean_absolute_error: 0.1075 - val_loss: 0.1014 - val_mean_absolute_error: 0.1014\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.09319\n",
      "Epoch 148/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1088 - mean_absolute_error: 0.1088 - val_loss: 0.1125 - val_mean_absolute_error: 0.1125\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.09319\n",
      "Epoch 149/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1063 - val_mean_absolute_error: 0.1063\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.09319\n",
      "Epoch 150/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.1033 - mean_absolute_error: 0.1033 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.09319\n",
      "Epoch 151/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.1063 - mean_absolute_error: 0.1063 - val_loss: 0.1081 - val_mean_absolute_error: 0.1081\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.09319\n",
      "Epoch 152/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0906 - mean_absolute_error: 0.0906 - val_loss: 0.1929 - val_mean_absolute_error: 0.1929\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.09319\n",
      "Epoch 153/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.1321 - mean_absolute_error: 0.1321 - val_loss: 0.1443 - val_mean_absolute_error: 0.1443\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.09319\n",
      "Epoch 154/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.1318 - mean_absolute_error: 0.1318 - val_loss: 0.0995 - val_mean_absolute_error: 0.0995\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.09319\n",
      "Epoch 155/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.1036 - mean_absolute_error: 0.1036 - val_loss: 0.1003 - val_mean_absolute_error: 0.1003\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.09319\n",
      "Epoch 156/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.1123 - mean_absolute_error: 0.1123 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.09319\n",
      "Epoch 157/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1249 - mean_absolute_error: 0.1249 - val_loss: 0.1457 - val_mean_absolute_error: 0.1457\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.09319\n",
      "Epoch 158/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.1086 - val_mean_absolute_error: 0.1086\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.09319\n",
      "Epoch 159/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0923 - mean_absolute_error: 0.0923 - val_loss: 0.0978 - val_mean_absolute_error: 0.0978\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.09319\n",
      "Epoch 160/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.1005 - mean_absolute_error: 0.1005 - val_loss: 0.1295 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.09319\n",
      "Epoch 161/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.1098 - mean_absolute_error: 0.1098 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.09319\n",
      "Epoch 162/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0960 - mean_absolute_error: 0.0960 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.09319\n",
      "Epoch 163/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.0751 - mean_absolute_error: 0.0751 - val_loss: 0.0970 - val_mean_absolute_error: 0.0970\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.09319\n",
      "Epoch 164/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.1252 - mean_absolute_error: 0.1252 - val_loss: 0.3321 - val_mean_absolute_error: 0.3321\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.09319\n",
      "Epoch 165/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.2427 - mean_absolute_error: 0.2427 - val_loss: 0.1705 - val_mean_absolute_error: 0.1705\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.09319\n",
      "Epoch 166/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.1108 - mean_absolute_error: 0.1108 - val_loss: 0.1315 - val_mean_absolute_error: 0.1315\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.09319\n",
      "Epoch 167/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0811 - mean_absolute_error: 0.0811 - val_loss: 0.1131 - val_mean_absolute_error: 0.1131\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.09319\n",
      "Epoch 168/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0970 - mean_absolute_error: 0.0970 - val_loss: 0.1985 - val_mean_absolute_error: 0.1985\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.09319\n",
      "Epoch 169/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1606 - val_mean_absolute_error: 0.1606\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.09319\n",
      "Epoch 170/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.0952 - val_mean_absolute_error: 0.0952\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.09319\n",
      "Epoch 171/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.1089 - mean_absolute_error: 0.1089 - val_loss: 0.0996 - val_mean_absolute_error: 0.0996\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.09319\n",
      "Epoch 172/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1718 - mean_absolute_error: 0.1718 - val_loss: 0.2960 - val_mean_absolute_error: 0.2960\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.09319\n",
      "Epoch 173/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.1191 - mean_absolute_error: 0.1191 - val_loss: 0.1197 - val_mean_absolute_error: 0.1197\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.09319\n",
      "Epoch 174/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0863 - mean_absolute_error: 0.0863 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.09319\n",
      "Epoch 175/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.1595 - mean_absolute_error: 0.1595 - val_loss: 0.2300 - val_mean_absolute_error: 0.2300\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.09319\n",
      "Epoch 176/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.1201 - mean_absolute_error: 0.1201 - val_loss: 0.1822 - val_mean_absolute_error: 0.1822\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.09319\n",
      "Epoch 177/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.1026 - mean_absolute_error: 0.1026 - val_loss: 0.1126 - val_mean_absolute_error: 0.1126\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.09319\n",
      "Epoch 178/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.1070 - mean_absolute_error: 0.1070 - val_loss: 0.1035 - val_mean_absolute_error: 0.1035\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.09319\n",
      "Epoch 179/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0869 - mean_absolute_error: 0.0869 - val_loss: 0.1224 - val_mean_absolute_error: 0.1224\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.09319\n",
      "Epoch 180/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.1270 - mean_absolute_error: 0.1270 - val_loss: 0.1658 - val_mean_absolute_error: 0.1658\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.09319\n",
      "Epoch 181/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.1104 - mean_absolute_error: 0.1104 - val_loss: 0.1582 - val_mean_absolute_error: 0.1582\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.09319\n",
      "Epoch 182/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0857 - mean_absolute_error: 0.0857 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.09319\n",
      "Epoch 183/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.1029 - mean_absolute_error: 0.1029 - val_loss: 0.1845 - val_mean_absolute_error: 0.1845\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.09319\n",
      "Epoch 184/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.0804 - mean_absolute_error: 0.0804 - val_loss: 0.0906 - val_mean_absolute_error: 0.0906\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.09319 to 0.09065, saving model to Weights-184--0.09065.hdf5\n",
      "Epoch 185/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.0949 - mean_absolute_error: 0.0949 - val_loss: 0.1460 - val_mean_absolute_error: 0.1460\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.09065\n",
      "Epoch 186/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1085 - mean_absolute_error: 0.1085 - val_loss: 0.0971 - val_mean_absolute_error: 0.0971\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.09065\n",
      "Epoch 187/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.1856 - mean_absolute_error: 0.1856 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.09065\n",
      "Epoch 188/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.1230 - mean_absolute_error: 0.1230 - val_loss: 0.1130 - val_mean_absolute_error: 0.1130\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.09065\n",
      "Epoch 189/500\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 0.1535 - mean_absolute_error: 0.1535 - val_loss: 0.1074 - val_mean_absolute_error: 0.1074\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.09065\n",
      "Epoch 190/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.1450 - mean_absolute_error: 0.1450 - val_loss: 0.2920 - val_mean_absolute_error: 0.2920\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.09065\n",
      "Epoch 191/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1419 - mean_absolute_error: 0.1419 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.09065\n",
      "Epoch 192/500\n",
      "1168/1168 [==============================] - 0s 230us/step - loss: 0.0921 - mean_absolute_error: 0.0921 - val_loss: 0.1160 - val_mean_absolute_error: 0.1160\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.09065\n",
      "Epoch 193/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.0881 - mean_absolute_error: 0.0881 - val_loss: 0.1094 - val_mean_absolute_error: 0.1094\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.09065\n",
      "Epoch 194/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.1263 - mean_absolute_error: 0.1263 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.09065\n",
      "Epoch 195/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.0974 - mean_absolute_error: 0.0974 - val_loss: 0.1090 - val_mean_absolute_error: 0.1090\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.09065\n",
      "Epoch 196/500\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 0.0958 - mean_absolute_error: 0.0958 - val_loss: 0.1027 - val_mean_absolute_error: 0.1027\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.09065\n",
      "Epoch 197/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.1082 - mean_absolute_error: 0.1082 - val_loss: 0.1225 - val_mean_absolute_error: 0.1225\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.09065\n",
      "Epoch 198/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.1066 - mean_absolute_error: 0.1066 - val_loss: 0.1076 - val_mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.09065\n",
      "Epoch 199/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.1107 - mean_absolute_error: 0.1107 - val_loss: 0.1648 - val_mean_absolute_error: 0.1648\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.09065\n",
      "Epoch 200/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.1706 - mean_absolute_error: 0.1706 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.09065\n",
      "Epoch 201/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.1021 - mean_absolute_error: 0.1021 - val_loss: 0.1067 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.09065\n",
      "Epoch 202/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.1017 - mean_absolute_error: 0.1017 - val_loss: 0.0999 - val_mean_absolute_error: 0.0999\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.09065\n",
      "Epoch 203/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.1514 - mean_absolute_error: 0.1514 - val_loss: 0.2020 - val_mean_absolute_error: 0.2020\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.09065\n",
      "Epoch 204/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.1685 - mean_absolute_error: 0.1685 - val_loss: 0.1848 - val_mean_absolute_error: 0.1848\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.09065\n",
      "Epoch 205/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0959 - mean_absolute_error: 0.0959 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.09065\n",
      "Epoch 206/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0852 - mean_absolute_error: 0.0852 - val_loss: 0.1295 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.09065\n",
      "Epoch 207/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1096 - mean_absolute_error: 0.1096 - val_loss: 0.1085 - val_mean_absolute_error: 0.1085\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.09065\n",
      "Epoch 208/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0927 - mean_absolute_error: 0.0927 - val_loss: 0.0959 - val_mean_absolute_error: 0.0959\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.09065\n",
      "Epoch 209/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0921 - mean_absolute_error: 0.0921 - val_loss: 0.1219 - val_mean_absolute_error: 0.1219\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.09065\n",
      "Epoch 210/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.1172 - mean_absolute_error: 0.1172 - val_loss: 0.1714 - val_mean_absolute_error: 0.1714\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.09065\n",
      "Epoch 211/500\n",
      "1168/1168 [==============================] - 0s 231us/step - loss: 0.1262 - mean_absolute_error: 0.1262 - val_loss: 0.1698 - val_mean_absolute_error: 0.1698\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.09065\n",
      "Epoch 212/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.1170 - mean_absolute_error: 0.1170 - val_loss: 0.3024 - val_mean_absolute_error: 0.3024\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.09065\n",
      "Epoch 213/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.1455 - mean_absolute_error: 0.1455 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.09065\n",
      "Epoch 214/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.09065\n",
      "Epoch 215/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.0767 - mean_absolute_error: 0.0767 - val_loss: 0.1473 - val_mean_absolute_error: 0.1473\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.09065\n",
      "Epoch 216/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0857 - mean_absolute_error: 0.0857 - val_loss: 0.1059 - val_mean_absolute_error: 0.1059\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.09065\n",
      "Epoch 217/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.0839 - mean_absolute_error: 0.0839 - val_loss: 0.0967 - val_mean_absolute_error: 0.0967\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.09065\n",
      "Epoch 218/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0842 - mean_absolute_error: 0.0842 - val_loss: 0.1760 - val_mean_absolute_error: 0.1760\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.09065\n",
      "Epoch 219/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.1057 - mean_absolute_error: 0.1057 - val_loss: 0.1532 - val_mean_absolute_error: 0.1532\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.09065\n",
      "Epoch 220/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.1635 - mean_absolute_error: 0.1635 - val_loss: 0.1500 - val_mean_absolute_error: 0.1500\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.09065\n",
      "Epoch 221/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1023 - mean_absolute_error: 0.1023 - val_loss: 0.1048 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.09065\n",
      "Epoch 222/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0920 - mean_absolute_error: 0.0920 - val_loss: 0.1079 - val_mean_absolute_error: 0.1079\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.09065\n",
      "Epoch 223/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.1191 - mean_absolute_error: 0.1191 - val_loss: 0.1381 - val_mean_absolute_error: 0.1381\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.09065\n",
      "Epoch 224/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1285 - mean_absolute_error: 0.1285 - val_loss: 0.2299 - val_mean_absolute_error: 0.2299\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.09065\n",
      "Epoch 225/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.1766 - mean_absolute_error: 0.1766 - val_loss: 0.1007 - val_mean_absolute_error: 0.1007\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.09065\n",
      "Epoch 226/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0959 - mean_absolute_error: 0.0959 - val_loss: 0.1883 - val_mean_absolute_error: 0.1883\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.09065\n",
      "Epoch 227/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1369 - mean_absolute_error: 0.1369 - val_loss: 0.1454 - val_mean_absolute_error: 0.1454\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.09065\n",
      "Epoch 228/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.0721 - mean_absolute_error: 0.0721 - val_loss: 0.1061 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.09065\n",
      "Epoch 229/500\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 0.0952 - mean_absolute_error: 0.0952 - val_loss: 0.1429 - val_mean_absolute_error: 0.1429\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.09065\n",
      "Epoch 230/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.1023 - mean_absolute_error: 0.1023 - val_loss: 0.1692 - val_mean_absolute_error: 0.1692\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.09065\n",
      "Epoch 231/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.0729 - mean_absolute_error: 0.0729 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.09065\n",
      "Epoch 232/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.1899 - mean_absolute_error: 0.1899 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.09065\n",
      "Epoch 233/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.1038 - mean_absolute_error: 0.1038 - val_loss: 0.1104 - val_mean_absolute_error: 0.1104\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.09065\n",
      "Epoch 234/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1271 - mean_absolute_error: 0.1271 - val_loss: 0.1182 - val_mean_absolute_error: 0.1182\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.09065\n",
      "Epoch 235/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.1008 - mean_absolute_error: 0.1008 - val_loss: 0.1056 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.09065\n",
      "Epoch 236/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.1149 - mean_absolute_error: 0.1149 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.09065\n",
      "Epoch 237/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0681 - mean_absolute_error: 0.0681 - val_loss: 0.1719 - val_mean_absolute_error: 0.1719\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.09065\n",
      "Epoch 238/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.2859 - val_mean_absolute_error: 0.2859\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.09065\n",
      "Epoch 239/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.2098 - mean_absolute_error: 0.2098 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.09065\n",
      "Epoch 240/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0931 - mean_absolute_error: 0.0931 - val_loss: 0.1079 - val_mean_absolute_error: 0.1079\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.09065\n",
      "Epoch 241/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0778 - mean_absolute_error: 0.0778 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.09065\n",
      "Epoch 242/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.1009 - mean_absolute_error: 0.1009 - val_loss: 0.1762 - val_mean_absolute_error: 0.1762\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.09065\n",
      "Epoch 243/500\n",
      "1168/1168 [==============================] - 0s 221us/step - loss: 0.1063 - mean_absolute_error: 0.1063 - val_loss: 0.2366 - val_mean_absolute_error: 0.2366\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.09065\n",
      "Epoch 244/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.1475 - mean_absolute_error: 0.1475 - val_loss: 0.1185 - val_mean_absolute_error: 0.1185\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.09065\n",
      "Epoch 245/500\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 0.0790 - mean_absolute_error: 0.0790 - val_loss: 0.1285 - val_mean_absolute_error: 0.1285\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.09065\n",
      "Epoch 246/500\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 0.1012 - mean_absolute_error: 0.1012 - val_loss: 0.1014 - val_mean_absolute_error: 0.1014\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.09065\n",
      "Epoch 247/500\n",
      "1168/1168 [==============================] - 0s 221us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.09065\n",
      "Epoch 248/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.1037 - mean_absolute_error: 0.1037 - val_loss: 0.2120 - val_mean_absolute_error: 0.2120\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.09065\n",
      "Epoch 249/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.1102 - mean_absolute_error: 0.1102 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.09065\n",
      "Epoch 250/500\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0783 - mean_absolute_error: 0.0783 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.09065\n",
      "Epoch 251/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1508 - mean_absolute_error: 0.1508 - val_loss: 0.2041 - val_mean_absolute_error: 0.2041\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.09065\n",
      "Epoch 252/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.0986 - mean_absolute_error: 0.0986 - val_loss: 0.1450 - val_mean_absolute_error: 0.1450\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.09065\n",
      "Epoch 253/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0916 - mean_absolute_error: 0.0916 - val_loss: 0.0999 - val_mean_absolute_error: 0.0999\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.09065\n",
      "Epoch 254/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.0932 - mean_absolute_error: 0.0932 - val_loss: 0.1783 - val_mean_absolute_error: 0.1783\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.09065\n",
      "Epoch 255/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.1623 - val_mean_absolute_error: 0.1623\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.09065\n",
      "Epoch 256/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.1048 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.09065\n",
      "Epoch 257/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0792 - mean_absolute_error: 0.0792 - val_loss: 0.1301 - val_mean_absolute_error: 0.1301\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.09065\n",
      "Epoch 258/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.0974 - mean_absolute_error: 0.0974 - val_loss: 0.1294 - val_mean_absolute_error: 0.1294\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.09065\n",
      "Epoch 259/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.1048 - mean_absolute_error: 0.1048 - val_loss: 0.1860 - val_mean_absolute_error: 0.1860\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.09065\n",
      "Epoch 260/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1258 - mean_absolute_error: 0.1258 - val_loss: 0.0979 - val_mean_absolute_error: 0.0979\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.09065\n",
      "Epoch 261/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.0813 - mean_absolute_error: 0.0813 - val_loss: 0.0932 - val_mean_absolute_error: 0.0932\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.09065\n",
      "Epoch 262/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.0923 - mean_absolute_error: 0.0923 - val_loss: 0.1143 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.09065\n",
      "Epoch 263/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.0792 - mean_absolute_error: 0.0792 - val_loss: 0.1732 - val_mean_absolute_error: 0.1732\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.09065\n",
      "Epoch 264/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.1092 - mean_absolute_error: 0.1092 - val_loss: 0.1056 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.09065\n",
      "Epoch 265/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0801 - mean_absolute_error: 0.0801 - val_loss: 0.1619 - val_mean_absolute_error: 0.1619\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.09065\n",
      "Epoch 266/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.1606 - mean_absolute_error: 0.1606 - val_loss: 0.1199 - val_mean_absolute_error: 0.1199\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.09065\n",
      "Epoch 267/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0942 - mean_absolute_error: 0.0942 - val_loss: 0.0978 - val_mean_absolute_error: 0.0978\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.09065\n",
      "Epoch 268/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.0991 - mean_absolute_error: 0.0991 - val_loss: 0.1318 - val_mean_absolute_error: 0.1318\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.09065\n",
      "Epoch 269/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.1051 - mean_absolute_error: 0.1051 - val_loss: 0.1148 - val_mean_absolute_error: 0.1148\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.09065\n",
      "Epoch 270/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0952 - mean_absolute_error: 0.0952 - val_loss: 0.1307 - val_mean_absolute_error: 0.1307\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.09065\n",
      "Epoch 271/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0808 - mean_absolute_error: 0.0808 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.09065\n",
      "Epoch 272/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 0.1040 - mean_absolute_error: 0.1040 - val_loss: 0.0932 - val_mean_absolute_error: 0.0932\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.09065\n",
      "Epoch 273/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0705 - mean_absolute_error: 0.0705 - val_loss: 0.1199 - val_mean_absolute_error: 0.1199\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.09065\n",
      "Epoch 274/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1124 - mean_absolute_error: 0.1124 - val_loss: 0.1713 - val_mean_absolute_error: 0.1713\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.09065\n",
      "Epoch 275/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0954 - mean_absolute_error: 0.0954 - val_loss: 0.1413 - val_mean_absolute_error: 0.1413\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.09065\n",
      "Epoch 276/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.0818 - mean_absolute_error: 0.0818 - val_loss: 0.1348 - val_mean_absolute_error: 0.1348\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.09065\n",
      "Epoch 277/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0743 - mean_absolute_error: 0.0743 - val_loss: 0.1198 - val_mean_absolute_error: 0.1198\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.09065\n",
      "Epoch 278/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0981 - mean_absolute_error: 0.0981 - val_loss: 0.1307 - val_mean_absolute_error: 0.1307\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.09065\n",
      "Epoch 279/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.0733 - mean_absolute_error: 0.0733 - val_loss: 0.1571 - val_mean_absolute_error: 0.1571\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.09065\n",
      "Epoch 280/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1513 - mean_absolute_error: 0.1513 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.09065\n",
      "Epoch 281/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0940 - mean_absolute_error: 0.0940 - val_loss: 0.1098 - val_mean_absolute_error: 0.1098\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.09065\n",
      "Epoch 282/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.1324 - mean_absolute_error: 0.1324 - val_loss: 0.1509 - val_mean_absolute_error: 0.1509\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.09065\n",
      "Epoch 283/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.0953 - val_mean_absolute_error: 0.0953\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.09065\n",
      "Epoch 284/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.09065\n",
      "Epoch 285/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0977 - mean_absolute_error: 0.0977 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.09065\n",
      "Epoch 286/500\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 0.0982 - mean_absolute_error: 0.0982 - val_loss: 0.1033 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.09065\n",
      "Epoch 287/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0794 - mean_absolute_error: 0.0794 - val_loss: 0.1374 - val_mean_absolute_error: 0.1374\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.09065\n",
      "Epoch 288/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.1100 - mean_absolute_error: 0.1100 - val_loss: 0.0984 - val_mean_absolute_error: 0.0984\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.09065\n",
      "Epoch 289/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 0.1067 - mean_absolute_error: 0.1067 - val_loss: 0.1542 - val_mean_absolute_error: 0.1542\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.09065\n",
      "Epoch 290/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0988 - mean_absolute_error: 0.0988 - val_loss: 0.1542 - val_mean_absolute_error: 0.1542\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.09065\n",
      "Epoch 291/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 0.0701 - mean_absolute_error: 0.0701 - val_loss: 0.0985 - val_mean_absolute_error: 0.0985\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.09065\n",
      "Epoch 292/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0705 - mean_absolute_error: 0.0705 - val_loss: 0.0989 - val_mean_absolute_error: 0.0989\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.09065\n",
      "Epoch 293/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.1020 - val_mean_absolute_error: 0.1020\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.09065\n",
      "Epoch 294/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.1006 - mean_absolute_error: 0.1006 - val_loss: 0.1056 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.09065\n",
      "Epoch 295/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0842 - mean_absolute_error: 0.0842 - val_loss: 0.1490 - val_mean_absolute_error: 0.1490\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.09065\n",
      "Epoch 296/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0918 - mean_absolute_error: 0.0918 - val_loss: 0.1737 - val_mean_absolute_error: 0.1737\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.09065\n",
      "Epoch 297/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.0929 - mean_absolute_error: 0.0929 - val_loss: 0.0976 - val_mean_absolute_error: 0.0976\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.09065\n",
      "Epoch 298/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0695 - mean_absolute_error: 0.0695 - val_loss: 0.1103 - val_mean_absolute_error: 0.1103\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.09065\n",
      "Epoch 299/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.0976 - mean_absolute_error: 0.0976 - val_loss: 0.1121 - val_mean_absolute_error: 0.1121\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.09065\n",
      "Epoch 300/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.0866 - mean_absolute_error: 0.0866 - val_loss: 0.1524 - val_mean_absolute_error: 0.1524\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.09065\n",
      "Epoch 301/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0860 - mean_absolute_error: 0.0860 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.09065\n",
      "Epoch 302/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.1039 - mean_absolute_error: 0.1039 - val_loss: 0.1074 - val_mean_absolute_error: 0.1074\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.09065\n",
      "Epoch 303/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.0837 - mean_absolute_error: 0.0837 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.09065\n",
      "Epoch 304/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.1002 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.09065\n",
      "Epoch 305/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0876 - mean_absolute_error: 0.0876 - val_loss: 0.3762 - val_mean_absolute_error: 0.3762\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.09065\n",
      "Epoch 306/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.1040 - mean_absolute_error: 0.1040 - val_loss: 0.1693 - val_mean_absolute_error: 0.1693\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.09065\n",
      "Epoch 307/500\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 0.0940 - mean_absolute_error: 0.0940 - val_loss: 0.1189 - val_mean_absolute_error: 0.1189\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.09065\n",
      "Epoch 308/500\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 0.0973 - mean_absolute_error: 0.0973 - val_loss: 0.1354 - val_mean_absolute_error: 0.1354\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.09065\n",
      "Epoch 309/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0937 - mean_absolute_error: 0.0937 - val_loss: 0.1721 - val_mean_absolute_error: 0.1721\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.09065\n",
      "Epoch 310/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.0841 - mean_absolute_error: 0.0841 - val_loss: 0.1002 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.09065\n",
      "Epoch 311/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.1150 - val_mean_absolute_error: 0.1150\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.09065\n",
      "Epoch 312/500\n",
      "1168/1168 [==============================] - 0s 222us/step - loss: 0.0936 - mean_absolute_error: 0.0936 - val_loss: 0.1060 - val_mean_absolute_error: 0.1060\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.09065\n",
      "Epoch 313/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0910 - mean_absolute_error: 0.0910 - val_loss: 0.1168 - val_mean_absolute_error: 0.1168\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.09065\n",
      "Epoch 314/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.0958 - mean_absolute_error: 0.0958 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.09065\n",
      "Epoch 315/500\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 0.1069 - mean_absolute_error: 0.1069 - val_loss: 0.1051 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.09065\n",
      "Epoch 316/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1354 - mean_absolute_error: 0.1354 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.09065\n",
      "Epoch 317/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0746 - mean_absolute_error: 0.0746 - val_loss: 0.1625 - val_mean_absolute_error: 0.1625\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.09065\n",
      "Epoch 318/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.0846 - mean_absolute_error: 0.0846 - val_loss: 0.0949 - val_mean_absolute_error: 0.0949\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.09065\n",
      "Epoch 319/500\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 0.0841 - mean_absolute_error: 0.0841 - val_loss: 0.1019 - val_mean_absolute_error: 0.1019\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.09065\n",
      "Epoch 320/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0971 - mean_absolute_error: 0.0971 - val_loss: 0.1018 - val_mean_absolute_error: 0.1018\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.09065\n",
      "Epoch 321/500\n",
      "1168/1168 [==============================] - 0s 229us/step - loss: 0.0969 - mean_absolute_error: 0.0969 - val_loss: 0.1123 - val_mean_absolute_error: 0.1123\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.09065\n",
      "Epoch 322/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0918 - mean_absolute_error: 0.0918 - val_loss: 0.1013 - val_mean_absolute_error: 0.1013\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.09065\n",
      "Epoch 323/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0934 - mean_absolute_error: 0.0934 - val_loss: 0.2289 - val_mean_absolute_error: 0.2289\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.09065\n",
      "Epoch 324/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0977 - mean_absolute_error: 0.0977 - val_loss: 0.1356 - val_mean_absolute_error: 0.1356\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.09065\n",
      "Epoch 325/500\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 0.0925 - mean_absolute_error: 0.0925 - val_loss: 0.1038 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.09065\n",
      "Epoch 326/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.0775 - mean_absolute_error: 0.0775 - val_loss: 0.0982 - val_mean_absolute_error: 0.0982\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.09065\n",
      "Epoch 327/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0905 - mean_absolute_error: 0.0905 - val_loss: 0.1160 - val_mean_absolute_error: 0.1160\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.09065\n",
      "Epoch 328/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.0818 - mean_absolute_error: 0.0818 - val_loss: 0.1190 - val_mean_absolute_error: 0.1190\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.09065\n",
      "Epoch 329/500\n",
      "1168/1168 [==============================] - 0s 225us/step - loss: 0.0993 - mean_absolute_error: 0.0993 - val_loss: 0.1000 - val_mean_absolute_error: 0.1000\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.09065\n",
      "Epoch 330/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0894 - mean_absolute_error: 0.0894 - val_loss: 0.1653 - val_mean_absolute_error: 0.1653\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.09065\n",
      "Epoch 331/500\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 0.1016 - mean_absolute_error: 0.1016 - val_loss: 0.1010 - val_mean_absolute_error: 0.1010\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.09065\n",
      "Epoch 332/500\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.09065\n",
      "Epoch 333/500\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 0.1332 - mean_absolute_error: 0.1332 - val_loss: 0.1199 - val_mean_absolute_error: 0.1199\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.09065\n",
      "Epoch 334/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0633 - mean_absolute_error: 0.0633 - val_loss: 0.1133 - val_mean_absolute_error: 0.1133\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.09065\n",
      "Epoch 335/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.1186 - val_mean_absolute_error: 0.1186\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.09065\n",
      "Epoch 336/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.1094 - mean_absolute_error: 0.1094 - val_loss: 0.1012 - val_mean_absolute_error: 0.1012\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.09065\n",
      "Epoch 337/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0990 - mean_absolute_error: 0.0990 - val_loss: 0.1012 - val_mean_absolute_error: 0.1012\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.09065\n",
      "Epoch 338/500\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 0.0728 - mean_absolute_error: 0.0728 - val_loss: 0.0967 - val_mean_absolute_error: 0.0967\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.09065\n",
      "Epoch 339/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0762 - mean_absolute_error: 0.0762 - val_loss: 0.1951 - val_mean_absolute_error: 0.1951\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.09065\n",
      "Epoch 340/500\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 0.1788 - mean_absolute_error: 0.1788 - val_loss: 0.1631 - val_mean_absolute_error: 0.1631\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.09065\n",
      "Epoch 341/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 0.0993 - mean_absolute_error: 0.0993 - val_loss: 0.2101 - val_mean_absolute_error: 0.2101\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.09065\n",
      "Epoch 342/500\n",
      "1168/1168 [==============================] - 0s 228us/step - loss: 0.1853 - mean_absolute_error: 0.1853 - val_loss: 0.1993 - val_mean_absolute_error: 0.1993\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.09065\n",
      "Epoch 343/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.1550 - mean_absolute_error: 0.1550 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.09065\n",
      "Epoch 344/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1654 - mean_absolute_error: 0.1654 - val_loss: 0.1912 - val_mean_absolute_error: 0.1912\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.09065\n",
      "Epoch 345/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0730 - mean_absolute_error: 0.0730 - val_loss: 0.1124 - val_mean_absolute_error: 0.1124\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.09065\n",
      "Epoch 346/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0880 - mean_absolute_error: 0.0880 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.09065\n",
      "Epoch 347/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.09065\n",
      "Epoch 348/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0859 - mean_absolute_error: 0.0859 - val_loss: 0.1010 - val_mean_absolute_error: 0.1010\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.09065\n",
      "Epoch 349/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0716 - mean_absolute_error: 0.0716 - val_loss: 0.1100 - val_mean_absolute_error: 0.1100\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.09065\n",
      "Epoch 350/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0891 - mean_absolute_error: 0.0891 - val_loss: 0.0982 - val_mean_absolute_error: 0.0982\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.09065\n",
      "Epoch 351/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0881 - mean_absolute_error: 0.0881 - val_loss: 0.1410 - val_mean_absolute_error: 0.1410\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.09065\n",
      "Epoch 352/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.09065\n",
      "Epoch 353/500\n",
      "1168/1168 [==============================] - 0s 221us/step - loss: 0.0752 - mean_absolute_error: 0.0752 - val_loss: 0.1495 - val_mean_absolute_error: 0.1495\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.09065\n",
      "Epoch 354/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0915 - mean_absolute_error: 0.0915 - val_loss: 0.1401 - val_mean_absolute_error: 0.1401\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.09065\n",
      "Epoch 355/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0889 - mean_absolute_error: 0.0889 - val_loss: 0.0939 - val_mean_absolute_error: 0.0939\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.09065\n",
      "Epoch 356/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0577 - mean_absolute_error: 0.0577 - val_loss: 0.1104 - val_mean_absolute_error: 0.1104\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.09065\n",
      "Epoch 357/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0892 - mean_absolute_error: 0.0892 - val_loss: 0.1223 - val_mean_absolute_error: 0.1223\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.09065\n",
      "Epoch 358/500\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 0.1528 - mean_absolute_error: 0.1528 - val_loss: 0.1911 - val_mean_absolute_error: 0.1911\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.09065\n",
      "Epoch 359/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.1071 - mean_absolute_error: 0.1071 - val_loss: 0.1509 - val_mean_absolute_error: 0.1509\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.09065\n",
      "Epoch 360/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0755 - mean_absolute_error: 0.0755 - val_loss: 0.1536 - val_mean_absolute_error: 0.1536\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.09065\n",
      "Epoch 361/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.1502 - mean_absolute_error: 0.1502 - val_loss: 0.1828 - val_mean_absolute_error: 0.1828\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.09065\n",
      "Epoch 362/500\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 0.1184 - mean_absolute_error: 0.1184 - val_loss: 0.1873 - val_mean_absolute_error: 0.1873\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.09065\n",
      "Epoch 363/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1106 - val_mean_absolute_error: 0.1106\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.09065\n",
      "Epoch 364/500\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 0.0766 - mean_absolute_error: 0.0766 - val_loss: 0.0983 - val_mean_absolute_error: 0.0983\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.09065\n",
      "Epoch 365/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.0743 - mean_absolute_error: 0.0743 - val_loss: 0.1040 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.09065\n",
      "Epoch 366/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.1377 - mean_absolute_error: 0.1377 - val_loss: 0.2065 - val_mean_absolute_error: 0.2065\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.09065\n",
      "Epoch 367/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.0945 - mean_absolute_error: 0.0945 - val_loss: 0.1783 - val_mean_absolute_error: 0.1783\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.09065\n",
      "Epoch 368/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0911 - mean_absolute_error: 0.0911 - val_loss: 0.1142 - val_mean_absolute_error: 0.1142\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.09065\n",
      "Epoch 369/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0670 - mean_absolute_error: 0.0670 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.09065\n",
      "Epoch 370/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.0661 - mean_absolute_error: 0.0661 - val_loss: 0.1003 - val_mean_absolute_error: 0.1003\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.09065\n",
      "Epoch 371/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.0737 - mean_absolute_error: 0.0737 - val_loss: 0.1114 - val_mean_absolute_error: 0.1114\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.09065\n",
      "Epoch 372/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.1143 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.09065\n",
      "Epoch 373/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 0.0845 - mean_absolute_error: 0.0845 - val_loss: 0.1161 - val_mean_absolute_error: 0.1161\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.09065\n",
      "Epoch 374/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.0980 - val_mean_absolute_error: 0.0980\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.09065\n",
      "Epoch 375/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0591 - mean_absolute_error: 0.0591 - val_loss: 0.2060 - val_mean_absolute_error: 0.2060\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.09065\n",
      "Epoch 376/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.2151 - mean_absolute_error: 0.2151 - val_loss: 0.1345 - val_mean_absolute_error: 0.1345\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.09065\n",
      "Epoch 377/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.0793 - mean_absolute_error: 0.0793 - val_loss: 0.1019 - val_mean_absolute_error: 0.1019\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.09065\n",
      "Epoch 378/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0926 - mean_absolute_error: 0.0926 - val_loss: 0.1005 - val_mean_absolute_error: 0.1005\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.09065\n",
      "Epoch 379/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.0756 - mean_absolute_error: 0.0756 - val_loss: 0.1483 - val_mean_absolute_error: 0.1483\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.09065\n",
      "Epoch 380/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0989 - val_mean_absolute_error: 0.0989\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.09065\n",
      "Epoch 381/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0866 - mean_absolute_error: 0.0866 - val_loss: 0.1338 - val_mean_absolute_error: 0.1338\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.09065\n",
      "Epoch 382/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0693 - mean_absolute_error: 0.0693 - val_loss: 0.1047 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.09065\n",
      "Epoch 383/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0745 - mean_absolute_error: 0.0745 - val_loss: 0.1181 - val_mean_absolute_error: 0.1181\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.09065\n",
      "Epoch 384/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.1408 - val_mean_absolute_error: 0.1408\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.09065\n",
      "Epoch 385/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.1680 - mean_absolute_error: 0.1680 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.09065\n",
      "Epoch 386/500\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 0.1495 - mean_absolute_error: 0.1495 - val_loss: 0.1345 - val_mean_absolute_error: 0.1345\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.09065\n",
      "Epoch 387/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0667 - mean_absolute_error: 0.0667 - val_loss: 0.1105 - val_mean_absolute_error: 0.1105\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.09065\n",
      "Epoch 388/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0778 - mean_absolute_error: 0.0778 - val_loss: 0.1640 - val_mean_absolute_error: 0.1640\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.09065\n",
      "Epoch 389/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0885 - mean_absolute_error: 0.0885 - val_loss: 0.2621 - val_mean_absolute_error: 0.2621\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.09065\n",
      "Epoch 390/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.1085 - mean_absolute_error: 0.1085 - val_loss: 0.1053 - val_mean_absolute_error: 0.1053\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.09065\n",
      "Epoch 391/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.0778 - mean_absolute_error: 0.0778 - val_loss: 0.1464 - val_mean_absolute_error: 0.1464\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.09065\n",
      "Epoch 392/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0973 - mean_absolute_error: 0.0973 - val_loss: 0.1011 - val_mean_absolute_error: 0.1011\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.09065\n",
      "Epoch 393/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.0832 - mean_absolute_error: 0.0832 - val_loss: 0.1338 - val_mean_absolute_error: 0.1338\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.09065\n",
      "Epoch 394/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0813 - mean_absolute_error: 0.0813 - val_loss: 0.1210 - val_mean_absolute_error: 0.1210\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.09065\n",
      "Epoch 395/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.1050 - mean_absolute_error: 0.1050 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.09065\n",
      "Epoch 396/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0695 - mean_absolute_error: 0.0695 - val_loss: 0.1504 - val_mean_absolute_error: 0.1504\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.09065\n",
      "Epoch 397/500\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.1225 - val_mean_absolute_error: 0.1225\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.09065\n",
      "Epoch 398/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0910 - mean_absolute_error: 0.0910 - val_loss: 0.1055 - val_mean_absolute_error: 0.1055\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.09065\n",
      "Epoch 399/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0887 - mean_absolute_error: 0.0887 - val_loss: 0.1420 - val_mean_absolute_error: 0.1420\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.09065\n",
      "Epoch 400/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.1030 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.09065\n",
      "Epoch 401/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0554 - mean_absolute_error: 0.0554 - val_loss: 0.1009 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.09065\n",
      "Epoch 402/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.1555 - mean_absolute_error: 0.1555 - val_loss: 0.2830 - val_mean_absolute_error: 0.2830\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.09065\n",
      "Epoch 403/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.0904 - mean_absolute_error: 0.0904 - val_loss: 0.1232 - val_mean_absolute_error: 0.1232\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.09065\n",
      "Epoch 404/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0695 - mean_absolute_error: 0.0695 - val_loss: 0.1053 - val_mean_absolute_error: 0.1053\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.09065\n",
      "Epoch 405/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 0.0912 - mean_absolute_error: 0.0912 - val_loss: 0.1068 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.09065\n",
      "Epoch 406/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0865 - mean_absolute_error: 0.0865 - val_loss: 0.1237 - val_mean_absolute_error: 0.1237\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.09065\n",
      "Epoch 407/500\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1167 - val_mean_absolute_error: 0.1167\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.09065\n",
      "Epoch 408/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0691 - mean_absolute_error: 0.0691 - val_loss: 0.1011 - val_mean_absolute_error: 0.1011\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.09065\n",
      "Epoch 409/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.0887 - mean_absolute_error: 0.0887 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.09065\n",
      "Epoch 410/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.1443 - mean_absolute_error: 0.1443 - val_loss: 0.1028 - val_mean_absolute_error: 0.1028\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.09065\n",
      "Epoch 411/500\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 0.0526 - mean_absolute_error: 0.0526 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.09065\n",
      "Epoch 412/500\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 0.0845 - mean_absolute_error: 0.0845 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.09065\n",
      "Epoch 413/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0843 - mean_absolute_error: 0.0843 - val_loss: 0.1020 - val_mean_absolute_error: 0.1020\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.09065\n",
      "Epoch 414/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.1182 - mean_absolute_error: 0.1182 - val_loss: 0.1438 - val_mean_absolute_error: 0.1438\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.09065\n",
      "Epoch 415/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0715 - mean_absolute_error: 0.0715 - val_loss: 0.1123 - val_mean_absolute_error: 0.1123\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.09065\n",
      "Epoch 416/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 0.0795 - mean_absolute_error: 0.0795 - val_loss: 0.1360 - val_mean_absolute_error: 0.1360\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.09065\n",
      "Epoch 417/500\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 0.0906 - mean_absolute_error: 0.0906 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.09065\n",
      "Epoch 418/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.0755 - mean_absolute_error: 0.0755 - val_loss: 0.1326 - val_mean_absolute_error: 0.1326\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.09065\n",
      "Epoch 419/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0770 - mean_absolute_error: 0.0770 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.09065\n",
      "Epoch 420/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0945 - mean_absolute_error: 0.0945 - val_loss: 0.1063 - val_mean_absolute_error: 0.1063\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.09065\n",
      "Epoch 421/500\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.1109 - val_mean_absolute_error: 0.1109\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.09065\n",
      "Epoch 422/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.0642 - mean_absolute_error: 0.0642 - val_loss: 0.1183 - val_mean_absolute_error: 0.1183\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.09065\n",
      "Epoch 423/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.1645 - val_mean_absolute_error: 0.1645\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.09065\n",
      "Epoch 424/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.0551 - mean_absolute_error: 0.0551 - val_loss: 0.1014 - val_mean_absolute_error: 0.1014\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.09065\n",
      "Epoch 425/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0840 - mean_absolute_error: 0.0840 - val_loss: 0.1344 - val_mean_absolute_error: 0.1344\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.09065\n",
      "Epoch 426/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1481 - mean_absolute_error: 0.1481 - val_loss: 0.1456 - val_mean_absolute_error: 0.1456\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.09065\n",
      "Epoch 427/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0813 - mean_absolute_error: 0.0813 - val_loss: 0.1760 - val_mean_absolute_error: 0.1760\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.09065\n",
      "Epoch 428/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0703 - mean_absolute_error: 0.0703 - val_loss: 0.1102 - val_mean_absolute_error: 0.1102\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.09065\n",
      "Epoch 429/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.0623 - mean_absolute_error: 0.0623 - val_loss: 0.0988 - val_mean_absolute_error: 0.0988\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.09065\n",
      "Epoch 430/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.0654 - mean_absolute_error: 0.0654 - val_loss: 0.1363 - val_mean_absolute_error: 0.1363\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.09065\n",
      "Epoch 431/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1018 - val_mean_absolute_error: 0.1018\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.09065\n",
      "Epoch 432/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0689 - mean_absolute_error: 0.0689 - val_loss: 0.1098 - val_mean_absolute_error: 0.1098\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.09065\n",
      "Epoch 433/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.1447 - mean_absolute_error: 0.1447 - val_loss: 0.1977 - val_mean_absolute_error: 0.1977\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.09065\n",
      "Epoch 434/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.1606 - mean_absolute_error: 0.1606 - val_loss: 0.1533 - val_mean_absolute_error: 0.1533\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.09065\n",
      "Epoch 435/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.09065\n",
      "Epoch 436/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0721 - mean_absolute_error: 0.0721 - val_loss: 0.1953 - val_mean_absolute_error: 0.1953\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.09065\n",
      "Epoch 437/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 0.1465 - mean_absolute_error: 0.1465 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.09065\n",
      "Epoch 438/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0568 - mean_absolute_error: 0.0568 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.09065\n",
      "Epoch 439/500\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1995 - val_mean_absolute_error: 0.1995\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.09065\n",
      "Epoch 440/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.0706 - mean_absolute_error: 0.0706 - val_loss: 0.1356 - val_mean_absolute_error: 0.1356\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.09065\n",
      "Epoch 441/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.0649 - mean_absolute_error: 0.0649 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.09065\n",
      "Epoch 442/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 0.0722 - mean_absolute_error: 0.0722 - val_loss: 0.1408 - val_mean_absolute_error: 0.1408\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.09065\n",
      "Epoch 443/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.1002 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.09065\n",
      "Epoch 444/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.09065\n",
      "Epoch 445/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 0.0819 - mean_absolute_error: 0.0819 - val_loss: 0.1187 - val_mean_absolute_error: 0.1187\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.09065\n",
      "Epoch 446/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.0886 - mean_absolute_error: 0.0886 - val_loss: 0.1152 - val_mean_absolute_error: 0.1152\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.09065\n",
      "Epoch 447/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0685 - mean_absolute_error: 0.0685 - val_loss: 0.1063 - val_mean_absolute_error: 0.1063\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.09065\n",
      "Epoch 448/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.1068 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.09065\n",
      "Epoch 449/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.0535 - mean_absolute_error: 0.0535 - val_loss: 0.1113 - val_mean_absolute_error: 0.1113\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.09065\n",
      "Epoch 450/500\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.1049 - mean_absolute_error: 0.1049 - val_loss: 0.1607 - val_mean_absolute_error: 0.1607\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.09065\n",
      "Epoch 451/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0692 - mean_absolute_error: 0.0692 - val_loss: 0.1004 - val_mean_absolute_error: 0.1004\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.09065\n",
      "Epoch 452/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0475 - mean_absolute_error: 0.0475 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.09065\n",
      "Epoch 453/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.0601 - mean_absolute_error: 0.0601 - val_loss: 0.1092 - val_mean_absolute_error: 0.1092\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.09065\n",
      "Epoch 454/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0600 - mean_absolute_error: 0.0600 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.09065\n",
      "Epoch 455/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0779 - mean_absolute_error: 0.0779 - val_loss: 0.1405 - val_mean_absolute_error: 0.1405\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.09065\n",
      "Epoch 456/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.1811 - val_mean_absolute_error: 0.1811\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.09065\n",
      "Epoch 457/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.1657 - mean_absolute_error: 0.1657 - val_loss: 0.2177 - val_mean_absolute_error: 0.2177\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.09065\n",
      "Epoch 458/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 0.1072 - mean_absolute_error: 0.1072 - val_loss: 0.1403 - val_mean_absolute_error: 0.1403\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.09065\n",
      "Epoch 459/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0704 - mean_absolute_error: 0.0704 - val_loss: 0.1060 - val_mean_absolute_error: 0.1060\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.09065\n",
      "Epoch 460/500\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0811 - mean_absolute_error: 0.0811 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.09065\n",
      "Epoch 461/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.0756 - mean_absolute_error: 0.0756 - val_loss: 0.1730 - val_mean_absolute_error: 0.1730\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.09065\n",
      "Epoch 462/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.09065\n",
      "Epoch 463/500\n",
      "1168/1168 [==============================] - 0s 231us/step - loss: 0.0997 - mean_absolute_error: 0.0997 - val_loss: 0.1007 - val_mean_absolute_error: 0.1007\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.09065\n",
      "Epoch 464/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0823 - mean_absolute_error: 0.0823 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.09065\n",
      "Epoch 465/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0651 - mean_absolute_error: 0.0651 - val_loss: 0.1657 - val_mean_absolute_error: 0.1657\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.09065\n",
      "Epoch 466/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0858 - mean_absolute_error: 0.0858 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.09065\n",
      "Epoch 467/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 0.0891 - mean_absolute_error: 0.0891 - val_loss: 0.0974 - val_mean_absolute_error: 0.0974\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.09065\n",
      "Epoch 468/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0563 - mean_absolute_error: 0.0563 - val_loss: 0.1302 - val_mean_absolute_error: 0.1302\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.09065\n",
      "Epoch 469/500\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 0.0846 - mean_absolute_error: 0.0846 - val_loss: 0.1144 - val_mean_absolute_error: 0.1144\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.09065\n",
      "Epoch 470/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 0.0786 - mean_absolute_error: 0.0786 - val_loss: 0.1199 - val_mean_absolute_error: 0.1199\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.09065\n",
      "Epoch 471/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0835 - mean_absolute_error: 0.0835 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.09065\n",
      "Epoch 472/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0526 - mean_absolute_error: 0.0526 - val_loss: 0.1000 - val_mean_absolute_error: 0.1000\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.09065\n",
      "Epoch 473/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0766 - mean_absolute_error: 0.0766 - val_loss: 0.1062 - val_mean_absolute_error: 0.1062\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.09065\n",
      "Epoch 474/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 0.0753 - mean_absolute_error: 0.0753 - val_loss: 0.1598 - val_mean_absolute_error: 0.1598\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.09065\n",
      "Epoch 475/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.0828 - mean_absolute_error: 0.0828 - val_loss: 0.1038 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.09065\n",
      "Epoch 476/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0580 - mean_absolute_error: 0.0580 - val_loss: 0.1039 - val_mean_absolute_error: 0.1039\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.09065\n",
      "Epoch 477/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 0.1472 - mean_absolute_error: 0.1472 - val_loss: 0.1228 - val_mean_absolute_error: 0.1228\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.09065\n",
      "Epoch 478/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.0665 - mean_absolute_error: 0.0665 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.09065\n",
      "Epoch 479/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0858 - mean_absolute_error: 0.0858 - val_loss: 0.1155 - val_mean_absolute_error: 0.1155\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.09065\n",
      "Epoch 480/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0851 - mean_absolute_error: 0.0851 - val_loss: 0.1106 - val_mean_absolute_error: 0.1106\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.09065\n",
      "Epoch 481/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.1242 - val_mean_absolute_error: 0.1242\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.09065\n",
      "Epoch 482/500\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.0748 - mean_absolute_error: 0.0748 - val_loss: 0.1401 - val_mean_absolute_error: 0.1401\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.09065\n",
      "Epoch 483/500\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 0.0846 - mean_absolute_error: 0.0846 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.09065\n",
      "Epoch 484/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0501 - mean_absolute_error: 0.0501 - val_loss: 0.1046 - val_mean_absolute_error: 0.1046\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.09065\n",
      "Epoch 485/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.0643 - mean_absolute_error: 0.0643 - val_loss: 0.1128 - val_mean_absolute_error: 0.1128\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.09065\n",
      "Epoch 486/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 0.0712 - mean_absolute_error: 0.0712 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.09065\n",
      "Epoch 487/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0481 - mean_absolute_error: 0.0481 - val_loss: 0.1318 - val_mean_absolute_error: 0.1318\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.09065\n",
      "Epoch 488/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0688 - mean_absolute_error: 0.0688 - val_loss: 0.1194 - val_mean_absolute_error: 0.1194\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.09065\n",
      "Epoch 489/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.0776 - mean_absolute_error: 0.0776 - val_loss: 0.1203 - val_mean_absolute_error: 0.1203\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.09065\n",
      "Epoch 490/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.1132 - val_mean_absolute_error: 0.1132\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.09065\n",
      "Epoch 491/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0694 - mean_absolute_error: 0.0694 - val_loss: 0.1101 - val_mean_absolute_error: 0.1101\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.09065\n",
      "Epoch 492/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.09065\n",
      "Epoch 493/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.1061 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.09065\n",
      "Epoch 494/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 0.1390 - mean_absolute_error: 0.1390 - val_loss: 0.1912 - val_mean_absolute_error: 0.1912\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.09065\n",
      "Epoch 495/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.1582 - mean_absolute_error: 0.1582 - val_loss: 0.1759 - val_mean_absolute_error: 0.1759\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.09065\n",
      "Epoch 496/500\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.1331 - val_mean_absolute_error: 0.1331\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.09065\n",
      "Epoch 497/500\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.1109 - mean_absolute_error: 0.1109 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.09065\n",
      "Epoch 498/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0699 - mean_absolute_error: 0.0699 - val_loss: 0.1322 - val_mean_absolute_error: 0.1322\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.09065\n",
      "Epoch 499/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 0.0773 - mean_absolute_error: 0.0773 - val_loss: 0.1104 - val_mean_absolute_error: 0.1104\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.09065\n",
      "Epoch 500/500\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1145 - val_mean_absolute_error: 0.1145\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.09065\n"
     ]
    }
   ],
   "source": [
    "history = NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "TQ9-SUuePKkF",
    "outputId": "ee2f9706-4a2a-434a-b462-dccc42e8ab8d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mean_absolute_error = history.history['mean_absolute_error']\n",
    "val_mean_absolute_error = history.history['val_mean_absolute_error']\n",
    "\n",
    "epochs = range(len(mean_absolute_error))\n",
    "\n",
    "plt.plot(epochs, mean_absolute_error, 'bo', label='Training error')\n",
    "plt.plot(epochs, val_mean_absolute_error, 'r', label='Val error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qLaHGR8HQ3n-"
   },
   "outputs": [],
   "source": [
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-184--0.09065.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0O8U-rAXRCHv"
   },
   "outputs": [],
   "source": [
    "# Make prediction on our test set\n",
    "predictions = NN_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mb0I6BJgREqT",
    "outputId": "0fdaf096-9451-495f-daba-341a94e05451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A submission file has been made\n"
     ]
    }
   ],
   "source": [
    "def make_submission(prediction, sub_name, ID):\n",
    "  my_submission = pd.DataFrame({'Id':ID,'SalePrice':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')\n",
    "\n",
    "make_submission(predictions[:,0],'submission(NN)', test_ID)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model-building-1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
