{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3VXKkY1cUSTZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "\n",
    "# Pipeline and scaling preprocessing will be used for models that are sensitive\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mi7AekTCV7wZ",
    "outputId": "a11537e1-3548-4942-f285-42c2f90b1558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 223)\n",
      "(1459, 222)\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('/content/train_new.csv')\n",
    "test=pd.read_csv('/content/test_new.csv')\n",
    "test_old=pd.read_csv('/content/test.csv')\n",
    "\n",
    "#check the shape of train and test data\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TJyy4FqQeiBA",
    "outputId": "9282409a-0571-45ee-e79b-be76e6383863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 222)\n",
      "(1459, 221)\n"
     ]
    }
   ],
   "source": [
    "#drop the first column which is the redundant\n",
    "train = train.drop(train.columns[[0]], axis=1)\n",
    "test = test.drop(test.columns[[0]], axis=1)\n",
    "\n",
    "#check the shape of train and test data\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "EDLIFlQ_d7sv",
    "outputId": "d54d41bb-638c-49e2-b04e-3d6331421a6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>Alley</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>Fence</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>Functional</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_NoRidge</th>\n",
       "      <th>Neighborhood_NridgHt</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Neighborhood_SWISU</th>\n",
       "      <th>Neighborhood_Sawyer</th>\n",
       "      <th>Neighborhood_SawyerW</th>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "      <th>Neighborhood_StoneBr</th>\n",
       "      <th>Neighborhood_Timber</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>RoofMatl_ClyTile</th>\n",
       "      <th>RoofMatl_CompShg</th>\n",
       "      <th>RoofMatl_Membran</th>\n",
       "      <th>RoofMatl_Metal</th>\n",
       "      <th>RoofMatl_Roll</th>\n",
       "      <th>RoofMatl_Tar&amp;Grv</th>\n",
       "      <th>RoofMatl_WdShake</th>\n",
       "      <th>RoofMatl_WdShngl</th>\n",
       "      <th>RoofStyle_Flat</th>\n",
       "      <th>RoofStyle_Gable</th>\n",
       "      <th>RoofStyle_Gambrel</th>\n",
       "      <th>RoofStyle_Hip</th>\n",
       "      <th>RoofStyle_Mansard</th>\n",
       "      <th>RoofStyle_Shed</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleType_COD</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.818680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>10.105326</td>\n",
       "      <td>7.397498</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>8.780689</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>11.259868</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.121376</td>\n",
       "      <td>11.818680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.479373</td>\n",
       "      <td>6.221214</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.943735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>11.901094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>9.752379</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>9.118181</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.116605</td>\n",
       "      <td>12.943735</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.327220</td>\n",
       "      <td>6.244956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.916131</td>\n",
       "      <td>11.151348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>11.476685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>7.293500</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>10.179489</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.178149</td>\n",
       "      <td>13.551272</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.196905</td>\n",
       "      <td>6.073289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.910125</td>\n",
       "      <td>11.062536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>10.749651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>9.207511</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>10.116035</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.179714</td>\n",
       "      <td>13.504453</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.865444</td>\n",
       "      <td>6.172972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.833625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>8.720170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>12.172900</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>10.302477</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.170316</td>\n",
       "      <td>12.833625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.257255</td>\n",
       "      <td>5.093857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>10.496872</td>\n",
       "      <td>10.496872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>10.496872</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.374860</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.081426</td>\n",
       "      <td>3.932510</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>10.496872</td>\n",
       "      <td>10.496872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>8.622254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>8.978567</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>8.914179</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.135652</td>\n",
       "      <td>12.374860</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.013314</td>\n",
       "      <td>3.932510</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>12.703313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>12.703313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>10.634887</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.119786</td>\n",
       "      <td>12.703313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.782058</td>\n",
       "      <td>7.620056</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>12.039794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.301176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>10.630386</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.039794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.046557</td>\n",
       "      <td>5.744420</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>12.114087</td>\n",
       "      <td>12.136615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.361228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>8.492259</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>10.950899</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.055642</td>\n",
       "      <td>14.171884</td>\n",
       "      <td>14.182841</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>19.723319</td>\n",
       "      <td>6.073289</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1stFlrSF   2ndFlrSF  3SsnPorch  ...  SaleType_Oth  SaleType_WD  ID\n",
       "0     11.818680   0.000000        0.0  ...             0            1 NaN\n",
       "1     12.943735   0.000000        0.0  ...             0            1 NaN\n",
       "2     11.916131  11.151348        0.0  ...             0            1 NaN\n",
       "3     11.910125  11.062536        0.0  ...             0            1 NaN\n",
       "4     12.833625   0.000000        0.0  ...             0            1 NaN\n",
       "...         ...        ...        ...  ...           ...          ...  ..\n",
       "1454  10.496872  10.496872        0.0  ...             0            1 NaN\n",
       "1455  10.496872  10.496872        0.0  ...             0            1 NaN\n",
       "1456  12.703313   0.000000        0.0  ...             0            1 NaN\n",
       "1457  12.039794   0.000000        0.0  ...             0            1 NaN\n",
       "1458  12.114087  12.136615        0.0  ...             0            1 NaN\n",
       "\n",
       "[1459 rows x 221 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HCv7ihfogl-4"
   },
   "outputs": [],
   "source": [
    "#save ID \n",
    "train_ID = train['ID']\n",
    "test_ID = test_old['Id']\n",
    "# FIx this bug\n",
    "train.drop(\"ID\", axis = 1, inplace = True)\n",
    "test.drop(\"ID\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "jB3RpLFQxPdA",
    "outputId": "79909465-7b9b-4a48-9239-76ca182b7225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          2\n",
       "2          3\n",
       "3          4\n",
       "4          5\n",
       "        ... \n",
       "1455    1456\n",
       "1456    1457\n",
       "1457    1458\n",
       "1458    1459\n",
       "1459    1460\n",
       "Name: ID, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ksDJyrOWtfZe"
   },
   "outputs": [],
   "source": [
    "# Now we use the train to do the model building\n",
    "target = train[\"SalePrice\"]\n",
    "# Define the variable\n",
    "df_1 = train.copy()\n",
    "df_1.drop(\"SalePrice\", axis = 1, inplace = True)\n",
    "train_var = df_1\n",
    "\n",
    "# train_var,target,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2QlS8CsuuSqv",
    "outputId": "9cb5a244-2798-486c-eead-fdcaa9fcc94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460,)\n",
      "(1460, 220)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(train_var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgJWiy2yrDBV"
   },
   "source": [
    "# Model 1: Nerual Network for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tfw3G1oLiR9X"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "c-eI0B3prvTc",
    "outputId": "8eb84c5e-10fc-4d2d-a6a2-cc900c6a764f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               28288     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 193,153\n",
      "Trainable params: 193,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the NN network\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train_var.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lIhU6zUzsCLs"
   },
   "outputs": [],
   "source": [
    "# set a check point \n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "f5wfM1b9skth",
    "outputId": "9ed06eb6-047a-4bda-8b20-4942e250af7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1168/1168 [==============================] - 5s 4ms/step - loss: 179419.9677 - mean_absolute_error: 179419.9677 - val_loss: 173439.4253 - val_mean_absolute_error: 173439.4253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 173439.42530, saving model to Weights-001--173439.42530.hdf5\n",
      "Epoch 2/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 99702.4468 - mean_absolute_error: 99702.4468 - val_loss: 47439.6064 - val_mean_absolute_error: 47439.6064\n",
      "\n",
      "Epoch 00002: val_loss improved from 173439.42530 to 47439.60643, saving model to Weights-002--47439.60643.hdf5\n",
      "Epoch 3/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 46534.9655 - mean_absolute_error: 46534.9655 - val_loss: 44419.6706 - val_mean_absolute_error: 44419.6706\n",
      "\n",
      "Epoch 00003: val_loss improved from 47439.60643 to 44419.67059, saving model to Weights-003--44419.67059.hdf5\n",
      "Epoch 4/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 42394.4415 - mean_absolute_error: 42394.4415 - val_loss: 41596.4734 - val_mean_absolute_error: 41596.4734\n",
      "\n",
      "Epoch 00004: val_loss improved from 44419.67059 to 41596.47335, saving model to Weights-004--41596.47335.hdf5\n",
      "Epoch 5/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 39566.3182 - mean_absolute_error: 39566.3182 - val_loss: 38980.2846 - val_mean_absolute_error: 38980.2846\n",
      "\n",
      "Epoch 00005: val_loss improved from 41596.47335 to 38980.28462, saving model to Weights-005--38980.28462.hdf5\n",
      "Epoch 6/500\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 36596.8229 - mean_absolute_error: 36596.8229 - val_loss: 36505.6145 - val_mean_absolute_error: 36505.6145\n",
      "\n",
      "Epoch 00006: val_loss improved from 38980.28462 to 36505.61446, saving model to Weights-006--36505.61446.hdf5\n",
      "Epoch 7/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 34094.8528 - mean_absolute_error: 34094.8528 - val_loss: 34625.0538 - val_mean_absolute_error: 34625.0538\n",
      "\n",
      "Epoch 00007: val_loss improved from 36505.61446 to 34625.05383, saving model to Weights-007--34625.05383.hdf5\n",
      "Epoch 8/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 32370.4222 - mean_absolute_error: 32370.4222 - val_loss: 33630.8022 - val_mean_absolute_error: 33630.8022\n",
      "\n",
      "Epoch 00008: val_loss improved from 34625.05383 to 33630.80223, saving model to Weights-008--33630.80223.hdf5\n",
      "Epoch 9/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 31230.5387 - mean_absolute_error: 31230.5387 - val_loss: 32667.8770 - val_mean_absolute_error: 32667.8770\n",
      "\n",
      "Epoch 00009: val_loss improved from 33630.80223 to 32667.87703, saving model to Weights-009--32667.87703.hdf5\n",
      "Epoch 10/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 30394.6440 - mean_absolute_error: 30394.6440 - val_loss: 31794.1128 - val_mean_absolute_error: 31794.1128\n",
      "\n",
      "Epoch 00010: val_loss improved from 32667.87703 to 31794.11280, saving model to Weights-010--31794.11280.hdf5\n",
      "Epoch 11/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 29481.5299 - mean_absolute_error: 29481.5299 - val_loss: 30750.1327 - val_mean_absolute_error: 30750.1327\n",
      "\n",
      "Epoch 00011: val_loss improved from 31794.11280 to 30750.13271, saving model to Weights-011--30750.13271.hdf5\n",
      "Epoch 12/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 28422.1249 - mean_absolute_error: 28422.1249 - val_loss: 30512.2802 - val_mean_absolute_error: 30512.2802\n",
      "\n",
      "Epoch 00012: val_loss improved from 30750.13271 to 30512.28023, saving model to Weights-012--30512.28023.hdf5\n",
      "Epoch 13/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 27763.5133 - mean_absolute_error: 27763.5133 - val_loss: 29974.7796 - val_mean_absolute_error: 29974.7796\n",
      "\n",
      "Epoch 00013: val_loss improved from 30512.28023 to 29974.77959, saving model to Weights-013--29974.77959.hdf5\n",
      "Epoch 14/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 26838.0874 - mean_absolute_error: 26838.0874 - val_loss: 27976.4789 - val_mean_absolute_error: 27976.4789\n",
      "\n",
      "Epoch 00014: val_loss improved from 29974.77959 to 27976.47886, saving model to Weights-014--27976.47886.hdf5\n",
      "Epoch 15/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 25533.6262 - mean_absolute_error: 25533.6262 - val_loss: 27484.9034 - val_mean_absolute_error: 27484.9034\n",
      "\n",
      "Epoch 00015: val_loss improved from 27976.47886 to 27484.90339, saving model to Weights-015--27484.90339.hdf5\n",
      "Epoch 16/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 24694.6162 - mean_absolute_error: 24694.6162 - val_loss: 26565.9041 - val_mean_absolute_error: 26565.9041\n",
      "\n",
      "Epoch 00016: val_loss improved from 27484.90339 to 26565.90414, saving model to Weights-016--26565.90414.hdf5\n",
      "Epoch 17/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 24102.1411 - mean_absolute_error: 24102.1411 - val_loss: 26824.2059 - val_mean_absolute_error: 26824.2059\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 26565.90414\n",
      "Epoch 18/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 23386.1768 - mean_absolute_error: 23386.1768 - val_loss: 25338.3051 - val_mean_absolute_error: 25338.3051\n",
      "\n",
      "Epoch 00018: val_loss improved from 26565.90414 to 25338.30512, saving model to Weights-018--25338.30512.hdf5\n",
      "Epoch 19/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 22602.3484 - mean_absolute_error: 22602.3484 - val_loss: 24620.8405 - val_mean_absolute_error: 24620.8405\n",
      "\n",
      "Epoch 00019: val_loss improved from 25338.30512 to 24620.84049, saving model to Weights-019--24620.84049.hdf5\n",
      "Epoch 20/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 21802.2044 - mean_absolute_error: 21802.2044 - val_loss: 24611.0667 - val_mean_absolute_error: 24611.0667\n",
      "\n",
      "Epoch 00020: val_loss improved from 24620.84049 to 24611.06673, saving model to Weights-020--24611.06673.hdf5\n",
      "Epoch 21/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 21265.7045 - mean_absolute_error: 21265.7045 - val_loss: 23854.1733 - val_mean_absolute_error: 23854.1733\n",
      "\n",
      "Epoch 00021: val_loss improved from 24611.06673 to 23854.17332, saving model to Weights-021--23854.17332.hdf5\n",
      "Epoch 22/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 20661.9714 - mean_absolute_error: 20661.9714 - val_loss: 25709.9636 - val_mean_absolute_error: 25709.9636\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 23854.17332\n",
      "Epoch 23/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 21048.6266 - mean_absolute_error: 21048.6266 - val_loss: 23266.1524 - val_mean_absolute_error: 23266.1524\n",
      "\n",
      "Epoch 00023: val_loss improved from 23854.17332 to 23266.15240, saving model to Weights-023--23266.15240.hdf5\n",
      "Epoch 24/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 19632.2624 - mean_absolute_error: 19632.2624 - val_loss: 23797.9463 - val_mean_absolute_error: 23797.9463\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23266.15240\n",
      "Epoch 25/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 19733.9336 - mean_absolute_error: 19733.9336 - val_loss: 22713.0980 - val_mean_absolute_error: 22713.0980\n",
      "\n",
      "Epoch 00025: val_loss improved from 23266.15240 to 22713.09803, saving model to Weights-025--22713.09803.hdf5\n",
      "Epoch 26/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 19495.8838 - mean_absolute_error: 19495.8838 - val_loss: 22305.7697 - val_mean_absolute_error: 22305.7697\n",
      "\n",
      "Epoch 00026: val_loss improved from 22713.09803 to 22305.76969, saving model to Weights-026--22305.76969.hdf5\n",
      "Epoch 27/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 19208.9005 - mean_absolute_error: 19208.9005 - val_loss: 22478.0862 - val_mean_absolute_error: 22478.0862\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22305.76969\n",
      "Epoch 28/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 19508.5362 - mean_absolute_error: 19508.5362 - val_loss: 22469.3854 - val_mean_absolute_error: 22469.3854\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22305.76969\n",
      "Epoch 29/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 18441.4734 - mean_absolute_error: 18441.4734 - val_loss: 21959.8880 - val_mean_absolute_error: 21959.8880\n",
      "\n",
      "Epoch 00029: val_loss improved from 22305.76969 to 21959.88798, saving model to Weights-029--21959.88798.hdf5\n",
      "Epoch 30/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 18475.5911 - mean_absolute_error: 18475.5911 - val_loss: 21598.6335 - val_mean_absolute_error: 21598.6335\n",
      "\n",
      "Epoch 00030: val_loss improved from 21959.88798 to 21598.63348, saving model to Weights-030--21598.63348.hdf5\n",
      "Epoch 31/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 17913.4891 - mean_absolute_error: 17913.4891 - val_loss: 21238.3171 - val_mean_absolute_error: 21238.3171\n",
      "\n",
      "Epoch 00031: val_loss improved from 21598.63348 to 21238.31710, saving model to Weights-031--21238.31710.hdf5\n",
      "Epoch 32/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 18319.7397 - mean_absolute_error: 18319.7397 - val_loss: 21059.8863 - val_mean_absolute_error: 21059.8863\n",
      "\n",
      "Epoch 00032: val_loss improved from 21238.31710 to 21059.88626, saving model to Weights-032--21059.88626.hdf5\n",
      "Epoch 33/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 17909.6668 - mean_absolute_error: 17909.6668 - val_loss: 20963.6568 - val_mean_absolute_error: 20963.6568\n",
      "\n",
      "Epoch 00033: val_loss improved from 21059.88626 to 20963.65676, saving model to Weights-033--20963.65676.hdf5\n",
      "Epoch 34/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 17453.3243 - mean_absolute_error: 17453.3243 - val_loss: 21345.5057 - val_mean_absolute_error: 21345.5057\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 20963.65676\n",
      "Epoch 35/500\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 17122.8627 - mean_absolute_error: 17122.8627 - val_loss: 20829.8644 - val_mean_absolute_error: 20829.8644\n",
      "\n",
      "Epoch 00035: val_loss improved from 20963.65676 to 20829.86440, saving model to Weights-035--20829.86440.hdf5\n",
      "Epoch 36/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 17025.6554 - mean_absolute_error: 17025.6554 - val_loss: 24192.3523 - val_mean_absolute_error: 24192.3523\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 20829.86440\n",
      "Epoch 37/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 17393.5385 - mean_absolute_error: 17393.5385 - val_loss: 20847.0223 - val_mean_absolute_error: 20847.0223\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 20829.86440\n",
      "Epoch 38/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 17271.2114 - mean_absolute_error: 17271.2114 - val_loss: 21179.5306 - val_mean_absolute_error: 21179.5306\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 20829.86440\n",
      "Epoch 39/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 17040.7217 - mean_absolute_error: 17040.7217 - val_loss: 20730.4112 - val_mean_absolute_error: 20730.4112\n",
      "\n",
      "Epoch 00039: val_loss improved from 20829.86440 to 20730.41123, saving model to Weights-039--20730.41123.hdf5\n",
      "Epoch 40/500\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 16379.9220 - mean_absolute_error: 16379.9220 - val_loss: 20315.0247 - val_mean_absolute_error: 20315.0247\n",
      "\n",
      "Epoch 00040: val_loss improved from 20730.41123 to 20315.02475, saving model to Weights-040--20315.02475.hdf5\n",
      "Epoch 41/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 16191.1471 - mean_absolute_error: 16191.1471 - val_loss: 20632.9980 - val_mean_absolute_error: 20632.9980\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 20315.02475\n",
      "Epoch 42/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 16219.8474 - mean_absolute_error: 16219.8474 - val_loss: 20395.4751 - val_mean_absolute_error: 20395.4751\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 20315.02475\n",
      "Epoch 43/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 16556.2225 - mean_absolute_error: 16556.2225 - val_loss: 22785.9834 - val_mean_absolute_error: 22785.9834\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 20315.02475\n",
      "Epoch 44/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 16305.1737 - mean_absolute_error: 16305.1737 - val_loss: 20277.0381 - val_mean_absolute_error: 20277.0381\n",
      "\n",
      "Epoch 00044: val_loss improved from 20315.02475 to 20277.03807, saving model to Weights-044--20277.03807.hdf5\n",
      "Epoch 45/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 16767.0777 - mean_absolute_error: 16767.0777 - val_loss: 20498.1185 - val_mean_absolute_error: 20498.1185\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 20277.03807\n",
      "Epoch 46/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 15969.7538 - mean_absolute_error: 15969.7538 - val_loss: 20304.9576 - val_mean_absolute_error: 20304.9576\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 20277.03807\n",
      "Epoch 47/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 15737.5971 - mean_absolute_error: 15737.5971 - val_loss: 20011.6678 - val_mean_absolute_error: 20011.6678\n",
      "\n",
      "Epoch 00047: val_loss improved from 20277.03807 to 20011.66781, saving model to Weights-047--20011.66781.hdf5\n",
      "Epoch 48/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 15588.3110 - mean_absolute_error: 15588.3110 - val_loss: 20160.3030 - val_mean_absolute_error: 20160.3030\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 20011.66781\n",
      "Epoch 49/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 15429.3319 - mean_absolute_error: 15429.3319 - val_loss: 19948.4122 - val_mean_absolute_error: 19948.4122\n",
      "\n",
      "Epoch 00049: val_loss improved from 20011.66781 to 19948.41222, saving model to Weights-049--19948.41222.hdf5\n",
      "Epoch 50/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 16145.8915 - mean_absolute_error: 16145.8915 - val_loss: 19817.7336 - val_mean_absolute_error: 19817.7336\n",
      "\n",
      "Epoch 00050: val_loss improved from 19948.41222 to 19817.73357, saving model to Weights-050--19817.73357.hdf5\n",
      "Epoch 51/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 15936.6943 - mean_absolute_error: 15936.6943 - val_loss: 20811.4513 - val_mean_absolute_error: 20811.4513\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 19817.73357\n",
      "Epoch 52/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 15579.8885 - mean_absolute_error: 15579.8885 - val_loss: 21084.1865 - val_mean_absolute_error: 21084.1865\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 19817.73357\n",
      "Epoch 53/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 15608.9682 - mean_absolute_error: 15608.9682 - val_loss: 19638.3376 - val_mean_absolute_error: 19638.3376\n",
      "\n",
      "Epoch 00053: val_loss improved from 19817.73357 to 19638.33765, saving model to Weights-053--19638.33765.hdf5\n",
      "Epoch 54/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 15155.0909 - mean_absolute_error: 15155.0909 - val_loss: 19890.0913 - val_mean_absolute_error: 19890.0913\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 19638.33765\n",
      "Epoch 55/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 15596.8148 - mean_absolute_error: 15596.8148 - val_loss: 21792.4880 - val_mean_absolute_error: 21792.4880\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 19638.33765\n",
      "Epoch 56/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 15514.2369 - mean_absolute_error: 15514.2369 - val_loss: 19647.1594 - val_mean_absolute_error: 19647.1594\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 19638.33765\n",
      "Epoch 57/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 15341.7619 - mean_absolute_error: 15341.7619 - val_loss: 19658.4624 - val_mean_absolute_error: 19658.4624\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 19638.33765\n",
      "Epoch 58/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 14711.8634 - mean_absolute_error: 14711.8634 - val_loss: 19798.2083 - val_mean_absolute_error: 19798.2083\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 19638.33765\n",
      "Epoch 59/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 14348.4018 - mean_absolute_error: 14348.4018 - val_loss: 20278.6532 - val_mean_absolute_error: 20278.6532\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 19638.33765\n",
      "Epoch 60/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 15334.4805 - mean_absolute_error: 15334.4805 - val_loss: 19702.4362 - val_mean_absolute_error: 19702.4362\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 19638.33765\n",
      "Epoch 61/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 15423.9289 - mean_absolute_error: 15423.9289 - val_loss: 19855.7193 - val_mean_absolute_error: 19855.7193\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 19638.33765\n",
      "Epoch 62/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 14564.0481 - mean_absolute_error: 14564.0481 - val_loss: 20042.8188 - val_mean_absolute_error: 20042.8188\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 19638.33765\n",
      "Epoch 63/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 14331.8173 - mean_absolute_error: 14331.8173 - val_loss: 20130.5345 - val_mean_absolute_error: 20130.5345\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 19638.33765\n",
      "Epoch 64/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 14103.9031 - mean_absolute_error: 14103.9031 - val_loss: 19806.7318 - val_mean_absolute_error: 19806.7318\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 19638.33765\n",
      "Epoch 65/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 14046.5984 - mean_absolute_error: 14046.5984 - val_loss: 19422.1343 - val_mean_absolute_error: 19422.1343\n",
      "\n",
      "Epoch 00065: val_loss improved from 19638.33765 to 19422.13428, saving model to Weights-065--19422.13428.hdf5\n",
      "Epoch 66/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 13952.0421 - mean_absolute_error: 13952.0421 - val_loss: 19700.7336 - val_mean_absolute_error: 19700.7336\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 19422.13428\n",
      "Epoch 67/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 13918.2676 - mean_absolute_error: 13918.2676 - val_loss: 19533.0037 - val_mean_absolute_error: 19533.0037\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 19422.13428\n",
      "Epoch 68/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 13777.1327 - mean_absolute_error: 13777.1327 - val_loss: 19415.5107 - val_mean_absolute_error: 19415.5107\n",
      "\n",
      "Epoch 00068: val_loss improved from 19422.13428 to 19415.51070, saving model to Weights-068--19415.51070.hdf5\n",
      "Epoch 69/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 14006.2312 - mean_absolute_error: 14006.2312 - val_loss: 20774.7663 - val_mean_absolute_error: 20774.7663\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 19415.51070\n",
      "Epoch 70/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 13862.2625 - mean_absolute_error: 13862.2625 - val_loss: 20018.8871 - val_mean_absolute_error: 20018.8871\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 19415.51070\n",
      "Epoch 71/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 13781.1385 - mean_absolute_error: 13781.1385 - val_loss: 19331.6283 - val_mean_absolute_error: 19331.6283\n",
      "\n",
      "Epoch 00071: val_loss improved from 19415.51070 to 19331.62826, saving model to Weights-071--19331.62826.hdf5\n",
      "Epoch 72/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 13634.3071 - mean_absolute_error: 13634.3071 - val_loss: 20039.9584 - val_mean_absolute_error: 20039.9584\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 19331.62826\n",
      "Epoch 73/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 14383.0077 - mean_absolute_error: 14383.0077 - val_loss: 20400.3042 - val_mean_absolute_error: 20400.3042\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 19331.62826\n",
      "Epoch 74/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 14006.7373 - mean_absolute_error: 14006.7373 - val_loss: 19125.4877 - val_mean_absolute_error: 19125.4877\n",
      "\n",
      "Epoch 00074: val_loss improved from 19331.62826 to 19125.48775, saving model to Weights-074--19125.48775.hdf5\n",
      "Epoch 75/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 13566.1869 - mean_absolute_error: 13566.1869 - val_loss: 19044.3522 - val_mean_absolute_error: 19044.3522\n",
      "\n",
      "Epoch 00075: val_loss improved from 19125.48775 to 19044.35218, saving model to Weights-075--19044.35218.hdf5\n",
      "Epoch 76/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 13478.6094 - mean_absolute_error: 13478.6094 - val_loss: 19042.1024 - val_mean_absolute_error: 19042.1024\n",
      "\n",
      "Epoch 00076: val_loss improved from 19044.35218 to 19042.10242, saving model to Weights-076--19042.10242.hdf5\n",
      "Epoch 77/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 13214.1886 - mean_absolute_error: 13214.1886 - val_loss: 19534.1077 - val_mean_absolute_error: 19534.1077\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 19042.10242\n",
      "Epoch 78/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 13323.8134 - mean_absolute_error: 13323.8134 - val_loss: 19115.4046 - val_mean_absolute_error: 19115.4046\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 19042.10242\n",
      "Epoch 79/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 13517.8691 - mean_absolute_error: 13517.8691 - val_loss: 19038.2550 - val_mean_absolute_error: 19038.2550\n",
      "\n",
      "Epoch 00079: val_loss improved from 19042.10242 to 19038.25503, saving model to Weights-079--19038.25503.hdf5\n",
      "Epoch 80/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 13510.7663 - mean_absolute_error: 13510.7663 - val_loss: 20285.1615 - val_mean_absolute_error: 20285.1615\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 19038.25503\n",
      "Epoch 81/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 13958.7608 - mean_absolute_error: 13958.7608 - val_loss: 19095.7777 - val_mean_absolute_error: 19095.7777\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 19038.25503\n",
      "Epoch 82/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 12830.5196 - mean_absolute_error: 12830.5196 - val_loss: 18984.4233 - val_mean_absolute_error: 18984.4233\n",
      "\n",
      "Epoch 00082: val_loss improved from 19038.25503 to 18984.42329, saving model to Weights-082--18984.42329.hdf5\n",
      "Epoch 83/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 12969.8754 - mean_absolute_error: 12969.8754 - val_loss: 19263.6252 - val_mean_absolute_error: 19263.6252\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 18984.42329\n",
      "Epoch 84/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 13013.1007 - mean_absolute_error: 13013.1007 - val_loss: 18808.6603 - val_mean_absolute_error: 18808.6603\n",
      "\n",
      "Epoch 00084: val_loss improved from 18984.42329 to 18808.66034, saving model to Weights-084--18808.66034.hdf5\n",
      "Epoch 85/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 12630.0775 - mean_absolute_error: 12630.0775 - val_loss: 19036.6583 - val_mean_absolute_error: 19036.6583\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 18808.66034\n",
      "Epoch 86/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 12910.2611 - mean_absolute_error: 12910.2611 - val_loss: 19385.2320 - val_mean_absolute_error: 19385.2320\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 18808.66034\n",
      "Epoch 87/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 12627.1917 - mean_absolute_error: 12627.1917 - val_loss: 18908.3792 - val_mean_absolute_error: 18908.3792\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 18808.66034\n",
      "Epoch 88/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 13450.0872 - mean_absolute_error: 13450.0872 - val_loss: 19043.2174 - val_mean_absolute_error: 19043.2174\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 18808.66034\n",
      "Epoch 89/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 13069.2897 - mean_absolute_error: 13069.2897 - val_loss: 18905.8463 - val_mean_absolute_error: 18905.8463\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 18808.66034\n",
      "Epoch 90/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 12534.8583 - mean_absolute_error: 12534.8583 - val_loss: 19084.1252 - val_mean_absolute_error: 19084.1252\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 18808.66034\n",
      "Epoch 91/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 12550.6196 - mean_absolute_error: 12550.6196 - val_loss: 19194.3629 - val_mean_absolute_error: 19194.3629\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 18808.66034\n",
      "Epoch 92/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 12897.8653 - mean_absolute_error: 12897.8653 - val_loss: 19414.4031 - val_mean_absolute_error: 19414.4031\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 18808.66034\n",
      "Epoch 93/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 12461.4252 - mean_absolute_error: 12461.4252 - val_loss: 18950.0234 - val_mean_absolute_error: 18950.0234\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 18808.66034\n",
      "Epoch 94/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 12255.9369 - mean_absolute_error: 12255.9369 - val_loss: 18787.5099 - val_mean_absolute_error: 18787.5099\n",
      "\n",
      "Epoch 00094: val_loss improved from 18808.66034 to 18787.50990, saving model to Weights-094--18787.50990.hdf5\n",
      "Epoch 95/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 12224.6658 - mean_absolute_error: 12224.6658 - val_loss: 19142.1648 - val_mean_absolute_error: 19142.1648\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 18787.50990\n",
      "Epoch 96/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 12238.7179 - mean_absolute_error: 12238.7179 - val_loss: 18727.0226 - val_mean_absolute_error: 18727.0226\n",
      "\n",
      "Epoch 00096: val_loss improved from 18787.50990 to 18727.02255, saving model to Weights-096--18727.02255.hdf5\n",
      "Epoch 97/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 12372.2239 - mean_absolute_error: 12372.2239 - val_loss: 18911.1633 - val_mean_absolute_error: 18911.1633\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 18727.02255\n",
      "Epoch 98/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 12575.4017 - mean_absolute_error: 12575.4017 - val_loss: 18667.8889 - val_mean_absolute_error: 18667.8889\n",
      "\n",
      "Epoch 00098: val_loss improved from 18727.02255 to 18667.88889, saving model to Weights-098--18667.88889.hdf5\n",
      "Epoch 99/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 12074.5119 - mean_absolute_error: 12074.5119 - val_loss: 18779.6738 - val_mean_absolute_error: 18779.6738\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 18667.88889\n",
      "Epoch 100/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 12162.6553 - mean_absolute_error: 12162.6553 - val_loss: 18791.6890 - val_mean_absolute_error: 18791.6890\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 18667.88889\n",
      "Epoch 101/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 11603.0865 - mean_absolute_error: 11603.0865 - val_loss: 18728.9352 - val_mean_absolute_error: 18728.9352\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 18667.88889\n",
      "Epoch 102/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 12069.7536 - mean_absolute_error: 12069.7536 - val_loss: 18805.4525 - val_mean_absolute_error: 18805.4525\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 18667.88889\n",
      "Epoch 103/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 11785.5084 - mean_absolute_error: 11785.5084 - val_loss: 19098.2800 - val_mean_absolute_error: 19098.2800\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 18667.88889\n",
      "Epoch 104/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 11628.4375 - mean_absolute_error: 11628.4375 - val_loss: 18861.9516 - val_mean_absolute_error: 18861.9516\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 18667.88889\n",
      "Epoch 105/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 11502.0114 - mean_absolute_error: 11502.0114 - val_loss: 18796.8075 - val_mean_absolute_error: 18796.8075\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 18667.88889\n",
      "Epoch 106/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 12894.2349 - mean_absolute_error: 12894.2349 - val_loss: 19656.8791 - val_mean_absolute_error: 19656.8791\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 18667.88889\n",
      "Epoch 107/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 11817.2534 - mean_absolute_error: 11817.2534 - val_loss: 18618.2650 - val_mean_absolute_error: 18618.2650\n",
      "\n",
      "Epoch 00107: val_loss improved from 18667.88889 to 18618.26498, saving model to Weights-107--18618.26498.hdf5\n",
      "Epoch 108/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 11458.9057 - mean_absolute_error: 11458.9057 - val_loss: 18516.4584 - val_mean_absolute_error: 18516.4584\n",
      "\n",
      "Epoch 00108: val_loss improved from 18618.26498 to 18516.45840, saving model to Weights-108--18516.45840.hdf5\n",
      "Epoch 109/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 11565.7376 - mean_absolute_error: 11565.7376 - val_loss: 18607.1740 - val_mean_absolute_error: 18607.1740\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 18516.45840\n",
      "Epoch 110/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 11859.6435 - mean_absolute_error: 11859.6435 - val_loss: 18962.7099 - val_mean_absolute_error: 18962.7099\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 18516.45840\n",
      "Epoch 111/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 11293.8521 - mean_absolute_error: 11293.8521 - val_loss: 18605.8051 - val_mean_absolute_error: 18605.8051\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 18516.45840\n",
      "Epoch 112/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 11397.7327 - mean_absolute_error: 11397.7327 - val_loss: 18681.6219 - val_mean_absolute_error: 18681.6219\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 18516.45840\n",
      "Epoch 113/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 11700.7492 - mean_absolute_error: 11700.7492 - val_loss: 18576.3825 - val_mean_absolute_error: 18576.3825\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 18516.45840\n",
      "Epoch 114/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 11483.6339 - mean_absolute_error: 11483.6339 - val_loss: 18554.4441 - val_mean_absolute_error: 18554.4441\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 18516.45840\n",
      "Epoch 115/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 11353.8189 - mean_absolute_error: 11353.8189 - val_loss: 18685.5771 - val_mean_absolute_error: 18685.5771\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 18516.45840\n",
      "Epoch 116/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 11069.2748 - mean_absolute_error: 11069.2748 - val_loss: 19112.6121 - val_mean_absolute_error: 19112.6121\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 18516.45840\n",
      "Epoch 117/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 12037.0419 - mean_absolute_error: 12037.0419 - val_loss: 18251.5765 - val_mean_absolute_error: 18251.5765\n",
      "\n",
      "Epoch 00117: val_loss improved from 18516.45840 to 18251.57647, saving model to Weights-117--18251.57647.hdf5\n",
      "Epoch 118/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 11081.6899 - mean_absolute_error: 11081.6899 - val_loss: 18377.4514 - val_mean_absolute_error: 18377.4514\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 18251.57647\n",
      "Epoch 119/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 10778.9398 - mean_absolute_error: 10778.9398 - val_loss: 18635.5804 - val_mean_absolute_error: 18635.5804\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 18251.57647\n",
      "Epoch 120/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 10917.0884 - mean_absolute_error: 10917.0884 - val_loss: 18890.8056 - val_mean_absolute_error: 18890.8056\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 18251.57647\n",
      "Epoch 121/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 11057.9391 - mean_absolute_error: 11057.9391 - val_loss: 18564.2271 - val_mean_absolute_error: 18564.2271\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 18251.57647\n",
      "Epoch 122/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 11100.6180 - mean_absolute_error: 11100.6180 - val_loss: 18586.1536 - val_mean_absolute_error: 18586.1536\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 18251.57647\n",
      "Epoch 123/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 10730.3402 - mean_absolute_error: 10730.3402 - val_loss: 18668.3307 - val_mean_absolute_error: 18668.3307\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 18251.57647\n",
      "Epoch 124/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 11240.1061 - mean_absolute_error: 11240.1061 - val_loss: 18688.9435 - val_mean_absolute_error: 18688.9435\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 18251.57647\n",
      "Epoch 125/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 10672.9019 - mean_absolute_error: 10672.9019 - val_loss: 18272.1438 - val_mean_absolute_error: 18272.1438\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 18251.57647\n",
      "Epoch 126/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 10875.2647 - mean_absolute_error: 10875.2647 - val_loss: 18314.0928 - val_mean_absolute_error: 18314.0928\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 18251.57647\n",
      "Epoch 127/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 10783.9931 - mean_absolute_error: 10783.9931 - val_loss: 18360.4891 - val_mean_absolute_error: 18360.4891\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 18251.57647\n",
      "Epoch 128/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 10984.4961 - mean_absolute_error: 10984.4961 - val_loss: 18354.3234 - val_mean_absolute_error: 18354.3234\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 18251.57647\n",
      "Epoch 129/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 10586.6871 - mean_absolute_error: 10586.6871 - val_loss: 18679.4934 - val_mean_absolute_error: 18679.4934\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 18251.57647\n",
      "Epoch 130/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 10495.8120 - mean_absolute_error: 10495.8120 - val_loss: 18364.2061 - val_mean_absolute_error: 18364.2061\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 18251.57647\n",
      "Epoch 131/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 10429.4815 - mean_absolute_error: 10429.4815 - val_loss: 18214.3011 - val_mean_absolute_error: 18214.3011\n",
      "\n",
      "Epoch 00131: val_loss improved from 18251.57647 to 18214.30108, saving model to Weights-131--18214.30108.hdf5\n",
      "Epoch 132/500\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 10294.1772 - mean_absolute_error: 10294.1772 - val_loss: 19780.7813 - val_mean_absolute_error: 19780.7813\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 18214.30108\n",
      "Epoch 133/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 10955.0859 - mean_absolute_error: 10955.0859 - val_loss: 18159.8768 - val_mean_absolute_error: 18159.8768\n",
      "\n",
      "Epoch 00133: val_loss improved from 18214.30108 to 18159.87677, saving model to Weights-133--18159.87677.hdf5\n",
      "Epoch 134/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 10266.9787 - mean_absolute_error: 10266.9787 - val_loss: 18500.3433 - val_mean_absolute_error: 18500.3433\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 18159.87677\n",
      "Epoch 135/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 10842.3721 - mean_absolute_error: 10842.3721 - val_loss: 17997.2212 - val_mean_absolute_error: 17997.2212\n",
      "\n",
      "Epoch 00135: val_loss improved from 18159.87677 to 17997.22118, saving model to Weights-135--17997.22118.hdf5\n",
      "Epoch 136/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 10120.2235 - mean_absolute_error: 10120.2235 - val_loss: 18245.0955 - val_mean_absolute_error: 18245.0955\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 17997.22118\n",
      "Epoch 137/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 10921.6762 - mean_absolute_error: 10921.6762 - val_loss: 19517.6217 - val_mean_absolute_error: 19517.6217\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 17997.22118\n",
      "Epoch 138/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 10210.9181 - mean_absolute_error: 10210.9181 - val_loss: 18117.8005 - val_mean_absolute_error: 18117.8005\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 17997.22118\n",
      "Epoch 139/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 10553.1739 - mean_absolute_error: 10553.1739 - val_loss: 18847.9537 - val_mean_absolute_error: 18847.9537\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 17997.22118\n",
      "Epoch 140/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 10694.5352 - mean_absolute_error: 10694.5352 - val_loss: 18216.3566 - val_mean_absolute_error: 18216.3566\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 17997.22118\n",
      "Epoch 141/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 10107.0753 - mean_absolute_error: 10107.0753 - val_loss: 18239.9911 - val_mean_absolute_error: 18239.9911\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 17997.22118\n",
      "Epoch 142/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 10454.9173 - mean_absolute_error: 10454.9173 - val_loss: 18071.5978 - val_mean_absolute_error: 18071.5978\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 17997.22118\n",
      "Epoch 143/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 9848.5970 - mean_absolute_error: 9848.5970 - val_loss: 18322.8048 - val_mean_absolute_error: 18322.8048\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 17997.22118\n",
      "Epoch 144/500\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 9732.7739 - mean_absolute_error: 9732.7739 - val_loss: 18102.4990 - val_mean_absolute_error: 18102.4990\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 17997.22118\n",
      "Epoch 145/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 9831.4854 - mean_absolute_error: 9831.4854 - val_loss: 18948.0733 - val_mean_absolute_error: 18948.0733\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 17997.22118\n",
      "Epoch 146/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 10142.3223 - mean_absolute_error: 10142.3223 - val_loss: 18503.3095 - val_mean_absolute_error: 18503.3095\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 17997.22118\n",
      "Epoch 147/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 9693.5000 - mean_absolute_error: 9693.5000 - val_loss: 18453.1154 - val_mean_absolute_error: 18453.1154\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 17997.22118\n",
      "Epoch 148/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 10351.9611 - mean_absolute_error: 10351.9611 - val_loss: 18702.8748 - val_mean_absolute_error: 18702.8748\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 17997.22118\n",
      "Epoch 149/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 9786.8233 - mean_absolute_error: 9786.8233 - val_loss: 18087.6515 - val_mean_absolute_error: 18087.6515\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 17997.22118\n",
      "Epoch 150/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 9420.5052 - mean_absolute_error: 9420.5052 - val_loss: 18737.5916 - val_mean_absolute_error: 18737.5916\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 17997.22118\n",
      "Epoch 151/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 9544.1711 - mean_absolute_error: 9544.1711 - val_loss: 18885.4054 - val_mean_absolute_error: 18885.4054\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 17997.22118\n",
      "Epoch 152/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 10542.0792 - mean_absolute_error: 10542.0792 - val_loss: 18157.6599 - val_mean_absolute_error: 18157.6599\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 17997.22118\n",
      "Epoch 153/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 9829.7764 - mean_absolute_error: 9829.7764 - val_loss: 18327.4960 - val_mean_absolute_error: 18327.4960\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 17997.22118\n",
      "Epoch 154/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 9671.7078 - mean_absolute_error: 9671.7078 - val_loss: 18692.2028 - val_mean_absolute_error: 18692.2028\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 17997.22118\n",
      "Epoch 155/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 9753.2026 - mean_absolute_error: 9753.2026 - val_loss: 17917.7523 - val_mean_absolute_error: 17917.7523\n",
      "\n",
      "Epoch 00155: val_loss improved from 17997.22118 to 17917.75233, saving model to Weights-155--17917.75233.hdf5\n",
      "Epoch 156/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 9986.6333 - mean_absolute_error: 9986.6333 - val_loss: 18839.5934 - val_mean_absolute_error: 18839.5934\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 17917.75233\n",
      "Epoch 157/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 9512.1703 - mean_absolute_error: 9512.1703 - val_loss: 18375.4808 - val_mean_absolute_error: 18375.4808\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 17917.75233\n",
      "Epoch 158/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 9898.7208 - mean_absolute_error: 9898.7208 - val_loss: 18385.4854 - val_mean_absolute_error: 18385.4854\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 17917.75233\n",
      "Epoch 159/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 9721.3567 - mean_absolute_error: 9721.3567 - val_loss: 18489.5664 - val_mean_absolute_error: 18489.5664\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 17917.75233\n",
      "Epoch 160/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 9772.8589 - mean_absolute_error: 9772.8589 - val_loss: 17951.6829 - val_mean_absolute_error: 17951.6829\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 17917.75233\n",
      "Epoch 161/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 9055.2062 - mean_absolute_error: 9055.2062 - val_loss: 18304.5747 - val_mean_absolute_error: 18304.5747\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 17917.75233\n",
      "Epoch 162/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 9472.7934 - mean_absolute_error: 9472.7934 - val_loss: 18415.5688 - val_mean_absolute_error: 18415.5688\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 17917.75233\n",
      "Epoch 163/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 9276.1761 - mean_absolute_error: 9276.1761 - val_loss: 18807.1754 - val_mean_absolute_error: 18807.1754\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 17917.75233\n",
      "Epoch 164/500\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 9438.4186 - mean_absolute_error: 9438.4186 - val_loss: 18193.1433 - val_mean_absolute_error: 18193.1433\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 17917.75233\n",
      "Epoch 165/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 9976.7858 - mean_absolute_error: 9976.7858 - val_loss: 18378.8589 - val_mean_absolute_error: 18378.8589\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 17917.75233\n",
      "Epoch 166/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 9284.2031 - mean_absolute_error: 9284.2031 - val_loss: 18077.2875 - val_mean_absolute_error: 18077.2875\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 17917.75233\n",
      "Epoch 167/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 9101.9804 - mean_absolute_error: 9101.9804 - val_loss: 18165.5994 - val_mean_absolute_error: 18165.5994\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 17917.75233\n",
      "Epoch 168/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 9271.6944 - mean_absolute_error: 9271.6944 - val_loss: 18262.6899 - val_mean_absolute_error: 18262.6899\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 17917.75233\n",
      "Epoch 169/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 9298.6738 - mean_absolute_error: 9298.6738 - val_loss: 18212.9140 - val_mean_absolute_error: 18212.9140\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 17917.75233\n",
      "Epoch 170/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 9563.1335 - mean_absolute_error: 9563.1335 - val_loss: 19594.9911 - val_mean_absolute_error: 19594.9911\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 17917.75233\n",
      "Epoch 171/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 9376.8131 - mean_absolute_error: 9376.8131 - val_loss: 18025.3502 - val_mean_absolute_error: 18025.3502\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 17917.75233\n",
      "Epoch 172/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 9430.7098 - mean_absolute_error: 9430.7098 - val_loss: 17815.2828 - val_mean_absolute_error: 17815.2828\n",
      "\n",
      "Epoch 00172: val_loss improved from 17917.75233 to 17815.28280, saving model to Weights-172--17815.28280.hdf5\n",
      "Epoch 173/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 9586.3374 - mean_absolute_error: 9586.3374 - val_loss: 19579.4442 - val_mean_absolute_error: 19579.4442\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 17815.28280\n",
      "Epoch 174/500\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9407.9876 - mean_absolute_error: 9407.9876 - val_loss: 18220.5584 - val_mean_absolute_error: 18220.5584\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 17815.28280\n",
      "Epoch 175/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 9338.1880 - mean_absolute_error: 9338.1880 - val_loss: 18149.3103 - val_mean_absolute_error: 18149.3103\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 17815.28280\n",
      "Epoch 176/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 9030.5642 - mean_absolute_error: 9030.5642 - val_loss: 18464.1161 - val_mean_absolute_error: 18464.1161\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 17815.28280\n",
      "Epoch 177/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 8929.9500 - mean_absolute_error: 8929.9500 - val_loss: 18304.5097 - val_mean_absolute_error: 18304.5097\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 17815.28280\n",
      "Epoch 178/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 8740.1405 - mean_absolute_error: 8740.1405 - val_loss: 18171.6268 - val_mean_absolute_error: 18171.6268\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 17815.28280\n",
      "Epoch 179/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8590.3471 - mean_absolute_error: 8590.3471 - val_loss: 18241.3752 - val_mean_absolute_error: 18241.3752\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 17815.28280\n",
      "Epoch 180/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 9028.9237 - mean_absolute_error: 9028.9237 - val_loss: 18024.6781 - val_mean_absolute_error: 18024.6781\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 17815.28280\n",
      "Epoch 181/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 8754.6591 - mean_absolute_error: 8754.6591 - val_loss: 18012.3217 - val_mean_absolute_error: 18012.3217\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 17815.28280\n",
      "Epoch 182/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 8802.8268 - mean_absolute_error: 8802.8268 - val_loss: 18123.7244 - val_mean_absolute_error: 18123.7244\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 17815.28280\n",
      "Epoch 183/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 8702.2449 - mean_absolute_error: 8702.2449 - val_loss: 18257.5497 - val_mean_absolute_error: 18257.5497\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 17815.28280\n",
      "Epoch 184/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 8780.2923 - mean_absolute_error: 8780.2923 - val_loss: 18642.7114 - val_mean_absolute_error: 18642.7114\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 17815.28280\n",
      "Epoch 185/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 8706.0889 - mean_absolute_error: 8706.0889 - val_loss: 18037.7270 - val_mean_absolute_error: 18037.7270\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 17815.28280\n",
      "Epoch 186/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 9011.7189 - mean_absolute_error: 9011.7189 - val_loss: 18149.9581 - val_mean_absolute_error: 18149.9581\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 17815.28280\n",
      "Epoch 187/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 9189.8990 - mean_absolute_error: 9189.8990 - val_loss: 19027.7917 - val_mean_absolute_error: 19027.7917\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 17815.28280\n",
      "Epoch 188/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 9152.1498 - mean_absolute_error: 9152.1498 - val_loss: 18311.7739 - val_mean_absolute_error: 18311.7739\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 17815.28280\n",
      "Epoch 189/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 8951.3692 - mean_absolute_error: 8951.3692 - val_loss: 18129.9291 - val_mean_absolute_error: 18129.9291\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 17815.28280\n",
      "Epoch 190/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9023.6059 - mean_absolute_error: 9023.6059 - val_loss: 18045.0780 - val_mean_absolute_error: 18045.0780\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 17815.28280\n",
      "Epoch 191/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 8468.9898 - mean_absolute_error: 8468.9898 - val_loss: 18298.5269 - val_mean_absolute_error: 18298.5269\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 17815.28280\n",
      "Epoch 192/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 8695.5303 - mean_absolute_error: 8695.5303 - val_loss: 18643.1926 - val_mean_absolute_error: 18643.1926\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 17815.28280\n",
      "Epoch 193/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 8701.7150 - mean_absolute_error: 8701.7150 - val_loss: 18278.7174 - val_mean_absolute_error: 18278.7174\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 17815.28280\n",
      "Epoch 194/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8438.7297 - mean_absolute_error: 8438.7297 - val_loss: 18146.7489 - val_mean_absolute_error: 18146.7489\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 17815.28280\n",
      "Epoch 195/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 8506.1734 - mean_absolute_error: 8506.1734 - val_loss: 18329.1099 - val_mean_absolute_error: 18329.1099\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 17815.28280\n",
      "Epoch 196/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 8525.9691 - mean_absolute_error: 8525.9691 - val_loss: 18516.0292 - val_mean_absolute_error: 18516.0292\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 17815.28280\n",
      "Epoch 197/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 8380.1822 - mean_absolute_error: 8380.1822 - val_loss: 18121.4283 - val_mean_absolute_error: 18121.4283\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 17815.28280\n",
      "Epoch 198/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 8266.9845 - mean_absolute_error: 8266.9845 - val_loss: 18112.6303 - val_mean_absolute_error: 18112.6303\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 17815.28280\n",
      "Epoch 199/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 8248.0226 - mean_absolute_error: 8248.0226 - val_loss: 18185.0540 - val_mean_absolute_error: 18185.0540\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 17815.28280\n",
      "Epoch 200/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 7993.8891 - mean_absolute_error: 7993.8891 - val_loss: 18708.9099 - val_mean_absolute_error: 18708.9099\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 17815.28280\n",
      "Epoch 201/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 8485.3370 - mean_absolute_error: 8485.3370 - val_loss: 18191.0542 - val_mean_absolute_error: 18191.0542\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 17815.28280\n",
      "Epoch 202/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 8193.0727 - mean_absolute_error: 8193.0727 - val_loss: 18267.9148 - val_mean_absolute_error: 18267.9148\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 17815.28280\n",
      "Epoch 203/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 7950.4080 - mean_absolute_error: 7950.4080 - val_loss: 18197.7897 - val_mean_absolute_error: 18197.7897\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 17815.28280\n",
      "Epoch 204/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 7858.6689 - mean_absolute_error: 7858.6689 - val_loss: 18174.9804 - val_mean_absolute_error: 18174.9804\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 17815.28280\n",
      "Epoch 205/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7977.8316 - mean_absolute_error: 7977.8316 - val_loss: 18492.3650 - val_mean_absolute_error: 18492.3650\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 17815.28280\n",
      "Epoch 206/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 8919.6579 - mean_absolute_error: 8919.6579 - val_loss: 18244.8429 - val_mean_absolute_error: 18244.8429\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 17815.28280\n",
      "Epoch 207/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 7871.7078 - mean_absolute_error: 7871.7078 - val_loss: 18206.8475 - val_mean_absolute_error: 18206.8475\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 17815.28280\n",
      "Epoch 208/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 7787.1870 - mean_absolute_error: 7787.1870 - val_loss: 18334.9307 - val_mean_absolute_error: 18334.9307\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 17815.28280\n",
      "Epoch 209/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 8079.2339 - mean_absolute_error: 8079.2339 - val_loss: 18152.4482 - val_mean_absolute_error: 18152.4482\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 17815.28280\n",
      "Epoch 210/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7631.4431 - mean_absolute_error: 7631.4431 - val_loss: 18388.7032 - val_mean_absolute_error: 18388.7032\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 17815.28280\n",
      "Epoch 211/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 7781.2966 - mean_absolute_error: 7781.2966 - val_loss: 18431.9935 - val_mean_absolute_error: 18431.9935\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 17815.28280\n",
      "Epoch 212/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 7637.4967 - mean_absolute_error: 7637.4967 - val_loss: 18033.0193 - val_mean_absolute_error: 18033.0193\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 17815.28280\n",
      "Epoch 213/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 7736.7826 - mean_absolute_error: 7736.7826 - val_loss: 18132.4426 - val_mean_absolute_error: 18132.4426\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 17815.28280\n",
      "Epoch 214/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 7571.8261 - mean_absolute_error: 7571.8261 - val_loss: 18176.4693 - val_mean_absolute_error: 18176.4693\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 17815.28280\n",
      "Epoch 215/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 7695.4654 - mean_absolute_error: 7695.4654 - val_loss: 17993.5093 - val_mean_absolute_error: 17993.5093\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 17815.28280\n",
      "Epoch 216/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 8104.8217 - mean_absolute_error: 8104.8217 - val_loss: 18630.4968 - val_mean_absolute_error: 18630.4968\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 17815.28280\n",
      "Epoch 217/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 8085.1234 - mean_absolute_error: 8085.1234 - val_loss: 18145.0040 - val_mean_absolute_error: 18145.0040\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 17815.28280\n",
      "Epoch 218/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7438.6020 - mean_absolute_error: 7438.6020 - val_loss: 18219.5913 - val_mean_absolute_error: 18219.5913\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 17815.28280\n",
      "Epoch 219/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 9259.1928 - mean_absolute_error: 9259.1928 - val_loss: 20860.6163 - val_mean_absolute_error: 20860.6163\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 17815.28280\n",
      "Epoch 220/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 9014.0275 - mean_absolute_error: 9014.0275 - val_loss: 18097.5741 - val_mean_absolute_error: 18097.5741\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 17815.28280\n",
      "Epoch 221/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 8082.3646 - mean_absolute_error: 8082.3646 - val_loss: 18321.0106 - val_mean_absolute_error: 18321.0106\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 17815.28280\n",
      "Epoch 222/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 7570.8471 - mean_absolute_error: 7570.8471 - val_loss: 18230.7635 - val_mean_absolute_error: 18230.7635\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 17815.28280\n",
      "Epoch 223/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 7932.6520 - mean_absolute_error: 7932.6520 - val_loss: 19406.0221 - val_mean_absolute_error: 19406.0221\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 17815.28280\n",
      "Epoch 224/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 8335.6560 - mean_absolute_error: 8335.6560 - val_loss: 17988.2056 - val_mean_absolute_error: 17988.2056\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 17815.28280\n",
      "Epoch 225/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 7872.7406 - mean_absolute_error: 7872.7406 - val_loss: 18695.9630 - val_mean_absolute_error: 18695.9630\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 17815.28280\n",
      "Epoch 226/500\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 7481.6271 - mean_absolute_error: 7481.6271 - val_loss: 19589.1335 - val_mean_absolute_error: 19589.1335\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 17815.28280\n",
      "Epoch 227/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 8182.8301 - mean_absolute_error: 8182.8301 - val_loss: 18173.0942 - val_mean_absolute_error: 18173.0942\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 17815.28280\n",
      "Epoch 228/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 7409.7419 - mean_absolute_error: 7409.7419 - val_loss: 18252.8057 - val_mean_absolute_error: 18252.8057\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 17815.28280\n",
      "Epoch 229/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7279.2961 - mean_absolute_error: 7279.2961 - val_loss: 18119.8516 - val_mean_absolute_error: 18119.8516\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 17815.28280\n",
      "Epoch 230/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 7370.8453 - mean_absolute_error: 7370.8453 - val_loss: 18691.0213 - val_mean_absolute_error: 18691.0213\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 17815.28280\n",
      "Epoch 231/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 7532.4658 - mean_absolute_error: 7532.4658 - val_loss: 18395.6636 - val_mean_absolute_error: 18395.6636\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 17815.28280\n",
      "Epoch 232/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 7383.8097 - mean_absolute_error: 7383.8097 - val_loss: 18856.8910 - val_mean_absolute_error: 18856.8910\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 17815.28280\n",
      "Epoch 233/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 7431.7209 - mean_absolute_error: 7431.7209 - val_loss: 18326.3208 - val_mean_absolute_error: 18326.3208\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 17815.28280\n",
      "Epoch 234/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 7437.3121 - mean_absolute_error: 7437.3121 - val_loss: 18779.2613 - val_mean_absolute_error: 18779.2613\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 17815.28280\n",
      "Epoch 235/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 7755.5983 - mean_absolute_error: 7755.5983 - val_loss: 18489.5384 - val_mean_absolute_error: 18489.5384\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 17815.28280\n",
      "Epoch 236/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 7567.8224 - mean_absolute_error: 7567.8224 - val_loss: 18184.0990 - val_mean_absolute_error: 18184.0990\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 17815.28280\n",
      "Epoch 237/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 7629.4183 - mean_absolute_error: 7629.4183 - val_loss: 18410.1287 - val_mean_absolute_error: 18410.1287\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 17815.28280\n",
      "Epoch 238/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 7637.2701 - mean_absolute_error: 7637.2701 - val_loss: 18204.3968 - val_mean_absolute_error: 18204.3968\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 17815.28280\n",
      "Epoch 239/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 7778.5434 - mean_absolute_error: 7778.5434 - val_loss: 18305.3105 - val_mean_absolute_error: 18305.3105\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 17815.28280\n",
      "Epoch 240/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 7379.6786 - mean_absolute_error: 7379.6786 - val_loss: 18111.7067 - val_mean_absolute_error: 18111.7067\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 17815.28280\n",
      "Epoch 241/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7100.4162 - mean_absolute_error: 7100.4162 - val_loss: 18779.9113 - val_mean_absolute_error: 18779.9113\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 17815.28280\n",
      "Epoch 242/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 7387.9790 - mean_absolute_error: 7387.9790 - val_loss: 18564.7056 - val_mean_absolute_error: 18564.7056\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 17815.28280\n",
      "Epoch 243/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 6836.7688 - mean_absolute_error: 6836.7688 - val_loss: 18495.4988 - val_mean_absolute_error: 18495.4988\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 17815.28280\n",
      "Epoch 244/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 7724.5382 - mean_absolute_error: 7724.5382 - val_loss: 19019.4851 - val_mean_absolute_error: 19019.4851\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 17815.28280\n",
      "Epoch 245/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 7104.1320 - mean_absolute_error: 7104.1320 - val_loss: 18444.8976 - val_mean_absolute_error: 18444.8976\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 17815.28280\n",
      "Epoch 246/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 7216.0180 - mean_absolute_error: 7216.0180 - val_loss: 18677.7014 - val_mean_absolute_error: 18677.7014\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 17815.28280\n",
      "Epoch 247/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 7185.4287 - mean_absolute_error: 7185.4287 - val_loss: 18176.4299 - val_mean_absolute_error: 18176.4299\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 17815.28280\n",
      "Epoch 248/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6930.2439 - mean_absolute_error: 6930.2439 - val_loss: 18386.7318 - val_mean_absolute_error: 18386.7318\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 17815.28280\n",
      "Epoch 249/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 7024.0148 - mean_absolute_error: 7024.0148 - val_loss: 17995.3353 - val_mean_absolute_error: 17995.3353\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 17815.28280\n",
      "Epoch 250/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 7332.3520 - mean_absolute_error: 7332.3520 - val_loss: 19218.0125 - val_mean_absolute_error: 19218.0125\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 17815.28280\n",
      "Epoch 251/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 7389.4374 - mean_absolute_error: 7389.4374 - val_loss: 18538.8945 - val_mean_absolute_error: 18538.8945\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 17815.28280\n",
      "Epoch 252/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 7226.8909 - mean_absolute_error: 7226.8909 - val_loss: 19227.6020 - val_mean_absolute_error: 19227.6020\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 17815.28280\n",
      "Epoch 253/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 8062.4982 - mean_absolute_error: 8062.4982 - val_loss: 18197.0513 - val_mean_absolute_error: 18197.0513\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 17815.28280\n",
      "Epoch 254/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 6893.3510 - mean_absolute_error: 6893.3510 - val_loss: 17947.4697 - val_mean_absolute_error: 17947.4697\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 17815.28280\n",
      "Epoch 255/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 7209.7865 - mean_absolute_error: 7209.7865 - val_loss: 18328.1146 - val_mean_absolute_error: 18328.1146\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 17815.28280\n",
      "Epoch 256/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 6818.5570 - mean_absolute_error: 6818.5570 - val_loss: 18246.9552 - val_mean_absolute_error: 18246.9552\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 17815.28280\n",
      "Epoch 257/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7026.4243 - mean_absolute_error: 7026.4243 - val_loss: 18248.4975 - val_mean_absolute_error: 18248.4975\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 17815.28280\n",
      "Epoch 258/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 6905.3683 - mean_absolute_error: 6905.3683 - val_loss: 18437.4405 - val_mean_absolute_error: 18437.4405\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 17815.28280\n",
      "Epoch 259/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 7162.7941 - mean_absolute_error: 7162.7941 - val_loss: 18069.8563 - val_mean_absolute_error: 18069.8563\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 17815.28280\n",
      "Epoch 260/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 7197.5829 - mean_absolute_error: 7197.5829 - val_loss: 18259.1078 - val_mean_absolute_error: 18259.1078\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 17815.28280\n",
      "Epoch 261/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 7062.3739 - mean_absolute_error: 7062.3739 - val_loss: 19204.4810 - val_mean_absolute_error: 19204.4810\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 17815.28280\n",
      "Epoch 262/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 6997.1597 - mean_absolute_error: 6997.1597 - val_loss: 18497.1680 - val_mean_absolute_error: 18497.1680\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 17815.28280\n",
      "Epoch 263/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 6798.6492 - mean_absolute_error: 6798.6492 - val_loss: 18544.7739 - val_mean_absolute_error: 18544.7739\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 17815.28280\n",
      "Epoch 264/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7247.4355 - mean_absolute_error: 7247.4355 - val_loss: 18807.0323 - val_mean_absolute_error: 18807.0323\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 17815.28280\n",
      "Epoch 265/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 6799.6400 - mean_absolute_error: 6799.6400 - val_loss: 18297.3911 - val_mean_absolute_error: 18297.3911\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 17815.28280\n",
      "Epoch 266/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 6841.4102 - mean_absolute_error: 6841.4102 - val_loss: 18416.2277 - val_mean_absolute_error: 18416.2277\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 17815.28280\n",
      "Epoch 267/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 6587.3445 - mean_absolute_error: 6587.3445 - val_loss: 18320.5281 - val_mean_absolute_error: 18320.5281\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 17815.28280\n",
      "Epoch 268/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 6949.8060 - mean_absolute_error: 6949.8060 - val_loss: 18091.0281 - val_mean_absolute_error: 18091.0281\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 17815.28280\n",
      "Epoch 269/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6915.3820 - mean_absolute_error: 6915.3820 - val_loss: 18384.2543 - val_mean_absolute_error: 18384.2543\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 17815.28280\n",
      "Epoch 270/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 6856.0634 - mean_absolute_error: 6856.0634 - val_loss: 18381.6744 - val_mean_absolute_error: 18381.6744\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 17815.28280\n",
      "Epoch 271/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 7378.8070 - mean_absolute_error: 7378.8070 - val_loss: 18570.6626 - val_mean_absolute_error: 18570.6626\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 17815.28280\n",
      "Epoch 272/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6749.6161 - mean_absolute_error: 6749.6161 - val_loss: 18515.9685 - val_mean_absolute_error: 18515.9685\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 17815.28280\n",
      "Epoch 273/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7000.6639 - mean_absolute_error: 7000.6639 - val_loss: 18459.2605 - val_mean_absolute_error: 18459.2605\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 17815.28280\n",
      "Epoch 274/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 7199.2450 - mean_absolute_error: 7199.2450 - val_loss: 18715.5686 - val_mean_absolute_error: 18715.5686\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 17815.28280\n",
      "Epoch 275/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 6436.0677 - mean_absolute_error: 6436.0677 - val_loss: 18440.6608 - val_mean_absolute_error: 18440.6608\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 17815.28280\n",
      "Epoch 276/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6461.0009 - mean_absolute_error: 6461.0009 - val_loss: 18572.9147 - val_mean_absolute_error: 18572.9147\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 17815.28280\n",
      "Epoch 277/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 6633.9428 - mean_absolute_error: 6633.9428 - val_loss: 18326.4029 - val_mean_absolute_error: 18326.4029\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 17815.28280\n",
      "Epoch 278/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 7018.6988 - mean_absolute_error: 7018.6988 - val_loss: 19367.5389 - val_mean_absolute_error: 19367.5389\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 17815.28280\n",
      "Epoch 279/500\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 6604.2486 - mean_absolute_error: 6604.2486 - val_loss: 18749.4785 - val_mean_absolute_error: 18749.4785\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 17815.28280\n",
      "Epoch 280/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 6452.9155 - mean_absolute_error: 6452.9155 - val_loss: 18013.7022 - val_mean_absolute_error: 18013.7022\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 17815.28280\n",
      "Epoch 281/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 6613.9606 - mean_absolute_error: 6613.9606 - val_loss: 18399.4284 - val_mean_absolute_error: 18399.4284\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 17815.28280\n",
      "Epoch 282/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6407.1794 - mean_absolute_error: 6407.1794 - val_loss: 18345.6611 - val_mean_absolute_error: 18345.6611\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 17815.28280\n",
      "Epoch 283/500\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 6475.9703 - mean_absolute_error: 6475.9703 - val_loss: 19246.7367 - val_mean_absolute_error: 19246.7367\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 17815.28280\n",
      "Epoch 284/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6441.8306 - mean_absolute_error: 6441.8306 - val_loss: 18496.0782 - val_mean_absolute_error: 18496.0782\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 17815.28280\n",
      "Epoch 285/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 6351.3189 - mean_absolute_error: 6351.3189 - val_loss: 18482.6762 - val_mean_absolute_error: 18482.6762\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 17815.28280\n",
      "Epoch 286/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 6348.7228 - mean_absolute_error: 6348.7228 - val_loss: 18713.3707 - val_mean_absolute_error: 18713.3707\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 17815.28280\n",
      "Epoch 287/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 6345.4068 - mean_absolute_error: 6345.4068 - val_loss: 18508.8277 - val_mean_absolute_error: 18508.8277\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 17815.28280\n",
      "Epoch 288/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 6478.2773 - mean_absolute_error: 6478.2773 - val_loss: 18432.6365 - val_mean_absolute_error: 18432.6365\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 17815.28280\n",
      "Epoch 289/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 6268.7995 - mean_absolute_error: 6268.7995 - val_loss: 19018.4603 - val_mean_absolute_error: 19018.4603\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 17815.28280\n",
      "Epoch 290/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 6625.3856 - mean_absolute_error: 6625.3856 - val_loss: 18357.5841 - val_mean_absolute_error: 18357.5841\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 17815.28280\n",
      "Epoch 291/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 7149.6653 - mean_absolute_error: 7149.6653 - val_loss: 19554.1026 - val_mean_absolute_error: 19554.1026\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 17815.28280\n",
      "Epoch 292/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 7275.7797 - mean_absolute_error: 7275.7797 - val_loss: 20123.8021 - val_mean_absolute_error: 20123.8021\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 17815.28280\n",
      "Epoch 293/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6941.0676 - mean_absolute_error: 6941.0676 - val_loss: 18242.2320 - val_mean_absolute_error: 18242.2320\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 17815.28280\n",
      "Epoch 294/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6701.7982 - mean_absolute_error: 6701.7982 - val_loss: 19074.5771 - val_mean_absolute_error: 19074.5771\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 17815.28280\n",
      "Epoch 295/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 7323.2676 - mean_absolute_error: 7323.2676 - val_loss: 19524.5055 - val_mean_absolute_error: 19524.5055\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 17815.28280\n",
      "Epoch 296/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 6734.6471 - mean_absolute_error: 6734.6471 - val_loss: 18674.1368 - val_mean_absolute_error: 18674.1368\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 17815.28280\n",
      "Epoch 297/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6470.8697 - mean_absolute_error: 6470.8697 - val_loss: 18244.7957 - val_mean_absolute_error: 18244.7957\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 17815.28280\n",
      "Epoch 298/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 6691.3634 - mean_absolute_error: 6691.3634 - val_loss: 18810.1812 - val_mean_absolute_error: 18810.1812\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 17815.28280\n",
      "Epoch 299/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6818.0256 - mean_absolute_error: 6818.0256 - val_loss: 18637.1407 - val_mean_absolute_error: 18637.1407\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 17815.28280\n",
      "Epoch 300/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 6685.5597 - mean_absolute_error: 6685.5597 - val_loss: 19477.6936 - val_mean_absolute_error: 19477.6936\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 17815.28280\n",
      "Epoch 301/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 7175.4501 - mean_absolute_error: 7175.4501 - val_loss: 18355.8027 - val_mean_absolute_error: 18355.8027\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 17815.28280\n",
      "Epoch 302/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 6556.0281 - mean_absolute_error: 6556.0281 - val_loss: 18483.4723 - val_mean_absolute_error: 18483.4723\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 17815.28280\n",
      "Epoch 303/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 5993.8285 - mean_absolute_error: 5993.8285 - val_loss: 18201.3955 - val_mean_absolute_error: 18201.3955\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 17815.28280\n",
      "Epoch 304/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 6128.2302 - mean_absolute_error: 6128.2302 - val_loss: 18541.9801 - val_mean_absolute_error: 18541.9801\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 17815.28280\n",
      "Epoch 305/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 6372.2175 - mean_absolute_error: 6372.2175 - val_loss: 18934.4533 - val_mean_absolute_error: 18934.4533\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 17815.28280\n",
      "Epoch 306/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6273.7676 - mean_absolute_error: 6273.7676 - val_loss: 18505.3884 - val_mean_absolute_error: 18505.3884\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 17815.28280\n",
      "Epoch 307/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 6377.8496 - mean_absolute_error: 6377.8496 - val_loss: 18361.3648 - val_mean_absolute_error: 18361.3648\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 17815.28280\n",
      "Epoch 308/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 6343.5426 - mean_absolute_error: 6343.5426 - val_loss: 18127.6838 - val_mean_absolute_error: 18127.6838\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 17815.28280\n",
      "Epoch 309/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6383.9010 - mean_absolute_error: 6383.9010 - val_loss: 18555.1376 - val_mean_absolute_error: 18555.1376\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 17815.28280\n",
      "Epoch 310/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 6420.2877 - mean_absolute_error: 6420.2877 - val_loss: 18254.1977 - val_mean_absolute_error: 18254.1977\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 17815.28280\n",
      "Epoch 311/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 6041.1027 - mean_absolute_error: 6041.1027 - val_loss: 18582.9199 - val_mean_absolute_error: 18582.9199\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 17815.28280\n",
      "Epoch 312/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 5940.2778 - mean_absolute_error: 5940.2778 - val_loss: 18178.8310 - val_mean_absolute_error: 18178.8310\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 17815.28280\n",
      "Epoch 313/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 5929.4209 - mean_absolute_error: 5929.4209 - val_loss: 18372.5450 - val_mean_absolute_error: 18372.5450\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 17815.28280\n",
      "Epoch 314/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 6167.0089 - mean_absolute_error: 6167.0089 - val_loss: 18143.9560 - val_mean_absolute_error: 18143.9560\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 17815.28280\n",
      "Epoch 315/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 5962.0486 - mean_absolute_error: 5962.0486 - val_loss: 19187.7621 - val_mean_absolute_error: 19187.7621\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 17815.28280\n",
      "Epoch 316/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 6521.7596 - mean_absolute_error: 6521.7596 - val_loss: 19202.4480 - val_mean_absolute_error: 19202.4480\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 17815.28280\n",
      "Epoch 317/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 6280.4301 - mean_absolute_error: 6280.4301 - val_loss: 18449.2776 - val_mean_absolute_error: 18449.2776\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 17815.28280\n",
      "Epoch 318/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 5876.2591 - mean_absolute_error: 5876.2591 - val_loss: 18120.8738 - val_mean_absolute_error: 18120.8738\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 17815.28280\n",
      "Epoch 319/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 5872.2688 - mean_absolute_error: 5872.2688 - val_loss: 19014.1116 - val_mean_absolute_error: 19014.1116\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 17815.28280\n",
      "Epoch 320/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 6016.8902 - mean_absolute_error: 6016.8902 - val_loss: 18319.8464 - val_mean_absolute_error: 18319.8464\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 17815.28280\n",
      "Epoch 321/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 5734.0504 - mean_absolute_error: 5734.0504 - val_loss: 18801.6874 - val_mean_absolute_error: 18801.6874\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 17815.28280\n",
      "Epoch 322/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 6095.0782 - mean_absolute_error: 6095.0782 - val_loss: 18601.6704 - val_mean_absolute_error: 18601.6704\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 17815.28280\n",
      "Epoch 323/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 5986.4989 - mean_absolute_error: 5986.4989 - val_loss: 18276.2459 - val_mean_absolute_error: 18276.2459\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 17815.28280\n",
      "Epoch 324/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5672.8478 - mean_absolute_error: 5672.8478 - val_loss: 18967.3092 - val_mean_absolute_error: 18967.3092\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 17815.28280\n",
      "Epoch 325/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 5740.5123 - mean_absolute_error: 5740.5123 - val_loss: 18703.7897 - val_mean_absolute_error: 18703.7897\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 17815.28280\n",
      "Epoch 326/500\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 5999.8101 - mean_absolute_error: 5999.8101 - val_loss: 18556.5011 - val_mean_absolute_error: 18556.5011\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 17815.28280\n",
      "Epoch 327/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 5960.7089 - mean_absolute_error: 5960.7089 - val_loss: 18552.0498 - val_mean_absolute_error: 18552.0498\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 17815.28280\n",
      "Epoch 328/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 5984.6698 - mean_absolute_error: 5984.6698 - val_loss: 18761.1692 - val_mean_absolute_error: 18761.1692\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 17815.28280\n",
      "Epoch 329/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 5666.9237 - mean_absolute_error: 5666.9237 - val_loss: 18366.3822 - val_mean_absolute_error: 18366.3822\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 17815.28280\n",
      "Epoch 330/500\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 6173.2818 - mean_absolute_error: 6173.2818 - val_loss: 19169.8911 - val_mean_absolute_error: 19169.8911\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 17815.28280\n",
      "Epoch 331/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 5831.9198 - mean_absolute_error: 5831.9198 - val_loss: 19375.8090 - val_mean_absolute_error: 19375.8090\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 17815.28280\n",
      "Epoch 332/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 5925.5301 - mean_absolute_error: 5925.5301 - val_loss: 18606.2340 - val_mean_absolute_error: 18606.2340\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 17815.28280\n",
      "Epoch 333/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 5695.9512 - mean_absolute_error: 5695.9512 - val_loss: 19162.7748 - val_mean_absolute_error: 19162.7748\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 17815.28280\n",
      "Epoch 334/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 5787.2980 - mean_absolute_error: 5787.2980 - val_loss: 18730.7540 - val_mean_absolute_error: 18730.7540\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 17815.28280\n",
      "Epoch 335/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 5722.8550 - mean_absolute_error: 5722.8550 - val_loss: 18604.6700 - val_mean_absolute_error: 18604.6700\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 17815.28280\n",
      "Epoch 336/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 5777.0167 - mean_absolute_error: 5777.0167 - val_loss: 19458.6325 - val_mean_absolute_error: 19458.6325\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 17815.28280\n",
      "Epoch 337/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 5475.1512 - mean_absolute_error: 5475.1512 - val_loss: 18660.1992 - val_mean_absolute_error: 18660.1992\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 17815.28280\n",
      "Epoch 338/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 6161.0194 - mean_absolute_error: 6161.0194 - val_loss: 18802.3103 - val_mean_absolute_error: 18802.3103\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 17815.28280\n",
      "Epoch 339/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 5911.0274 - mean_absolute_error: 5911.0274 - val_loss: 18893.8367 - val_mean_absolute_error: 18893.8367\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 17815.28280\n",
      "Epoch 340/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 5474.7795 - mean_absolute_error: 5474.7795 - val_loss: 18967.2381 - val_mean_absolute_error: 18967.2381\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 17815.28280\n",
      "Epoch 341/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 6038.4963 - mean_absolute_error: 6038.4963 - val_loss: 19110.6506 - val_mean_absolute_error: 19110.6506\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 17815.28280\n",
      "Epoch 342/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 5546.8218 - mean_absolute_error: 5546.8218 - val_loss: 19104.9374 - val_mean_absolute_error: 19104.9374\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 17815.28280\n",
      "Epoch 343/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 5444.2014 - mean_absolute_error: 5444.2014 - val_loss: 19173.8527 - val_mean_absolute_error: 19173.8527\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 17815.28280\n",
      "Epoch 344/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 5829.8201 - mean_absolute_error: 5829.8201 - val_loss: 18996.7239 - val_mean_absolute_error: 18996.7239\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 17815.28280\n",
      "Epoch 345/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6101.5919 - mean_absolute_error: 6101.5919 - val_loss: 18577.1115 - val_mean_absolute_error: 18577.1115\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 17815.28280\n",
      "Epoch 346/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 6286.3610 - mean_absolute_error: 6286.3610 - val_loss: 19323.8560 - val_mean_absolute_error: 19323.8560\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 17815.28280\n",
      "Epoch 347/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 5886.7046 - mean_absolute_error: 5886.7046 - val_loss: 18771.4292 - val_mean_absolute_error: 18771.4292\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 17815.28280\n",
      "Epoch 348/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 5606.7493 - mean_absolute_error: 5606.7493 - val_loss: 19290.7646 - val_mean_absolute_error: 19290.7646\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 17815.28280\n",
      "Epoch 349/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 5347.8744 - mean_absolute_error: 5347.8744 - val_loss: 18641.5752 - val_mean_absolute_error: 18641.5752\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 17815.28280\n",
      "Epoch 350/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 5753.6413 - mean_absolute_error: 5753.6413 - val_loss: 19044.0849 - val_mean_absolute_error: 19044.0849\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 17815.28280\n",
      "Epoch 351/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 5480.6478 - mean_absolute_error: 5480.6478 - val_loss: 18947.9189 - val_mean_absolute_error: 18947.9189\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 17815.28280\n",
      "Epoch 352/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5461.5201 - mean_absolute_error: 5461.5201 - val_loss: 18802.2792 - val_mean_absolute_error: 18802.2792\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 17815.28280\n",
      "Epoch 353/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 5408.6286 - mean_absolute_error: 5408.6286 - val_loss: 18893.7842 - val_mean_absolute_error: 18893.7842\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 17815.28280\n",
      "Epoch 354/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 5515.2080 - mean_absolute_error: 5515.2080 - val_loss: 19569.9503 - val_mean_absolute_error: 19569.9503\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 17815.28280\n",
      "Epoch 355/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 6238.5809 - mean_absolute_error: 6238.5809 - val_loss: 18684.9340 - val_mean_absolute_error: 18684.9340\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 17815.28280\n",
      "Epoch 356/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 6415.7096 - mean_absolute_error: 6415.7096 - val_loss: 18746.2595 - val_mean_absolute_error: 18746.2595\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 17815.28280\n",
      "Epoch 357/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5219.5021 - mean_absolute_error: 5219.5021 - val_loss: 18636.5601 - val_mean_absolute_error: 18636.5601\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 17815.28280\n",
      "Epoch 358/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 5823.6339 - mean_absolute_error: 5823.6339 - val_loss: 18721.4134 - val_mean_absolute_error: 18721.4134\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 17815.28280\n",
      "Epoch 359/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5393.7919 - mean_absolute_error: 5393.7919 - val_loss: 18526.0129 - val_mean_absolute_error: 18526.0129\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 17815.28280\n",
      "Epoch 360/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 6109.0207 - mean_absolute_error: 6109.0207 - val_loss: 18940.9322 - val_mean_absolute_error: 18940.9322\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 17815.28280\n",
      "Epoch 361/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5902.1333 - mean_absolute_error: 5902.1333 - val_loss: 18895.2018 - val_mean_absolute_error: 18895.2018\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 17815.28280\n",
      "Epoch 362/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 5604.7563 - mean_absolute_error: 5604.7563 - val_loss: 18830.0632 - val_mean_absolute_error: 18830.0632\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 17815.28280\n",
      "Epoch 363/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 5699.0182 - mean_absolute_error: 5699.0182 - val_loss: 19835.2667 - val_mean_absolute_error: 19835.2667\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 17815.28280\n",
      "Epoch 364/500\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 5934.8988 - mean_absolute_error: 5934.8988 - val_loss: 19447.5270 - val_mean_absolute_error: 19447.5270\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 17815.28280\n",
      "Epoch 365/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 6011.7558 - mean_absolute_error: 6011.7558 - val_loss: 18448.9479 - val_mean_absolute_error: 18448.9479\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 17815.28280\n",
      "Epoch 366/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 5614.4211 - mean_absolute_error: 5614.4211 - val_loss: 18642.7874 - val_mean_absolute_error: 18642.7874\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 17815.28280\n",
      "Epoch 367/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 5474.7223 - mean_absolute_error: 5474.7223 - val_loss: 19115.3011 - val_mean_absolute_error: 19115.3011\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 17815.28280\n",
      "Epoch 368/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 5226.0169 - mean_absolute_error: 5226.0169 - val_loss: 18659.7344 - val_mean_absolute_error: 18659.7344\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 17815.28280\n",
      "Epoch 369/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5091.9295 - mean_absolute_error: 5091.9295 - val_loss: 19192.9159 - val_mean_absolute_error: 19192.9159\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 17815.28280\n",
      "Epoch 370/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 5256.1356 - mean_absolute_error: 5256.1356 - val_loss: 18939.2937 - val_mean_absolute_error: 18939.2937\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 17815.28280\n",
      "Epoch 371/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 5224.3762 - mean_absolute_error: 5224.3762 - val_loss: 19003.4873 - val_mean_absolute_error: 19003.4873\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 17815.28280\n",
      "Epoch 372/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 5187.4864 - mean_absolute_error: 5187.4864 - val_loss: 19234.0197 - val_mean_absolute_error: 19234.0197\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 17815.28280\n",
      "Epoch 373/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 6103.3040 - mean_absolute_error: 6103.3040 - val_loss: 19041.6899 - val_mean_absolute_error: 19041.6899\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 17815.28280\n",
      "Epoch 374/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 5767.9952 - mean_absolute_error: 5767.9952 - val_loss: 19251.3938 - val_mean_absolute_error: 19251.3938\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 17815.28280\n",
      "Epoch 375/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 5109.8355 - mean_absolute_error: 5109.8355 - val_loss: 19245.4743 - val_mean_absolute_error: 19245.4743\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 17815.28280\n",
      "Epoch 376/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 5130.5648 - mean_absolute_error: 5130.5648 - val_loss: 19106.9747 - val_mean_absolute_error: 19106.9747\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 17815.28280\n",
      "Epoch 377/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 5231.6003 - mean_absolute_error: 5231.6003 - val_loss: 19162.3671 - val_mean_absolute_error: 19162.3671\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 17815.28280\n",
      "Epoch 378/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5062.6354 - mean_absolute_error: 5062.6354 - val_loss: 18650.7377 - val_mean_absolute_error: 18650.7377\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 17815.28280\n",
      "Epoch 379/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 5133.1681 - mean_absolute_error: 5133.1681 - val_loss: 19155.6098 - val_mean_absolute_error: 19155.6098\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 17815.28280\n",
      "Epoch 380/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 4842.4789 - mean_absolute_error: 4842.4789 - val_loss: 19702.6476 - val_mean_absolute_error: 19702.6476\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 17815.28280\n",
      "Epoch 381/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 5048.4988 - mean_absolute_error: 5048.4988 - val_loss: 18968.8482 - val_mean_absolute_error: 18968.8482\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 17815.28280\n",
      "Epoch 382/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 4872.2912 - mean_absolute_error: 4872.2912 - val_loss: 18920.0106 - val_mean_absolute_error: 18920.0106\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 17815.28280\n",
      "Epoch 383/500\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 5133.3678 - mean_absolute_error: 5133.3678 - val_loss: 18992.3729 - val_mean_absolute_error: 18992.3729\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 17815.28280\n",
      "Epoch 384/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 5083.8821 - mean_absolute_error: 5083.8821 - val_loss: 19208.5998 - val_mean_absolute_error: 19208.5998\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 17815.28280\n",
      "Epoch 385/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 5630.2900 - mean_absolute_error: 5630.2900 - val_loss: 19393.4975 - val_mean_absolute_error: 19393.4975\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 17815.28280\n",
      "Epoch 386/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 5136.4275 - mean_absolute_error: 5136.4275 - val_loss: 18940.4294 - val_mean_absolute_error: 18940.4294\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 17815.28280\n",
      "Epoch 387/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 4991.3156 - mean_absolute_error: 4991.3156 - val_loss: 18637.0993 - val_mean_absolute_error: 18637.0993\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 17815.28280\n",
      "Epoch 388/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 5249.0347 - mean_absolute_error: 5249.0347 - val_loss: 19532.6621 - val_mean_absolute_error: 19532.6621\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 17815.28280\n",
      "Epoch 389/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5143.1345 - mean_absolute_error: 5143.1345 - val_loss: 18859.9187 - val_mean_absolute_error: 18859.9187\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 17815.28280\n",
      "Epoch 390/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 5163.8261 - mean_absolute_error: 5163.8261 - val_loss: 18828.5935 - val_mean_absolute_error: 18828.5935\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 17815.28280\n",
      "Epoch 391/500\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 4848.2695 - mean_absolute_error: 4848.2695 - val_loss: 19171.0902 - val_mean_absolute_error: 19171.0902\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 17815.28280\n",
      "Epoch 392/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 5212.7840 - mean_absolute_error: 5212.7840 - val_loss: 19998.1247 - val_mean_absolute_error: 19998.1247\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 17815.28280\n",
      "Epoch 393/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 5009.5861 - mean_absolute_error: 5009.5861 - val_loss: 18790.4532 - val_mean_absolute_error: 18790.4532\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 17815.28280\n",
      "Epoch 394/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 5884.4971 - mean_absolute_error: 5884.4971 - val_loss: 20226.9277 - val_mean_absolute_error: 20226.9277\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 17815.28280\n",
      "Epoch 395/500\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 6146.8666 - mean_absolute_error: 6146.8666 - val_loss: 19190.3698 - val_mean_absolute_error: 19190.3698\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 17815.28280\n",
      "Epoch 396/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 4882.7719 - mean_absolute_error: 4882.7719 - val_loss: 19529.6069 - val_mean_absolute_error: 19529.6069\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 17815.28280\n",
      "Epoch 397/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 5803.4784 - mean_absolute_error: 5803.4784 - val_loss: 19260.6292 - val_mean_absolute_error: 19260.6292\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 17815.28280\n",
      "Epoch 398/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 5222.1237 - mean_absolute_error: 5222.1237 - val_loss: 18778.6158 - val_mean_absolute_error: 18778.6158\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 17815.28280\n",
      "Epoch 399/500\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 5125.1493 - mean_absolute_error: 5125.1493 - val_loss: 19934.8420 - val_mean_absolute_error: 19934.8420\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 17815.28280\n",
      "Epoch 400/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 4760.1182 - mean_absolute_error: 4760.1182 - val_loss: 19577.2431 - val_mean_absolute_error: 19577.2431\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 17815.28280\n",
      "Epoch 401/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 5140.3218 - mean_absolute_error: 5140.3218 - val_loss: 19235.3809 - val_mean_absolute_error: 19235.3809\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 17815.28280\n",
      "Epoch 402/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 5234.3368 - mean_absolute_error: 5234.3368 - val_loss: 19750.0088 - val_mean_absolute_error: 19750.0088\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 17815.28280\n",
      "Epoch 403/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 5897.0417 - mean_absolute_error: 5897.0417 - val_loss: 18983.7716 - val_mean_absolute_error: 18983.7716\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 17815.28280\n",
      "Epoch 404/500\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 4883.8967 - mean_absolute_error: 4883.8967 - val_loss: 19129.5521 - val_mean_absolute_error: 19129.5521\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 17815.28280\n",
      "Epoch 405/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 5388.6214 - mean_absolute_error: 5388.6214 - val_loss: 19592.9767 - val_mean_absolute_error: 19592.9767\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 17815.28280\n",
      "Epoch 406/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 6137.8298 - mean_absolute_error: 6137.8298 - val_loss: 18987.1891 - val_mean_absolute_error: 18987.1891\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 17815.28280\n",
      "Epoch 407/500\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 4990.5342 - mean_absolute_error: 4990.5342 - val_loss: 19164.5537 - val_mean_absolute_error: 19164.5537\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 17815.28280\n",
      "Epoch 408/500\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 4639.1895 - mean_absolute_error: 4639.1895 - val_loss: 18967.7954 - val_mean_absolute_error: 18967.7954\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 17815.28280\n",
      "Epoch 409/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 4761.0123 - mean_absolute_error: 4761.0123 - val_loss: 18951.5669 - val_mean_absolute_error: 18951.5669\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 17815.28280\n",
      "Epoch 410/500\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 4569.8279 - mean_absolute_error: 4569.8279 - val_loss: 19582.1576 - val_mean_absolute_error: 19582.1576\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 17815.28280\n",
      "Epoch 411/500\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 4379.6453 - mean_absolute_error: 4379.6453 - val_loss: 19283.1498 - val_mean_absolute_error: 19283.1498\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 17815.28280\n",
      "Epoch 412/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 4471.1228 - mean_absolute_error: 4471.1228 - val_loss: 18777.2558 - val_mean_absolute_error: 18777.2558\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 17815.28280\n",
      "Epoch 413/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 5108.5002 - mean_absolute_error: 5108.5002 - val_loss: 19366.6412 - val_mean_absolute_error: 19366.6412\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 17815.28280\n",
      "Epoch 414/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 4936.5596 - mean_absolute_error: 4936.5596 - val_loss: 18845.9302 - val_mean_absolute_error: 18845.9302\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 17815.28280\n",
      "Epoch 415/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 4666.2127 - mean_absolute_error: 4666.2127 - val_loss: 19352.0032 - val_mean_absolute_error: 19352.0032\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 17815.28280\n",
      "Epoch 416/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 4950.5918 - mean_absolute_error: 4950.5918 - val_loss: 19624.3886 - val_mean_absolute_error: 19624.3886\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 17815.28280\n",
      "Epoch 417/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 4930.5583 - mean_absolute_error: 4930.5583 - val_loss: 19551.6121 - val_mean_absolute_error: 19551.6121\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 17815.28280\n",
      "Epoch 418/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 4868.8062 - mean_absolute_error: 4868.8062 - val_loss: 19176.4561 - val_mean_absolute_error: 19176.4561\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 17815.28280\n",
      "Epoch 419/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 4429.5609 - mean_absolute_error: 4429.5609 - val_loss: 19646.0431 - val_mean_absolute_error: 19646.0431\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 17815.28280\n",
      "Epoch 420/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 4458.8582 - mean_absolute_error: 4458.8582 - val_loss: 19693.9495 - val_mean_absolute_error: 19693.9495\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 17815.28280\n",
      "Epoch 421/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 4600.7332 - mean_absolute_error: 4600.7332 - val_loss: 19383.6060 - val_mean_absolute_error: 19383.6060\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 17815.28280\n",
      "Epoch 422/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 4505.6519 - mean_absolute_error: 4505.6519 - val_loss: 19984.8691 - val_mean_absolute_error: 19984.8691\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 17815.28280\n",
      "Epoch 423/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4780.6924 - mean_absolute_error: 4780.6924 - val_loss: 19040.2471 - val_mean_absolute_error: 19040.2471\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 17815.28280\n",
      "Epoch 424/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 4730.5235 - mean_absolute_error: 4730.5235 - val_loss: 19151.8001 - val_mean_absolute_error: 19151.8001\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 17815.28280\n",
      "Epoch 425/500\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 4811.6292 - mean_absolute_error: 4811.6292 - val_loss: 19742.6038 - val_mean_absolute_error: 19742.6038\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 17815.28280\n",
      "Epoch 426/500\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 5232.0063 - mean_absolute_error: 5232.0063 - val_loss: 19953.2414 - val_mean_absolute_error: 19953.2414\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 17815.28280\n",
      "Epoch 427/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 4818.5532 - mean_absolute_error: 4818.5532 - val_loss: 19097.7553 - val_mean_absolute_error: 19097.7553\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 17815.28280\n",
      "Epoch 428/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 4519.8693 - mean_absolute_error: 4519.8693 - val_loss: 20079.0456 - val_mean_absolute_error: 20079.0456\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 17815.28280\n",
      "Epoch 429/500\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 4691.4819 - mean_absolute_error: 4691.4819 - val_loss: 19610.4888 - val_mean_absolute_error: 19610.4888\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 17815.28280\n",
      "Epoch 430/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 4881.5597 - mean_absolute_error: 4881.5597 - val_loss: 21328.6224 - val_mean_absolute_error: 21328.6224\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 17815.28280\n",
      "Epoch 431/500\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 5345.9884 - mean_absolute_error: 5345.9884 - val_loss: 19337.2462 - val_mean_absolute_error: 19337.2462\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 17815.28280\n",
      "Epoch 432/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 4383.5869 - mean_absolute_error: 4383.5869 - val_loss: 19540.2751 - val_mean_absolute_error: 19540.2751\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 17815.28280\n",
      "Epoch 433/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 4471.9952 - mean_absolute_error: 4471.9952 - val_loss: 19198.5292 - val_mean_absolute_error: 19198.5292\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 17815.28280\n",
      "Epoch 434/500\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 4493.0922 - mean_absolute_error: 4493.0922 - val_loss: 19398.9724 - val_mean_absolute_error: 19398.9724\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 17815.28280\n",
      "Epoch 435/500\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 4808.2866 - mean_absolute_error: 4808.2866 - val_loss: 19540.1942 - val_mean_absolute_error: 19540.1942\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 17815.28280\n",
      "Epoch 436/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4413.5362 - mean_absolute_error: 4413.5362 - val_loss: 20229.3794 - val_mean_absolute_error: 20229.3794\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 17815.28280\n",
      "Epoch 437/500\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 4686.6026 - mean_absolute_error: 4686.6026 - val_loss: 19423.0947 - val_mean_absolute_error: 19423.0947\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 17815.28280\n",
      "Epoch 438/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 4704.7872 - mean_absolute_error: 4704.7872 - val_loss: 19147.2827 - val_mean_absolute_error: 19147.2827\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 17815.28280\n",
      "Epoch 439/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 4407.9128 - mean_absolute_error: 4407.9128 - val_loss: 19273.8235 - val_mean_absolute_error: 19273.8235\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 17815.28280\n",
      "Epoch 440/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5334.3648 - mean_absolute_error: 5334.3648 - val_loss: 19872.8381 - val_mean_absolute_error: 19872.8381\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 17815.28280\n",
      "Epoch 441/500\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 4326.5268 - mean_absolute_error: 4326.5268 - val_loss: 19309.5287 - val_mean_absolute_error: 19309.5287\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 17815.28280\n",
      "Epoch 442/500\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 4797.8845 - mean_absolute_error: 4797.8845 - val_loss: 19868.8614 - val_mean_absolute_error: 19868.8614\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 17815.28280\n",
      "Epoch 443/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 4829.2369 - mean_absolute_error: 4829.2369 - val_loss: 19814.3800 - val_mean_absolute_error: 19814.3800\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 17815.28280\n",
      "Epoch 444/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4494.4838 - mean_absolute_error: 4494.4838 - val_loss: 19488.1851 - val_mean_absolute_error: 19488.1851\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 17815.28280\n",
      "Epoch 445/500\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 4323.8219 - mean_absolute_error: 4323.8219 - val_loss: 19477.3489 - val_mean_absolute_error: 19477.3489\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 17815.28280\n",
      "Epoch 446/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 4031.2071 - mean_absolute_error: 4031.2071 - val_loss: 19562.4282 - val_mean_absolute_error: 19562.4282\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 17815.28280\n",
      "Epoch 447/500\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 4297.6163 - mean_absolute_error: 4297.6163 - val_loss: 19668.4679 - val_mean_absolute_error: 19668.4679\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 17815.28280\n",
      "Epoch 448/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 4601.4650 - mean_absolute_error: 4601.4650 - val_loss: 19576.2478 - val_mean_absolute_error: 19576.2478\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 17815.28280\n",
      "Epoch 449/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 4231.8873 - mean_absolute_error: 4231.8873 - val_loss: 19999.7702 - val_mean_absolute_error: 19999.7702\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 17815.28280\n",
      "Epoch 450/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 4196.1835 - mean_absolute_error: 4196.1835 - val_loss: 20037.2259 - val_mean_absolute_error: 20037.2259\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 17815.28280\n",
      "Epoch 451/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4510.0703 - mean_absolute_error: 4510.0703 - val_loss: 19737.8388 - val_mean_absolute_error: 19737.8388\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 17815.28280\n",
      "Epoch 452/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 3909.1932 - mean_absolute_error: 3909.1932 - val_loss: 19589.2503 - val_mean_absolute_error: 19589.2503\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 17815.28280\n",
      "Epoch 453/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 3969.4847 - mean_absolute_error: 3969.4847 - val_loss: 19483.6115 - val_mean_absolute_error: 19483.6115\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 17815.28280\n",
      "Epoch 454/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 4092.6648 - mean_absolute_error: 4092.6648 - val_loss: 19687.2914 - val_mean_absolute_error: 19687.2914\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 17815.28280\n",
      "Epoch 455/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 4475.2860 - mean_absolute_error: 4475.2860 - val_loss: 19746.3069 - val_mean_absolute_error: 19746.3069\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 17815.28280\n",
      "Epoch 456/500\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 4310.9670 - mean_absolute_error: 4310.9670 - val_loss: 19265.4064 - val_mean_absolute_error: 19265.4064\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 17815.28280\n",
      "Epoch 457/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 4492.3599 - mean_absolute_error: 4492.3599 - val_loss: 19848.6628 - val_mean_absolute_error: 19848.6628\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 17815.28280\n",
      "Epoch 458/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 4352.5224 - mean_absolute_error: 4352.5224 - val_loss: 19424.5783 - val_mean_absolute_error: 19424.5783\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 17815.28280\n",
      "Epoch 459/500\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 4307.1815 - mean_absolute_error: 4307.1815 - val_loss: 19308.4450 - val_mean_absolute_error: 19308.4450\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 17815.28280\n",
      "Epoch 460/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 5086.3290 - mean_absolute_error: 5086.3290 - val_loss: 19573.6316 - val_mean_absolute_error: 19573.6316\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 17815.28280\n",
      "Epoch 461/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4330.8348 - mean_absolute_error: 4330.8348 - val_loss: 20150.2243 - val_mean_absolute_error: 20150.2243\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 17815.28280\n",
      "Epoch 462/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 4228.8425 - mean_absolute_error: 4228.8425 - val_loss: 19922.4969 - val_mean_absolute_error: 19922.4969\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 17815.28280\n",
      "Epoch 463/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 4245.5173 - mean_absolute_error: 4245.5173 - val_loss: 20295.5131 - val_mean_absolute_error: 20295.5131\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 17815.28280\n",
      "Epoch 464/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 4203.4912 - mean_absolute_error: 4203.4912 - val_loss: 19557.1473 - val_mean_absolute_error: 19557.1473\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 17815.28280\n",
      "Epoch 465/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 4411.9400 - mean_absolute_error: 4411.9400 - val_loss: 19707.5228 - val_mean_absolute_error: 19707.5228\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 17815.28280\n",
      "Epoch 466/500\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 3985.3406 - mean_absolute_error: 3985.3406 - val_loss: 19529.8102 - val_mean_absolute_error: 19529.8102\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 17815.28280\n",
      "Epoch 467/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 4289.5620 - mean_absolute_error: 4289.5620 - val_loss: 19549.4894 - val_mean_absolute_error: 19549.4894\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 17815.28280\n",
      "Epoch 468/500\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 4365.2162 - mean_absolute_error: 4365.2162 - val_loss: 20367.8315 - val_mean_absolute_error: 20367.8315\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 17815.28280\n",
      "Epoch 469/500\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 3908.7343 - mean_absolute_error: 3908.7343 - val_loss: 19483.9337 - val_mean_absolute_error: 19483.9337\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 17815.28280\n",
      "Epoch 470/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 4048.7297 - mean_absolute_error: 4048.7297 - val_loss: 19282.4425 - val_mean_absolute_error: 19282.4425\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 17815.28280\n",
      "Epoch 471/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 4500.6708 - mean_absolute_error: 4500.6708 - val_loss: 19317.4197 - val_mean_absolute_error: 19317.4197\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 17815.28280\n",
      "Epoch 472/500\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 4777.0316 - mean_absolute_error: 4777.0316 - val_loss: 19105.9521 - val_mean_absolute_error: 19105.9521\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 17815.28280\n",
      "Epoch 473/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 4399.3120 - mean_absolute_error: 4399.3120 - val_loss: 19908.2341 - val_mean_absolute_error: 19908.2341\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 17815.28280\n",
      "Epoch 474/500\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 4072.3509 - mean_absolute_error: 4072.3509 - val_loss: 19956.2894 - val_mean_absolute_error: 19956.2894\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 17815.28280\n",
      "Epoch 475/500\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 4564.5409 - mean_absolute_error: 4564.5409 - val_loss: 19992.4253 - val_mean_absolute_error: 19992.4253\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 17815.28280\n",
      "Epoch 476/500\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 4466.1954 - mean_absolute_error: 4466.1954 - val_loss: 19551.5631 - val_mean_absolute_error: 19551.5631\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 17815.28280\n",
      "Epoch 477/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 4858.8392 - mean_absolute_error: 4858.8392 - val_loss: 20192.0658 - val_mean_absolute_error: 20192.0658\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 17815.28280\n",
      "Epoch 478/500\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 4465.0856 - mean_absolute_error: 4465.0856 - val_loss: 19851.6068 - val_mean_absolute_error: 19851.6068\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 17815.28280\n",
      "Epoch 479/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 5254.4192 - mean_absolute_error: 5254.4192 - val_loss: 21080.5131 - val_mean_absolute_error: 21080.5131\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 17815.28280\n",
      "Epoch 480/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4693.1534 - mean_absolute_error: 4693.1534 - val_loss: 19778.7820 - val_mean_absolute_error: 19778.7820\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 17815.28280\n",
      "Epoch 481/500\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 4081.5826 - mean_absolute_error: 4081.5826 - val_loss: 19847.9411 - val_mean_absolute_error: 19847.9411\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 17815.28280\n",
      "Epoch 482/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 3769.9904 - mean_absolute_error: 3769.9904 - val_loss: 19557.3675 - val_mean_absolute_error: 19557.3675\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 17815.28280\n",
      "Epoch 483/500\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 4124.4424 - mean_absolute_error: 4124.4424 - val_loss: 19582.9851 - val_mean_absolute_error: 19582.9851\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 17815.28280\n",
      "Epoch 484/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 3805.9995 - mean_absolute_error: 3805.9995 - val_loss: 19631.8454 - val_mean_absolute_error: 19631.8454\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 17815.28280\n",
      "Epoch 485/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4135.7952 - mean_absolute_error: 4135.7952 - val_loss: 19702.4354 - val_mean_absolute_error: 19702.4354\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 17815.28280\n",
      "Epoch 486/500\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 4155.6199 - mean_absolute_error: 4155.6199 - val_loss: 20420.5931 - val_mean_absolute_error: 20420.5931\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 17815.28280\n",
      "Epoch 487/500\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 4383.0688 - mean_absolute_error: 4383.0688 - val_loss: 19533.4704 - val_mean_absolute_error: 19533.4704\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 17815.28280\n",
      "Epoch 488/500\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 3716.2580 - mean_absolute_error: 3716.2580 - val_loss: 19652.1663 - val_mean_absolute_error: 19652.1663\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 17815.28280\n",
      "Epoch 489/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 3847.7459 - mean_absolute_error: 3847.7459 - val_loss: 19476.3322 - val_mean_absolute_error: 19476.3322\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 17815.28280\n",
      "Epoch 490/500\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4409.6253 - mean_absolute_error: 4409.6253 - val_loss: 19826.4482 - val_mean_absolute_error: 19826.4482\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 17815.28280\n",
      "Epoch 491/500\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 4481.0991 - mean_absolute_error: 4481.0991 - val_loss: 19845.3032 - val_mean_absolute_error: 19845.3032\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 17815.28280\n",
      "Epoch 492/500\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 4128.6631 - mean_absolute_error: 4128.6631 - val_loss: 19927.4763 - val_mean_absolute_error: 19927.4763\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 17815.28280\n",
      "Epoch 493/500\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 3717.8736 - mean_absolute_error: 3717.8736 - val_loss: 19815.1088 - val_mean_absolute_error: 19815.1088\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 17815.28280\n",
      "Epoch 494/500\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 3921.6820 - mean_absolute_error: 3921.6820 - val_loss: 19615.9992 - val_mean_absolute_error: 19615.9992\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 17815.28280\n",
      "Epoch 495/500\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 3880.9051 - mean_absolute_error: 3880.9051 - val_loss: 19628.1594 - val_mean_absolute_error: 19628.1594\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 17815.28280\n",
      "Epoch 496/500\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 3671.8755 - mean_absolute_error: 3671.8755 - val_loss: 19855.5531 - val_mean_absolute_error: 19855.5531\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 17815.28280\n",
      "Epoch 497/500\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 4020.4574 - mean_absolute_error: 4020.4574 - val_loss: 19716.3488 - val_mean_absolute_error: 19716.3488\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 17815.28280\n",
      "Epoch 498/500\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 4949.5472 - mean_absolute_error: 4949.5472 - val_loss: 19797.3495 - val_mean_absolute_error: 19797.3495\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 17815.28280\n",
      "Epoch 499/500\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 4468.1754 - mean_absolute_error: 4468.1754 - val_loss: 19450.5883 - val_mean_absolute_error: 19450.5883\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 17815.28280\n",
      "Epoch 500/500\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 4566.4293 - mean_absolute_error: 4566.4293 - val_loss: 19603.3562 - val_mean_absolute_error: 19603.3562\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 17815.28280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66a6408780>"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(train_var, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E0FlMZX9vvBK"
   },
   "outputs": [],
   "source": [
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-172--17815.28280.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eq84vR3TwDYF"
   },
   "outputs": [],
   "source": [
    "# Make prediction on our test set\n",
    "predictions = NN_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CesuFN13wU9q",
    "outputId": "3f3c6b30-cc04-46d9-a8b9-d2f56759e631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A submission file has been made\n"
     ]
    }
   ],
   "source": [
    "def make_submission(prediction, sub_name, ID):\n",
    "  my_submission = pd.DataFrame({'Id':ID,'SalePrice':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')\n",
    "\n",
    "make_submission(predictions[:,0],'submission(NN)', test_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "InWbs4rGwsC1",
    "outputId": "a34d7180-35d3-40f0-f94e-7ae058aee61f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1461\n",
       "1       1462\n",
       "2       1463\n",
       "3       1464\n",
       "4       1465\n",
       "        ... \n",
       "1454    2915\n",
       "1455    2916\n",
       "1456    2917\n",
       "1457    2918\n",
       "1458    2919\n",
       "Name: Id, Length: 1459, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ID\n",
    "\n",
    "# This result return us the rank: 2000(37%)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Model-Building-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
