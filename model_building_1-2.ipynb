{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DoMyyZHXMVQ6"
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bl08pfOPMadW"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/train.csv')\n",
    "test = pd.read_csv('/content/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rMR1Rz6SMbFG",
    "outputId": "031612e9-f2ae-43f1-a1c5-0e850325006b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The train data size after dropping Id feature is : (1460, 80) \n",
      "The test data size after dropping Id feature is : (1459, 79) \n"
     ]
    }
   ],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "MmG6SEyTMdgm",
    "outputId": "8225bf6c-3402-4824-be92-4e60d94391a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data size is : (2919, 79)\n",
      "Shape all_data: (2919, 78)\n",
      "\n",
      "Skew in numerical features: \n",
      "\n",
      "There are 59 skewed numerical features to Box Cox transform\n",
      "(2919, 221)\n"
     ]
    }
   ],
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "target = train[\"SalePrice\"]\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))\n",
    "\n",
    "# The Data cleaning process, can be find in the Data Cleaning File\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))       \n",
    "print('Shape all_data: {}'.format(all_data.shape))\n",
    "\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "  all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jNbeXp48M0o2",
    "outputId": "5339f9c7-3719-4bd5-b7c4-cca43df8e13d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 221)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "WxxYXP76M58x",
    "outputId": "dac7e919-1d40-40ef-9bcd-aac89209121d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Build the NN network\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rEyWRUFlOcXc"
   },
   "outputs": [],
   "source": [
    "# set a check point \n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jl9jTWFLOhRy",
    "outputId": "e62dec73-5b58-4763-d35c-6edea841eff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1168/1168 [==============================] - 1s 1ms/step - loss: 2.4506 - mean_absolute_error: 2.4506 - val_loss: 0.5957 - val_mean_absolute_error: 0.5957\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59573, saving model to Weights-001--0.59573.hdf5\n",
      "Epoch 2/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.3681 - mean_absolute_error: 0.3681 - val_loss: 0.3114 - val_mean_absolute_error: 0.3114\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59573 to 0.31143, saving model to Weights-002--0.31143.hdf5\n",
      "Epoch 3/500\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 0.3076 - mean_absolute_error: 0.3076 - val_loss: 0.4799 - val_mean_absolute_error: 0.4799\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31143\n",
      "Epoch 4/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.4194 - mean_absolute_error: 0.4194 - val_loss: 0.3386 - val_mean_absolute_error: 0.3386\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31143\n",
      "Epoch 5/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1798 - mean_absolute_error: 0.1798 - val_loss: 0.1618 - val_mean_absolute_error: 0.1618\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31143 to 0.16180, saving model to Weights-005--0.16180.hdf5\n",
      "Epoch 6/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.3606 - mean_absolute_error: 0.3606 - val_loss: 0.2656 - val_mean_absolute_error: 0.2656\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16180\n",
      "Epoch 7/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.4001 - mean_absolute_error: 0.4001 - val_loss: 0.3399 - val_mean_absolute_error: 0.3399\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16180\n",
      "Epoch 8/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.2141 - mean_absolute_error: 0.2141 - val_loss: 0.2537 - val_mean_absolute_error: 0.2537\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16180\n",
      "Epoch 9/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.2043 - mean_absolute_error: 0.2043 - val_loss: 0.1587 - val_mean_absolute_error: 0.1587\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16180 to 0.15866, saving model to Weights-009--0.15866.hdf5\n",
      "Epoch 10/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1473 - mean_absolute_error: 0.1473 - val_loss: 0.1499 - val_mean_absolute_error: 0.1499\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15866 to 0.14987, saving model to Weights-010--0.14987.hdf5\n",
      "Epoch 11/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1364 - mean_absolute_error: 0.1364 - val_loss: 0.3269 - val_mean_absolute_error: 0.3269\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14987\n",
      "Epoch 12/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.2507 - mean_absolute_error: 0.2507 - val_loss: 0.2676 - val_mean_absolute_error: 0.2676\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14987\n",
      "Epoch 13/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.2700 - mean_absolute_error: 0.2700 - val_loss: 0.1671 - val_mean_absolute_error: 0.1671\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14987\n",
      "Epoch 14/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.3066 - mean_absolute_error: 0.3066 - val_loss: 0.2102 - val_mean_absolute_error: 0.2102\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14987\n",
      "Epoch 15/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.3530 - val_mean_absolute_error: 0.3530\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14987\n",
      "Epoch 16/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.1571 - mean_absolute_error: 0.1571 - val_loss: 0.1450 - val_mean_absolute_error: 0.1450\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.14987 to 0.14496, saving model to Weights-016--0.14496.hdf5\n",
      "Epoch 17/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1468 - mean_absolute_error: 0.1468 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14496\n",
      "Epoch 18/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1859 - mean_absolute_error: 0.1859 - val_loss: 0.1673 - val_mean_absolute_error: 0.1673\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14496\n",
      "Epoch 19/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.2816 - mean_absolute_error: 0.2816 - val_loss: 0.1772 - val_mean_absolute_error: 0.1772\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14496\n",
      "Epoch 20/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1639 - mean_absolute_error: 0.1639 - val_loss: 0.1332 - val_mean_absolute_error: 0.1332\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.14496 to 0.13320, saving model to Weights-020--0.13320.hdf5\n",
      "Epoch 21/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1895 - mean_absolute_error: 0.1895 - val_loss: 0.1530 - val_mean_absolute_error: 0.1530\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13320\n",
      "Epoch 22/500\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 0.1688 - mean_absolute_error: 0.1688 - val_loss: 0.1688 - val_mean_absolute_error: 0.1688\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13320\n",
      "Epoch 23/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1935 - mean_absolute_error: 0.1935 - val_loss: 0.1627 - val_mean_absolute_error: 0.1627\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.13320\n",
      "Epoch 24/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.2176 - val_mean_absolute_error: 0.2176\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.13320\n",
      "Epoch 25/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1756 - mean_absolute_error: 0.1756 - val_loss: 0.2976 - val_mean_absolute_error: 0.2976\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13320\n",
      "Epoch 26/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1773 - mean_absolute_error: 0.1773 - val_loss: 0.3198 - val_mean_absolute_error: 0.3198\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.13320\n",
      "Epoch 27/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.4008 - mean_absolute_error: 0.4008 - val_loss: 0.5207 - val_mean_absolute_error: 0.5207\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.13320\n",
      "Epoch 28/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.2084 - mean_absolute_error: 0.2084 - val_loss: 0.1932 - val_mean_absolute_error: 0.1932\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13320\n",
      "Epoch 29/500\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 0.1994 - mean_absolute_error: 0.1994 - val_loss: 0.1368 - val_mean_absolute_error: 0.1368\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.13320\n",
      "Epoch 30/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1864 - mean_absolute_error: 0.1864 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.13320\n",
      "Epoch 31/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1782 - mean_absolute_error: 0.1782 - val_loss: 0.1304 - val_mean_absolute_error: 0.1304\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.13320 to 0.13044, saving model to Weights-031--0.13044.hdf5\n",
      "Epoch 32/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1806 - mean_absolute_error: 0.1806 - val_loss: 0.1221 - val_mean_absolute_error: 0.1221\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.13044 to 0.12214, saving model to Weights-032--0.12214.hdf5\n",
      "Epoch 33/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.3007 - mean_absolute_error: 0.3007 - val_loss: 0.1653 - val_mean_absolute_error: 0.1653\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.12214\n",
      "Epoch 34/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.3515 - mean_absolute_error: 0.3515 - val_loss: 0.1322 - val_mean_absolute_error: 0.1322\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.12214\n",
      "Epoch 35/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1870 - mean_absolute_error: 0.1870 - val_loss: 0.2950 - val_mean_absolute_error: 0.2950\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.12214\n",
      "Epoch 36/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.2037 - mean_absolute_error: 0.2037 - val_loss: 0.2116 - val_mean_absolute_error: 0.2116\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12214\n",
      "Epoch 37/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1976 - mean_absolute_error: 0.1976 - val_loss: 0.1230 - val_mean_absolute_error: 0.1230\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.12214\n",
      "Epoch 38/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1463 - mean_absolute_error: 0.1463 - val_loss: 0.1489 - val_mean_absolute_error: 0.1489\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.12214\n",
      "Epoch 39/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.3826 - mean_absolute_error: 0.3826 - val_loss: 0.4173 - val_mean_absolute_error: 0.4173\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.12214\n",
      "Epoch 40/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1973 - mean_absolute_error: 0.1973 - val_loss: 0.1514 - val_mean_absolute_error: 0.1514\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.12214\n",
      "Epoch 41/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1034 - mean_absolute_error: 0.1034 - val_loss: 0.1085 - val_mean_absolute_error: 0.1085\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.12214 to 0.10852, saving model to Weights-041--0.10852.hdf5\n",
      "Epoch 42/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1688 - val_mean_absolute_error: 0.1688\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.10852\n",
      "Epoch 43/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1478 - mean_absolute_error: 0.1478 - val_loss: 0.1805 - val_mean_absolute_error: 0.1805\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10852\n",
      "Epoch 44/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1867 - mean_absolute_error: 0.1867 - val_loss: 0.1446 - val_mean_absolute_error: 0.1446\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10852\n",
      "Epoch 45/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.1228 - mean_absolute_error: 0.1228 - val_loss: 0.1340 - val_mean_absolute_error: 0.1340\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.10852\n",
      "Epoch 46/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1100 - mean_absolute_error: 0.1100 - val_loss: 0.1682 - val_mean_absolute_error: 0.1682\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.10852\n",
      "Epoch 47/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1765 - mean_absolute_error: 0.1765 - val_loss: 0.2013 - val_mean_absolute_error: 0.2013\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.10852\n",
      "Epoch 48/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1881 - mean_absolute_error: 0.1881 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.10852\n",
      "Epoch 49/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1637 - mean_absolute_error: 0.1637 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.10852\n",
      "Epoch 50/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1274 - mean_absolute_error: 0.1274 - val_loss: 0.2001 - val_mean_absolute_error: 0.2001\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.10852\n",
      "Epoch 51/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1520 - mean_absolute_error: 0.1520 - val_loss: 0.1154 - val_mean_absolute_error: 0.1154\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.10852\n",
      "Epoch 52/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1650 - mean_absolute_error: 0.1650 - val_loss: 0.3115 - val_mean_absolute_error: 0.3115\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.10852\n",
      "Epoch 53/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1598 - mean_absolute_error: 0.1598 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.10852\n",
      "Epoch 54/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1641 - mean_absolute_error: 0.1641 - val_loss: 0.3059 - val_mean_absolute_error: 0.3059\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.10852\n",
      "Epoch 55/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1728 - mean_absolute_error: 0.1728 - val_loss: 0.2880 - val_mean_absolute_error: 0.2880\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.10852\n",
      "Epoch 56/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.2092 - mean_absolute_error: 0.2092 - val_loss: 0.1162 - val_mean_absolute_error: 0.1162\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.10852\n",
      "Epoch 57/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1551 - mean_absolute_error: 0.1551 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.10852\n",
      "Epoch 58/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1852 - val_mean_absolute_error: 0.1852\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.10852\n",
      "Epoch 59/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1577 - mean_absolute_error: 0.1577 - val_loss: 0.3250 - val_mean_absolute_error: 0.3250\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.10852\n",
      "Epoch 60/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1277 - mean_absolute_error: 0.1277 - val_loss: 0.2544 - val_mean_absolute_error: 0.2544\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.10852\n",
      "Epoch 61/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1686 - mean_absolute_error: 0.1686 - val_loss: 0.2040 - val_mean_absolute_error: 0.2040\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10852\n",
      "Epoch 62/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1192 - mean_absolute_error: 0.1192 - val_loss: 0.1056 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.10852 to 0.10563, saving model to Weights-062--0.10563.hdf5\n",
      "Epoch 63/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.2115 - mean_absolute_error: 0.2115 - val_loss: 0.1220 - val_mean_absolute_error: 0.1220\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10563\n",
      "Epoch 64/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1550 - mean_absolute_error: 0.1550 - val_loss: 0.2581 - val_mean_absolute_error: 0.2581\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10563\n",
      "Epoch 65/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1322 - mean_absolute_error: 0.1322 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10563\n",
      "Epoch 66/500\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 0.1784 - mean_absolute_error: 0.1784 - val_loss: 0.2943 - val_mean_absolute_error: 0.2943\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.10563\n",
      "Epoch 67/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1544 - mean_absolute_error: 0.1544 - val_loss: 0.1395 - val_mean_absolute_error: 0.1395\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10563\n",
      "Epoch 68/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1615 - mean_absolute_error: 0.1615 - val_loss: 0.1518 - val_mean_absolute_error: 0.1518\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10563\n",
      "Epoch 69/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.2300 - mean_absolute_error: 0.2300 - val_loss: 0.1943 - val_mean_absolute_error: 0.1943\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.10563\n",
      "Epoch 70/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1732 - mean_absolute_error: 0.1732 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.10563\n",
      "Epoch 71/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1381 - mean_absolute_error: 0.1381 - val_loss: 0.1195 - val_mean_absolute_error: 0.1195\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10563\n",
      "Epoch 72/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1334 - mean_absolute_error: 0.1334 - val_loss: 0.1477 - val_mean_absolute_error: 0.1477\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10563\n",
      "Epoch 73/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1380 - mean_absolute_error: 0.1380 - val_loss: 0.1332 - val_mean_absolute_error: 0.1332\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10563\n",
      "Epoch 74/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1598 - mean_absolute_error: 0.1598 - val_loss: 0.1073 - val_mean_absolute_error: 0.1073\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10563\n",
      "Epoch 75/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1203 - mean_absolute_error: 0.1203 - val_loss: 0.1872 - val_mean_absolute_error: 0.1872\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10563\n",
      "Epoch 76/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.1871 - val_mean_absolute_error: 0.1871\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10563\n",
      "Epoch 77/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1472 - mean_absolute_error: 0.1472 - val_loss: 0.1165 - val_mean_absolute_error: 0.1165\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10563\n",
      "Epoch 78/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1525 - mean_absolute_error: 0.1525 - val_loss: 0.1974 - val_mean_absolute_error: 0.1974\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10563\n",
      "Epoch 79/500\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 0.1163 - mean_absolute_error: 0.1163 - val_loss: 0.1662 - val_mean_absolute_error: 0.1662\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10563\n",
      "Epoch 80/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1404 - mean_absolute_error: 0.1404 - val_loss: 0.1321 - val_mean_absolute_error: 0.1321\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10563\n",
      "Epoch 81/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1320 - mean_absolute_error: 0.1320 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10563\n",
      "Epoch 82/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1123 - mean_absolute_error: 0.1123 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10563\n",
      "Epoch 83/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1351 - mean_absolute_error: 0.1351 - val_loss: 0.2590 - val_mean_absolute_error: 0.2590\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10563\n",
      "Epoch 84/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1551 - mean_absolute_error: 0.1551 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10563\n",
      "Epoch 85/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1317 - mean_absolute_error: 0.1317 - val_loss: 0.2603 - val_mean_absolute_error: 0.2603\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10563\n",
      "Epoch 86/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1020 - mean_absolute_error: 0.1020 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10563 to 0.10494, saving model to Weights-086--0.10494.hdf5\n",
      "Epoch 87/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.1081 - val_mean_absolute_error: 0.1081\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10494\n",
      "Epoch 88/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1242 - mean_absolute_error: 0.1242 - val_loss: 0.1747 - val_mean_absolute_error: 0.1747\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10494\n",
      "Epoch 89/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.1089 - val_mean_absolute_error: 0.1089\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10494\n",
      "Epoch 90/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.1184 - mean_absolute_error: 0.1184 - val_loss: 0.2174 - val_mean_absolute_error: 0.2174\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10494\n",
      "Epoch 91/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1560 - mean_absolute_error: 0.1560 - val_loss: 0.1677 - val_mean_absolute_error: 0.1677\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10494\n",
      "Epoch 92/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1899 - mean_absolute_error: 0.1899 - val_loss: 0.1620 - val_mean_absolute_error: 0.1620\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10494\n",
      "Epoch 93/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.1740 - val_mean_absolute_error: 0.1740\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.10494\n",
      "Epoch 94/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1429 - mean_absolute_error: 0.1429 - val_loss: 0.1130 - val_mean_absolute_error: 0.1130\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10494\n",
      "Epoch 95/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0998 - mean_absolute_error: 0.0998 - val_loss: 0.1029 - val_mean_absolute_error: 0.1029\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.10494 to 0.10291, saving model to Weights-095--0.10291.hdf5\n",
      "Epoch 96/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.1481 - mean_absolute_error: 0.1481 - val_loss: 0.1642 - val_mean_absolute_error: 0.1642\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10291\n",
      "Epoch 97/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1109 - mean_absolute_error: 0.1109 - val_loss: 0.1491 - val_mean_absolute_error: 0.1491\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10291\n",
      "Epoch 98/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1278 - mean_absolute_error: 0.1278 - val_loss: 0.2289 - val_mean_absolute_error: 0.2289\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10291\n",
      "Epoch 99/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1422 - mean_absolute_error: 0.1422 - val_loss: 0.1155 - val_mean_absolute_error: 0.1155\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.10291\n",
      "Epoch 100/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0831 - mean_absolute_error: 0.0831 - val_loss: 0.1929 - val_mean_absolute_error: 0.1929\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.10291\n",
      "Epoch 101/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1173 - mean_absolute_error: 0.1173 - val_loss: 0.1800 - val_mean_absolute_error: 0.1800\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10291\n",
      "Epoch 102/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1259 - mean_absolute_error: 0.1259 - val_loss: 0.1102 - val_mean_absolute_error: 0.1102\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.10291\n",
      "Epoch 103/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1360 - mean_absolute_error: 0.1360 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.10291\n",
      "Epoch 104/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0976 - mean_absolute_error: 0.0976 - val_loss: 0.2000 - val_mean_absolute_error: 0.2000\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.10291\n",
      "Epoch 105/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1302 - mean_absolute_error: 0.1302 - val_loss: 0.0958 - val_mean_absolute_error: 0.0958\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.10291 to 0.09583, saving model to Weights-105--0.09583.hdf5\n",
      "Epoch 106/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.1193 - mean_absolute_error: 0.1193 - val_loss: 0.1668 - val_mean_absolute_error: 0.1668\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.09583\n",
      "Epoch 107/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1423 - mean_absolute_error: 0.1423 - val_loss: 0.3155 - val_mean_absolute_error: 0.3155\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.09583\n",
      "Epoch 108/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1475 - mean_absolute_error: 0.1475 - val_loss: 0.1040 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.09583\n",
      "Epoch 109/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1033 - mean_absolute_error: 0.1033 - val_loss: 0.1409 - val_mean_absolute_error: 0.1409\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.09583\n",
      "Epoch 110/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1361 - mean_absolute_error: 0.1361 - val_loss: 0.1828 - val_mean_absolute_error: 0.1828\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.09583\n",
      "Epoch 111/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.1016 - mean_absolute_error: 0.1016 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.09583\n",
      "Epoch 112/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1554 - mean_absolute_error: 0.1554 - val_loss: 0.1361 - val_mean_absolute_error: 0.1361\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.09583\n",
      "Epoch 113/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1290 - mean_absolute_error: 0.1290 - val_loss: 0.1232 - val_mean_absolute_error: 0.1232\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.09583\n",
      "Epoch 114/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1088 - mean_absolute_error: 0.1088 - val_loss: 0.1057 - val_mean_absolute_error: 0.1057\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.09583\n",
      "Epoch 115/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1023 - mean_absolute_error: 0.1023 - val_loss: 0.0988 - val_mean_absolute_error: 0.0988\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.09583\n",
      "Epoch 116/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0842 - mean_absolute_error: 0.0842 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.09583\n",
      "Epoch 117/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1232 - mean_absolute_error: 0.1232 - val_loss: 0.1336 - val_mean_absolute_error: 0.1336\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.09583\n",
      "Epoch 118/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.2249 - mean_absolute_error: 0.2249 - val_loss: 0.3946 - val_mean_absolute_error: 0.3946\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.09583\n",
      "Epoch 119/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1807 - mean_absolute_error: 0.1807 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.09583\n",
      "Epoch 120/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.2375 - val_mean_absolute_error: 0.2375\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.09583\n",
      "Epoch 121/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1022 - mean_absolute_error: 0.1022 - val_loss: 0.1101 - val_mean_absolute_error: 0.1101\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.09583\n",
      "Epoch 122/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1511 - mean_absolute_error: 0.1511 - val_loss: 0.1112 - val_mean_absolute_error: 0.1112\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.09583\n",
      "Epoch 123/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1392 - mean_absolute_error: 0.1392 - val_loss: 0.1035 - val_mean_absolute_error: 0.1035\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.09583\n",
      "Epoch 124/500\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 0.1775 - mean_absolute_error: 0.1775 - val_loss: 0.3752 - val_mean_absolute_error: 0.3752\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.09583\n",
      "Epoch 125/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.3074 - mean_absolute_error: 0.3074 - val_loss: 0.1638 - val_mean_absolute_error: 0.1638\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.09583\n",
      "Epoch 126/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1150 - mean_absolute_error: 0.1150 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.09583\n",
      "Epoch 127/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1742 - mean_absolute_error: 0.1742 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.09583\n",
      "Epoch 128/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0960 - mean_absolute_error: 0.0960 - val_loss: 0.1044 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.09583\n",
      "Epoch 129/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.1285 - mean_absolute_error: 0.1285 - val_loss: 0.2186 - val_mean_absolute_error: 0.2186\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.09583\n",
      "Epoch 130/500\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 0.1373 - mean_absolute_error: 0.1373 - val_loss: 0.1003 - val_mean_absolute_error: 0.1003\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.09583\n",
      "Epoch 131/500\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 0.1057 - mean_absolute_error: 0.1057 - val_loss: 0.1066 - val_mean_absolute_error: 0.1066\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.09583\n",
      "Epoch 132/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0975 - mean_absolute_error: 0.0975 - val_loss: 0.1135 - val_mean_absolute_error: 0.1135\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.09583\n",
      "Epoch 133/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1113 - mean_absolute_error: 0.1113 - val_loss: 0.1208 - val_mean_absolute_error: 0.1208\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.09583\n",
      "Epoch 134/500\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 0.1373 - mean_absolute_error: 0.1373 - val_loss: 0.1458 - val_mean_absolute_error: 0.1458\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.09583\n",
      "Epoch 135/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.1274 - mean_absolute_error: 0.1274 - val_loss: 0.1870 - val_mean_absolute_error: 0.1870\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.09583\n",
      "Epoch 136/500\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 0.1156 - mean_absolute_error: 0.1156 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.09583\n",
      "Epoch 137/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1594 - mean_absolute_error: 0.1594 - val_loss: 0.1628 - val_mean_absolute_error: 0.1628\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.09583\n",
      "Epoch 138/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1165 - mean_absolute_error: 0.1165 - val_loss: 0.2776 - val_mean_absolute_error: 0.2776\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.09583\n",
      "Epoch 139/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.1166 - mean_absolute_error: 0.1166 - val_loss: 0.1027 - val_mean_absolute_error: 0.1027\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.09583\n",
      "Epoch 140/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0975 - mean_absolute_error: 0.0975 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.09583\n",
      "Epoch 141/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0942 - mean_absolute_error: 0.0942 - val_loss: 0.3074 - val_mean_absolute_error: 0.3074\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.09583\n",
      "Epoch 142/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.2211 - mean_absolute_error: 0.2211 - val_loss: 0.1320 - val_mean_absolute_error: 0.1320\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.09583\n",
      "Epoch 143/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.1298 - mean_absolute_error: 0.1298 - val_loss: 0.1086 - val_mean_absolute_error: 0.1086\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.09583\n",
      "Epoch 144/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1057 - val_mean_absolute_error: 0.1057\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.09583\n",
      "Epoch 145/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.1087 - mean_absolute_error: 0.1087 - val_loss: 0.1080 - val_mean_absolute_error: 0.1080\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.09583\n",
      "Epoch 146/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0805 - mean_absolute_error: 0.0805 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.09583\n",
      "Epoch 147/500\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.1092 - mean_absolute_error: 0.1092 - val_loss: 0.1154 - val_mean_absolute_error: 0.1154\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.09583\n",
      "Epoch 148/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1784 - val_mean_absolute_error: 0.1784\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.09583\n",
      "Epoch 149/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1131 - mean_absolute_error: 0.1131 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.09583\n",
      "Epoch 150/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.1065 - mean_absolute_error: 0.1065 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.09583\n",
      "Epoch 151/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1825 - mean_absolute_error: 0.1825 - val_loss: 0.1596 - val_mean_absolute_error: 0.1596\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.09583\n",
      "Epoch 152/500\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.1303 - mean_absolute_error: 0.1303 - val_loss: 0.1008 - val_mean_absolute_error: 0.1008\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.09583\n",
      "Epoch 153/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.1450 - val_mean_absolute_error: 0.1450\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.09583\n",
      "Epoch 154/500\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 0.1083 - mean_absolute_error: 0.1083 - val_loss: 0.1646 - val_mean_absolute_error: 0.1646\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.09583\n",
      "Epoch 155/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1089 - mean_absolute_error: 0.1089 - val_loss: 0.1751 - val_mean_absolute_error: 0.1751\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.09583\n",
      "Epoch 156/500\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 0.1195 - mean_absolute_error: 0.1195 - val_loss: 0.1555 - val_mean_absolute_error: 0.1555\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.09583\n",
      "Epoch 157/500\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 0.1109 - mean_absolute_error: 0.1109 - val_loss: 0.1440 - val_mean_absolute_error: 0.1440\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.09583\n",
      "Epoch 158/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1239 - mean_absolute_error: 0.1239 - val_loss: 0.1350 - val_mean_absolute_error: 0.1350\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.09583\n",
      "Epoch 159/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0981 - mean_absolute_error: 0.0981 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.09583\n",
      "Epoch 160/500\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.1162 - mean_absolute_error: 0.1162 - val_loss: 0.1367 - val_mean_absolute_error: 0.1367\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.09583\n",
      "Epoch 161/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.1463 - mean_absolute_error: 0.1463 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.09583\n",
      "Epoch 162/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.1086 - mean_absolute_error: 0.1086 - val_loss: 0.1152 - val_mean_absolute_error: 0.1152\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.09583\n",
      "Epoch 163/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.0989 - mean_absolute_error: 0.0989 - val_loss: 0.2214 - val_mean_absolute_error: 0.2214\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.09583\n",
      "Epoch 164/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1009 - mean_absolute_error: 0.1009 - val_loss: 0.1355 - val_mean_absolute_error: 0.1355\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.09583\n",
      "Epoch 165/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1854 - val_mean_absolute_error: 0.1854\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.09583\n",
      "Epoch 166/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.1251 - mean_absolute_error: 0.1251 - val_loss: 0.1414 - val_mean_absolute_error: 0.1414\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.09583\n",
      "Epoch 167/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.2676 - val_mean_absolute_error: 0.2676\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.09583\n",
      "Epoch 168/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1643 - mean_absolute_error: 0.1643 - val_loss: 0.1611 - val_mean_absolute_error: 0.1611\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.09583\n",
      "Epoch 169/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.0994 - mean_absolute_error: 0.0994 - val_loss: 0.1615 - val_mean_absolute_error: 0.1615\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.09583\n",
      "Epoch 170/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1623 - mean_absolute_error: 0.1623 - val_loss: 0.1694 - val_mean_absolute_error: 0.1694\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.09583\n",
      "Epoch 171/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1112 - mean_absolute_error: 0.1112 - val_loss: 0.1219 - val_mean_absolute_error: 0.1219\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.09583\n",
      "Epoch 172/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1856 - mean_absolute_error: 0.1856 - val_loss: 0.2651 - val_mean_absolute_error: 0.2651\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.09583\n",
      "Epoch 173/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.09583\n",
      "Epoch 174/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1102 - mean_absolute_error: 0.1102 - val_loss: 0.1823 - val_mean_absolute_error: 0.1823\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.09583\n",
      "Epoch 175/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0864 - mean_absolute_error: 0.0864 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.09583\n",
      "Epoch 176/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1242 - mean_absolute_error: 0.1242 - val_loss: 0.3330 - val_mean_absolute_error: 0.3330\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.09583\n",
      "Epoch 177/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.2846 - mean_absolute_error: 0.2846 - val_loss: 0.2912 - val_mean_absolute_error: 0.2912\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.09583\n",
      "Epoch 178/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1301 - mean_absolute_error: 0.1301 - val_loss: 0.1553 - val_mean_absolute_error: 0.1553\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.09583\n",
      "Epoch 179/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1079 - mean_absolute_error: 0.1079 - val_loss: 0.1353 - val_mean_absolute_error: 0.1353\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.09583\n",
      "Epoch 180/500\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 0.0838 - mean_absolute_error: 0.0838 - val_loss: 0.1981 - val_mean_absolute_error: 0.1981\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.09583\n",
      "Epoch 181/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1203 - mean_absolute_error: 0.1203 - val_loss: 0.1576 - val_mean_absolute_error: 0.1576\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.09583\n",
      "Epoch 182/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1158 - mean_absolute_error: 0.1158 - val_loss: 0.1205 - val_mean_absolute_error: 0.1205\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.09583\n",
      "Epoch 183/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1129 - mean_absolute_error: 0.1129 - val_loss: 0.2162 - val_mean_absolute_error: 0.2162\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.09583\n",
      "Epoch 184/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0839 - mean_absolute_error: 0.0839 - val_loss: 0.1025 - val_mean_absolute_error: 0.1025\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.09583\n",
      "Epoch 185/500\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 0.0963 - mean_absolute_error: 0.0963 - val_loss: 0.1796 - val_mean_absolute_error: 0.1796\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.09583\n",
      "Epoch 186/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.0985 - mean_absolute_error: 0.0985 - val_loss: 0.1727 - val_mean_absolute_error: 0.1727\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.09583\n",
      "Epoch 187/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.1724 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.09583\n",
      "Epoch 188/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.2612 - val_mean_absolute_error: 0.2612\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.09583\n",
      "Epoch 189/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1601 - mean_absolute_error: 0.1601 - val_loss: 0.2703 - val_mean_absolute_error: 0.2703\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.09583\n",
      "Epoch 190/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1126 - mean_absolute_error: 0.1126 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.09583\n",
      "Epoch 191/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0787 - mean_absolute_error: 0.0787 - val_loss: 0.1093 - val_mean_absolute_error: 0.1093\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.09583\n",
      "Epoch 192/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.1062 - val_mean_absolute_error: 0.1062\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.09583\n",
      "Epoch 193/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0810 - mean_absolute_error: 0.0810 - val_loss: 0.1372 - val_mean_absolute_error: 0.1372\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.09583\n",
      "Epoch 194/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0911 - mean_absolute_error: 0.0911 - val_loss: 0.1004 - val_mean_absolute_error: 0.1004\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.09583\n",
      "Epoch 195/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1076 - mean_absolute_error: 0.1076 - val_loss: 0.1027 - val_mean_absolute_error: 0.1027\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.09583\n",
      "Epoch 196/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1069 - mean_absolute_error: 0.1069 - val_loss: 0.1581 - val_mean_absolute_error: 0.1581\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.09583\n",
      "Epoch 197/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.1179 - mean_absolute_error: 0.1179 - val_loss: 0.1795 - val_mean_absolute_error: 0.1795\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.09583\n",
      "Epoch 198/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.1296 - mean_absolute_error: 0.1296 - val_loss: 0.1003 - val_mean_absolute_error: 0.1003\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.09583\n",
      "Epoch 199/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.1181 - mean_absolute_error: 0.1181 - val_loss: 0.1571 - val_mean_absolute_error: 0.1571\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.09583\n",
      "Epoch 200/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0825 - mean_absolute_error: 0.0825 - val_loss: 0.1116 - val_mean_absolute_error: 0.1116\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.09583\n",
      "Epoch 201/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.1066 - mean_absolute_error: 0.1066 - val_loss: 0.0966 - val_mean_absolute_error: 0.0966\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.09583\n",
      "Epoch 202/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0979 - mean_absolute_error: 0.0979 - val_loss: 0.1370 - val_mean_absolute_error: 0.1370\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.09583\n",
      "Epoch 203/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.0990 - val_mean_absolute_error: 0.0990\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.09583\n",
      "Epoch 204/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1044 - mean_absolute_error: 0.1044 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.09583\n",
      "Epoch 205/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1029 - mean_absolute_error: 0.1029 - val_loss: 0.1522 - val_mean_absolute_error: 0.1522\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.09583\n",
      "Epoch 206/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1071 - mean_absolute_error: 0.1071 - val_loss: 0.1438 - val_mean_absolute_error: 0.1438\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.09583\n",
      "Epoch 207/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1396 - mean_absolute_error: 0.1396 - val_loss: 0.2569 - val_mean_absolute_error: 0.2569\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.09583\n",
      "Epoch 208/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.2329 - mean_absolute_error: 0.2329 - val_loss: 0.1005 - val_mean_absolute_error: 0.1005\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.09583\n",
      "Epoch 209/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0997 - mean_absolute_error: 0.0997 - val_loss: 0.1068 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.09583\n",
      "Epoch 210/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0995 - mean_absolute_error: 0.0995 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.09583\n",
      "Epoch 211/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0817 - mean_absolute_error: 0.0817 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.09583\n",
      "Epoch 212/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1297 - mean_absolute_error: 0.1297 - val_loss: 0.1278 - val_mean_absolute_error: 0.1278\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.09583\n",
      "Epoch 213/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.1846 - mean_absolute_error: 0.1846 - val_loss: 0.1441 - val_mean_absolute_error: 0.1441\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.09583\n",
      "Epoch 214/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0933 - mean_absolute_error: 0.0933 - val_loss: 0.1007 - val_mean_absolute_error: 0.1007\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.09583\n",
      "Epoch 215/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0994 - mean_absolute_error: 0.0994 - val_loss: 0.1488 - val_mean_absolute_error: 0.1488\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.09583\n",
      "Epoch 216/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0991 - mean_absolute_error: 0.0991 - val_loss: 0.1284 - val_mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.09583\n",
      "Epoch 217/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0936 - mean_absolute_error: 0.0936 - val_loss: 0.1057 - val_mean_absolute_error: 0.1057\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.09583\n",
      "Epoch 218/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1151 - mean_absolute_error: 0.1151 - val_loss: 0.1030 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.09583\n",
      "Epoch 219/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.2711 - val_mean_absolute_error: 0.2711\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.09583\n",
      "Epoch 220/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.2002 - mean_absolute_error: 0.2002 - val_loss: 0.1305 - val_mean_absolute_error: 0.1305\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.09583\n",
      "Epoch 221/500\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 0.1432 - mean_absolute_error: 0.1432 - val_loss: 0.3014 - val_mean_absolute_error: 0.3014\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.09583\n",
      "Epoch 222/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.1372 - mean_absolute_error: 0.1372 - val_loss: 0.0931 - val_mean_absolute_error: 0.0931\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.09583 to 0.09314, saving model to Weights-222--0.09314.hdf5\n",
      "Epoch 223/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0829 - mean_absolute_error: 0.0829 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.09314\n",
      "Epoch 224/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0931 - mean_absolute_error: 0.0931 - val_loss: 0.0962 - val_mean_absolute_error: 0.0962\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.09314\n",
      "Epoch 225/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0839 - mean_absolute_error: 0.0839 - val_loss: 0.1014 - val_mean_absolute_error: 0.1014\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.09314\n",
      "Epoch 226/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0769 - mean_absolute_error: 0.0769 - val_loss: 0.1203 - val_mean_absolute_error: 0.1203\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.09314\n",
      "Epoch 227/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1089 - mean_absolute_error: 0.1089 - val_loss: 0.0968 - val_mean_absolute_error: 0.0968\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.09314\n",
      "Epoch 228/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0830 - mean_absolute_error: 0.0830 - val_loss: 0.1003 - val_mean_absolute_error: 0.1003\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.09314\n",
      "Epoch 229/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0834 - mean_absolute_error: 0.0834 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.09314\n",
      "Epoch 230/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1040 - mean_absolute_error: 0.1040 - val_loss: 0.1213 - val_mean_absolute_error: 0.1213\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.09314\n",
      "Epoch 231/500\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 0.1093 - mean_absolute_error: 0.1093 - val_loss: 0.1776 - val_mean_absolute_error: 0.1776\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.09314\n",
      "Epoch 232/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1198 - mean_absolute_error: 0.1198 - val_loss: 0.1424 - val_mean_absolute_error: 0.1424\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.09314\n",
      "Epoch 233/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0994 - mean_absolute_error: 0.0994 - val_loss: 0.1208 - val_mean_absolute_error: 0.1208\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.09314\n",
      "Epoch 234/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1336 - mean_absolute_error: 0.1336 - val_loss: 0.1530 - val_mean_absolute_error: 0.1530\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.09314\n",
      "Epoch 235/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0901 - mean_absolute_error: 0.0901 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.09314\n",
      "Epoch 236/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0940 - mean_absolute_error: 0.0940 - val_loss: 0.1222 - val_mean_absolute_error: 0.1222\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.09314\n",
      "Epoch 237/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1286 - mean_absolute_error: 0.1286 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.09314\n",
      "Epoch 238/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0940 - mean_absolute_error: 0.0940 - val_loss: 0.0959 - val_mean_absolute_error: 0.0959\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.09314\n",
      "Epoch 239/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0602 - mean_absolute_error: 0.0602 - val_loss: 0.1122 - val_mean_absolute_error: 0.1122\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.09314\n",
      "Epoch 240/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0899 - mean_absolute_error: 0.0899 - val_loss: 0.0971 - val_mean_absolute_error: 0.0971\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.09314\n",
      "Epoch 241/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.09314\n",
      "Epoch 242/500\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 0.1873 - mean_absolute_error: 0.1873 - val_loss: 0.1059 - val_mean_absolute_error: 0.1059\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.09314\n",
      "Epoch 243/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1352 - mean_absolute_error: 0.1352 - val_loss: 0.1441 - val_mean_absolute_error: 0.1441\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.09314\n",
      "Epoch 244/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1346 - mean_absolute_error: 0.1346 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.09314\n",
      "Epoch 245/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0930 - mean_absolute_error: 0.0930 - val_loss: 0.1368 - val_mean_absolute_error: 0.1368\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.09314\n",
      "Epoch 246/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0957 - mean_absolute_error: 0.0957 - val_loss: 0.1340 - val_mean_absolute_error: 0.1340\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.09314\n",
      "Epoch 247/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1750 - mean_absolute_error: 0.1750 - val_loss: 0.0960 - val_mean_absolute_error: 0.0960\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.09314\n",
      "Epoch 248/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0786 - mean_absolute_error: 0.0786 - val_loss: 0.0930 - val_mean_absolute_error: 0.0930\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.09314 to 0.09301, saving model to Weights-248--0.09301.hdf5\n",
      "Epoch 249/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.09301\n",
      "Epoch 250/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1074 - mean_absolute_error: 0.1074 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.09301\n",
      "Epoch 251/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1115 - mean_absolute_error: 0.1115 - val_loss: 0.1441 - val_mean_absolute_error: 0.1441\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.09301\n",
      "Epoch 252/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0945 - mean_absolute_error: 0.0945 - val_loss: 0.1206 - val_mean_absolute_error: 0.1206\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.09301\n",
      "Epoch 253/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1155 - mean_absolute_error: 0.1155 - val_loss: 0.1187 - val_mean_absolute_error: 0.1187\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.09301\n",
      "Epoch 254/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0917 - mean_absolute_error: 0.0917 - val_loss: 0.0979 - val_mean_absolute_error: 0.0979\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.09301\n",
      "Epoch 255/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0673 - mean_absolute_error: 0.0673 - val_loss: 0.1474 - val_mean_absolute_error: 0.1474\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.09301\n",
      "Epoch 256/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0752 - mean_absolute_error: 0.0752 - val_loss: 0.0979 - val_mean_absolute_error: 0.0979\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.09301\n",
      "Epoch 257/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1114 - mean_absolute_error: 0.1114 - val_loss: 0.1731 - val_mean_absolute_error: 0.1731\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.09301\n",
      "Epoch 258/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0986 - mean_absolute_error: 0.0986 - val_loss: 0.1309 - val_mean_absolute_error: 0.1309\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.09301\n",
      "Epoch 259/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0916 - mean_absolute_error: 0.0916 - val_loss: 0.1047 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.09301\n",
      "Epoch 260/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0861 - mean_absolute_error: 0.0861 - val_loss: 0.0975 - val_mean_absolute_error: 0.0975\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.09301\n",
      "Epoch 261/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1139 - mean_absolute_error: 0.1139 - val_loss: 0.1458 - val_mean_absolute_error: 0.1458\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.09301\n",
      "Epoch 262/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0939 - mean_absolute_error: 0.0939 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.09301\n",
      "Epoch 263/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0879 - mean_absolute_error: 0.0879 - val_loss: 0.1416 - val_mean_absolute_error: 0.1416\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.09301\n",
      "Epoch 264/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1111 - mean_absolute_error: 0.1111 - val_loss: 0.1295 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.09301\n",
      "Epoch 265/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0794 - mean_absolute_error: 0.0794 - val_loss: 0.1726 - val_mean_absolute_error: 0.1726\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.09301\n",
      "Epoch 266/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0957 - mean_absolute_error: 0.0957 - val_loss: 0.1102 - val_mean_absolute_error: 0.1102\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.09301\n",
      "Epoch 267/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.1471 - val_mean_absolute_error: 0.1471\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.09301\n",
      "Epoch 268/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0895 - mean_absolute_error: 0.0895 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.09301\n",
      "Epoch 269/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0848 - mean_absolute_error: 0.0848 - val_loss: 0.1244 - val_mean_absolute_error: 0.1244\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.09301\n",
      "Epoch 270/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.1032 - val_mean_absolute_error: 0.1032\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.09301\n",
      "Epoch 271/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0933 - mean_absolute_error: 0.0933 - val_loss: 0.1009 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.09301\n",
      "Epoch 272/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0988 - mean_absolute_error: 0.0988 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.09301\n",
      "Epoch 273/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0715 - mean_absolute_error: 0.0715 - val_loss: 0.1058 - val_mean_absolute_error: 0.1058\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.09301\n",
      "Epoch 274/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0865 - mean_absolute_error: 0.0865 - val_loss: 0.1220 - val_mean_absolute_error: 0.1220\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.09301\n",
      "Epoch 275/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0791 - mean_absolute_error: 0.0791 - val_loss: 0.1185 - val_mean_absolute_error: 0.1185\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.09301\n",
      "Epoch 276/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0759 - mean_absolute_error: 0.0759 - val_loss: 0.0976 - val_mean_absolute_error: 0.0976\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.09301\n",
      "Epoch 277/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0933 - mean_absolute_error: 0.0933 - val_loss: 0.1613 - val_mean_absolute_error: 0.1613\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.09301\n",
      "Epoch 278/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.0913 - mean_absolute_error: 0.0913 - val_loss: 0.1210 - val_mean_absolute_error: 0.1210\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.09301\n",
      "Epoch 279/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1054 - mean_absolute_error: 0.1054 - val_loss: 0.1283 - val_mean_absolute_error: 0.1283\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.09301\n",
      "Epoch 280/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.1139 - val_mean_absolute_error: 0.1139\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.09301\n",
      "Epoch 281/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1298 - mean_absolute_error: 0.1298 - val_loss: 0.1038 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.09301\n",
      "Epoch 282/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1259 - mean_absolute_error: 0.1259 - val_loss: 0.0990 - val_mean_absolute_error: 0.0990\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.09301\n",
      "Epoch 283/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1004 - mean_absolute_error: 0.1004 - val_loss: 0.2166 - val_mean_absolute_error: 0.2166\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.09301\n",
      "Epoch 284/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0772 - mean_absolute_error: 0.0772 - val_loss: 0.1908 - val_mean_absolute_error: 0.1908\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.09301\n",
      "Epoch 285/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1007 - mean_absolute_error: 0.1007 - val_loss: 0.1498 - val_mean_absolute_error: 0.1498\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.09301\n",
      "Epoch 286/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0738 - mean_absolute_error: 0.0738 - val_loss: 0.1021 - val_mean_absolute_error: 0.1021\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.09301\n",
      "Epoch 287/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1109 - mean_absolute_error: 0.1109 - val_loss: 0.1348 - val_mean_absolute_error: 0.1348\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.09301\n",
      "Epoch 288/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1316 - mean_absolute_error: 0.1316 - val_loss: 0.1023 - val_mean_absolute_error: 0.1023\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.09301\n",
      "Epoch 289/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0838 - mean_absolute_error: 0.0838 - val_loss: 0.1004 - val_mean_absolute_error: 0.1004\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.09301\n",
      "Epoch 290/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0787 - mean_absolute_error: 0.0787 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.09301\n",
      "Epoch 291/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0585 - mean_absolute_error: 0.0585 - val_loss: 0.1203 - val_mean_absolute_error: 0.1203\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.09301\n",
      "Epoch 292/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1191 - mean_absolute_error: 0.1191 - val_loss: 0.1235 - val_mean_absolute_error: 0.1235\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.09301\n",
      "Epoch 293/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1002 - mean_absolute_error: 0.1002 - val_loss: 0.1321 - val_mean_absolute_error: 0.1321\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.09301\n",
      "Epoch 294/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1177 - mean_absolute_error: 0.1177 - val_loss: 0.1007 - val_mean_absolute_error: 0.1007\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.09301\n",
      "Epoch 295/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1390 - val_mean_absolute_error: 0.1390\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.09301\n",
      "Epoch 296/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1311 - mean_absolute_error: 0.1311 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.09301\n",
      "Epoch 297/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1471 - mean_absolute_error: 0.1471 - val_loss: 0.1290 - val_mean_absolute_error: 0.1290\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.09301\n",
      "Epoch 298/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0822 - mean_absolute_error: 0.0822 - val_loss: 0.1451 - val_mean_absolute_error: 0.1451\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.09301\n",
      "Epoch 299/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.1679 - mean_absolute_error: 0.1679 - val_loss: 0.1085 - val_mean_absolute_error: 0.1085\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.09301\n",
      "Epoch 300/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0953 - mean_absolute_error: 0.0953 - val_loss: 0.1511 - val_mean_absolute_error: 0.1511\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.09301\n",
      "Epoch 301/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0935 - mean_absolute_error: 0.0935 - val_loss: 0.1206 - val_mean_absolute_error: 0.1206\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.09301\n",
      "Epoch 302/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0827 - mean_absolute_error: 0.0827 - val_loss: 0.0996 - val_mean_absolute_error: 0.0996\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.09301\n",
      "Epoch 303/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0848 - mean_absolute_error: 0.0848 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.09301\n",
      "Epoch 304/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0825 - mean_absolute_error: 0.0825 - val_loss: 0.1501 - val_mean_absolute_error: 0.1501\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.09301\n",
      "Epoch 305/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0927 - mean_absolute_error: 0.0927 - val_loss: 0.1188 - val_mean_absolute_error: 0.1188\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.09301\n",
      "Epoch 306/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1101 - mean_absolute_error: 0.1101 - val_loss: 0.1320 - val_mean_absolute_error: 0.1320\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.09301\n",
      "Epoch 307/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0882 - mean_absolute_error: 0.0882 - val_loss: 0.1996 - val_mean_absolute_error: 0.1996\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.09301\n",
      "Epoch 308/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0665 - mean_absolute_error: 0.0665 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.09301\n",
      "Epoch 309/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0890 - mean_absolute_error: 0.0890 - val_loss: 0.1189 - val_mean_absolute_error: 0.1189\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.09301\n",
      "Epoch 310/500\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 0.0933 - mean_absolute_error: 0.0933 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.09301\n",
      "Epoch 311/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.0783 - mean_absolute_error: 0.0783 - val_loss: 0.1057 - val_mean_absolute_error: 0.1057\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.09301\n",
      "Epoch 312/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1381 - mean_absolute_error: 0.1381 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.09301\n",
      "Epoch 313/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0926 - mean_absolute_error: 0.0926 - val_loss: 0.1141 - val_mean_absolute_error: 0.1141\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.09301\n",
      "Epoch 314/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.1340 - val_mean_absolute_error: 0.1340\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.09301\n",
      "Epoch 315/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0936 - mean_absolute_error: 0.0936 - val_loss: 0.1424 - val_mean_absolute_error: 0.1424\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.09301\n",
      "Epoch 316/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0679 - mean_absolute_error: 0.0679 - val_loss: 0.1610 - val_mean_absolute_error: 0.1610\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.09301\n",
      "Epoch 317/500\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 0.1006 - mean_absolute_error: 0.1006 - val_loss: 0.1598 - val_mean_absolute_error: 0.1598\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.09301\n",
      "Epoch 318/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0950 - mean_absolute_error: 0.0950 - val_loss: 0.1193 - val_mean_absolute_error: 0.1193\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.09301\n",
      "Epoch 319/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0799 - mean_absolute_error: 0.0799 - val_loss: 0.1000 - val_mean_absolute_error: 0.1000\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.09301\n",
      "Epoch 320/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.1166 - val_mean_absolute_error: 0.1166\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.09301\n",
      "Epoch 321/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0868 - mean_absolute_error: 0.0868 - val_loss: 0.1190 - val_mean_absolute_error: 0.1190\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.09301\n",
      "Epoch 322/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0843 - mean_absolute_error: 0.0843 - val_loss: 0.1026 - val_mean_absolute_error: 0.1026\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.09301\n",
      "Epoch 323/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.1862 - val_mean_absolute_error: 0.1862\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.09301\n",
      "Epoch 324/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0848 - mean_absolute_error: 0.0848 - val_loss: 0.1419 - val_mean_absolute_error: 0.1419\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.09301\n",
      "Epoch 325/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0786 - mean_absolute_error: 0.0786 - val_loss: 0.1076 - val_mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.09301\n",
      "Epoch 326/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0549 - mean_absolute_error: 0.0549 - val_loss: 0.1031 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.09301\n",
      "Epoch 327/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0721 - mean_absolute_error: 0.0721 - val_loss: 0.1816 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.09301\n",
      "Epoch 328/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.1123 - mean_absolute_error: 0.1123 - val_loss: 0.1018 - val_mean_absolute_error: 0.1018\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.09301\n",
      "Epoch 329/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0602 - mean_absolute_error: 0.0602 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.09301\n",
      "Epoch 330/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.0780 - mean_absolute_error: 0.0780 - val_loss: 0.1516 - val_mean_absolute_error: 0.1516\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.09301\n",
      "Epoch 331/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0905 - mean_absolute_error: 0.0905 - val_loss: 0.1684 - val_mean_absolute_error: 0.1684\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.09301\n",
      "Epoch 332/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0735 - mean_absolute_error: 0.0735 - val_loss: 0.1043 - val_mean_absolute_error: 0.1043\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.09301\n",
      "Epoch 333/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1513 - mean_absolute_error: 0.1513 - val_loss: 0.1578 - val_mean_absolute_error: 0.1578\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.09301\n",
      "Epoch 334/500\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 0.0929 - mean_absolute_error: 0.0929 - val_loss: 0.1143 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.09301\n",
      "Epoch 335/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0813 - mean_absolute_error: 0.0813 - val_loss: 0.1143 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.09301\n",
      "Epoch 336/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0792 - mean_absolute_error: 0.0792 - val_loss: 0.1337 - val_mean_absolute_error: 0.1337\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.09301\n",
      "Epoch 337/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0958 - mean_absolute_error: 0.0958 - val_loss: 0.1314 - val_mean_absolute_error: 0.1314\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.09301\n",
      "Epoch 338/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0921 - mean_absolute_error: 0.0921 - val_loss: 0.1197 - val_mean_absolute_error: 0.1197\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.09301\n",
      "Epoch 339/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0900 - mean_absolute_error: 0.0900 - val_loss: 0.1385 - val_mean_absolute_error: 0.1385\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.09301\n",
      "Epoch 340/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0955 - mean_absolute_error: 0.0955 - val_loss: 0.1231 - val_mean_absolute_error: 0.1231\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.09301\n",
      "Epoch 341/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1063 - mean_absolute_error: 0.1063 - val_loss: 0.1041 - val_mean_absolute_error: 0.1041\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.09301\n",
      "Epoch 342/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0779 - mean_absolute_error: 0.0779 - val_loss: 0.1539 - val_mean_absolute_error: 0.1539\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.09301\n",
      "Epoch 343/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0653 - mean_absolute_error: 0.0653 - val_loss: 0.1044 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.09301\n",
      "Epoch 344/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.2721 - val_mean_absolute_error: 0.2721\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.09301\n",
      "Epoch 345/500\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 0.0973 - mean_absolute_error: 0.0973 - val_loss: 0.1404 - val_mean_absolute_error: 0.1404\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.09301\n",
      "Epoch 346/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0861 - mean_absolute_error: 0.0861 - val_loss: 0.1211 - val_mean_absolute_error: 0.1211\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.09301\n",
      "Epoch 347/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.1171 - mean_absolute_error: 0.1171 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.09301\n",
      "Epoch 348/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0905 - mean_absolute_error: 0.0905 - val_loss: 0.1019 - val_mean_absolute_error: 0.1019\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.09301\n",
      "Epoch 349/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0561 - mean_absolute_error: 0.0561 - val_loss: 0.0987 - val_mean_absolute_error: 0.0987\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.09301\n",
      "Epoch 350/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0564 - mean_absolute_error: 0.0564 - val_loss: 0.1350 - val_mean_absolute_error: 0.1350\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.09301\n",
      "Epoch 351/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0969 - mean_absolute_error: 0.0969 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.09301\n",
      "Epoch 352/500\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 0.1459 - mean_absolute_error: 0.1459 - val_loss: 0.1095 - val_mean_absolute_error: 0.1095\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.09301\n",
      "Epoch 353/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.1080 - val_mean_absolute_error: 0.1080\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.09301\n",
      "Epoch 354/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0916 - mean_absolute_error: 0.0916 - val_loss: 0.1365 - val_mean_absolute_error: 0.1365\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.09301\n",
      "Epoch 355/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.09301\n",
      "Epoch 356/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0829 - mean_absolute_error: 0.0829 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.09301\n",
      "Epoch 357/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0768 - mean_absolute_error: 0.0768 - val_loss: 0.1754 - val_mean_absolute_error: 0.1754\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.09301\n",
      "Epoch 358/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0960 - mean_absolute_error: 0.0960 - val_loss: 0.1076 - val_mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.09301\n",
      "Epoch 359/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.1339 - val_mean_absolute_error: 0.1339\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.09301\n",
      "Epoch 360/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.0726 - mean_absolute_error: 0.0726 - val_loss: 0.1061 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.09301\n",
      "Epoch 361/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0803 - mean_absolute_error: 0.0803 - val_loss: 0.2323 - val_mean_absolute_error: 0.2323\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.09301\n",
      "Epoch 362/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1708 - mean_absolute_error: 0.1708 - val_loss: 0.1116 - val_mean_absolute_error: 0.1116\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.09301\n",
      "Epoch 363/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0906 - mean_absolute_error: 0.0906 - val_loss: 0.1505 - val_mean_absolute_error: 0.1505\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.09301\n",
      "Epoch 364/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0735 - mean_absolute_error: 0.0735 - val_loss: 0.1329 - val_mean_absolute_error: 0.1329\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.09301\n",
      "Epoch 365/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0497 - mean_absolute_error: 0.0497 - val_loss: 0.1130 - val_mean_absolute_error: 0.1130\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.09301\n",
      "Epoch 366/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0871 - mean_absolute_error: 0.0871 - val_loss: 0.1067 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.09301\n",
      "Epoch 367/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0813 - mean_absolute_error: 0.0813 - val_loss: 0.1346 - val_mean_absolute_error: 0.1346\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.09301\n",
      "Epoch 368/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.2298 - mean_absolute_error: 0.2298 - val_loss: 0.2436 - val_mean_absolute_error: 0.2436\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.09301\n",
      "Epoch 369/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.1755 - mean_absolute_error: 0.1755 - val_loss: 0.1495 - val_mean_absolute_error: 0.1495\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.09301\n",
      "Epoch 370/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1309 - mean_absolute_error: 0.1309 - val_loss: 0.1149 - val_mean_absolute_error: 0.1149\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.09301\n",
      "Epoch 371/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0783 - mean_absolute_error: 0.0783 - val_loss: 0.1154 - val_mean_absolute_error: 0.1154\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.09301\n",
      "Epoch 372/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0700 - mean_absolute_error: 0.0700 - val_loss: 0.1238 - val_mean_absolute_error: 0.1238\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.09301\n",
      "Epoch 373/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.1204 - val_mean_absolute_error: 0.1204\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.09301\n",
      "Epoch 374/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1063 - mean_absolute_error: 0.1063 - val_loss: 0.1024 - val_mean_absolute_error: 0.1024\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.09301\n",
      "Epoch 375/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0723 - mean_absolute_error: 0.0723 - val_loss: 0.0996 - val_mean_absolute_error: 0.0996\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.09301\n",
      "Epoch 376/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0989 - mean_absolute_error: 0.0989 - val_loss: 0.1959 - val_mean_absolute_error: 0.1959\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.09301\n",
      "Epoch 377/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0984 - mean_absolute_error: 0.0984 - val_loss: 0.1813 - val_mean_absolute_error: 0.1813\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.09301\n",
      "Epoch 378/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0955 - mean_absolute_error: 0.0955 - val_loss: 0.1019 - val_mean_absolute_error: 0.1019\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.09301\n",
      "Epoch 379/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0801 - mean_absolute_error: 0.0801 - val_loss: 0.2084 - val_mean_absolute_error: 0.2084\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.09301\n",
      "Epoch 380/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.1458 - mean_absolute_error: 0.1458 - val_loss: 0.1862 - val_mean_absolute_error: 0.1862\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.09301\n",
      "Epoch 381/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0991 - mean_absolute_error: 0.0991 - val_loss: 0.0991 - val_mean_absolute_error: 0.0991\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.09301\n",
      "Epoch 382/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0651 - mean_absolute_error: 0.0651 - val_loss: 0.1110 - val_mean_absolute_error: 0.1110\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.09301\n",
      "Epoch 383/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0830 - mean_absolute_error: 0.0830 - val_loss: 0.1566 - val_mean_absolute_error: 0.1566\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.09301\n",
      "Epoch 384/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.1172 - val_mean_absolute_error: 0.1172\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.09301\n",
      "Epoch 385/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0666 - mean_absolute_error: 0.0666 - val_loss: 0.1127 - val_mean_absolute_error: 0.1127\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.09301\n",
      "Epoch 386/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0869 - mean_absolute_error: 0.0869 - val_loss: 0.1028 - val_mean_absolute_error: 0.1028\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.09301\n",
      "Epoch 387/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0682 - mean_absolute_error: 0.0682 - val_loss: 0.1134 - val_mean_absolute_error: 0.1134\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.09301\n",
      "Epoch 388/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0630 - mean_absolute_error: 0.0630 - val_loss: 0.2011 - val_mean_absolute_error: 0.2011\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.09301\n",
      "Epoch 389/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0903 - mean_absolute_error: 0.0903 - val_loss: 0.3045 - val_mean_absolute_error: 0.3045\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.09301\n",
      "Epoch 390/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.1051 - mean_absolute_error: 0.1051 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.09301\n",
      "Epoch 391/500\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0680 - mean_absolute_error: 0.0680 - val_loss: 0.1067 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.09301\n",
      "Epoch 392/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0714 - mean_absolute_error: 0.0714 - val_loss: 0.1378 - val_mean_absolute_error: 0.1378\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.09301\n",
      "Epoch 393/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.1068 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.09301\n",
      "Epoch 394/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0922 - mean_absolute_error: 0.0922 - val_loss: 0.1414 - val_mean_absolute_error: 0.1414\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.09301\n",
      "Epoch 395/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0821 - mean_absolute_error: 0.0821 - val_loss: 0.1342 - val_mean_absolute_error: 0.1342\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.09301\n",
      "Epoch 396/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0879 - mean_absolute_error: 0.0879 - val_loss: 0.1081 - val_mean_absolute_error: 0.1081\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.09301\n",
      "Epoch 397/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.0667 - mean_absolute_error: 0.0667 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.09301\n",
      "Epoch 398/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0963 - mean_absolute_error: 0.0963 - val_loss: 0.1633 - val_mean_absolute_error: 0.1633\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.09301\n",
      "Epoch 399/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0860 - mean_absolute_error: 0.0860 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.09301\n",
      "Epoch 400/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0624 - mean_absolute_error: 0.0624 - val_loss: 0.1384 - val_mean_absolute_error: 0.1384\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.09301\n",
      "Epoch 401/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1025 - mean_absolute_error: 0.1025 - val_loss: 0.2290 - val_mean_absolute_error: 0.2290\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.09301\n",
      "Epoch 402/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.1141 - mean_absolute_error: 0.1141 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.09301\n",
      "Epoch 403/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0793 - mean_absolute_error: 0.0793 - val_loss: 0.1080 - val_mean_absolute_error: 0.1080\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.09301\n",
      "Epoch 404/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1317 - val_mean_absolute_error: 0.1317\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.09301\n",
      "Epoch 405/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1357 - mean_absolute_error: 0.1357 - val_loss: 0.1519 - val_mean_absolute_error: 0.1519\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.09301\n",
      "Epoch 406/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1182 - mean_absolute_error: 0.1182 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.09301\n",
      "Epoch 407/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0709 - mean_absolute_error: 0.0709 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.09301\n",
      "Epoch 408/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0896 - mean_absolute_error: 0.0896 - val_loss: 0.1171 - val_mean_absolute_error: 0.1171\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.09301\n",
      "Epoch 409/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0773 - mean_absolute_error: 0.0773 - val_loss: 0.1187 - val_mean_absolute_error: 0.1187\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.09301\n",
      "Epoch 410/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0807 - mean_absolute_error: 0.0807 - val_loss: 0.1102 - val_mean_absolute_error: 0.1102\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.09301\n",
      "Epoch 411/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0805 - mean_absolute_error: 0.0805 - val_loss: 0.1071 - val_mean_absolute_error: 0.1071\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.09301\n",
      "Epoch 412/500\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 0.1522 - mean_absolute_error: 0.1522 - val_loss: 0.1143 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.09301\n",
      "Epoch 413/500\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0845 - mean_absolute_error: 0.0845 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.09301\n",
      "Epoch 414/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.1018 - val_mean_absolute_error: 0.1018\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.09301\n",
      "Epoch 415/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0902 - mean_absolute_error: 0.0902 - val_loss: 0.1136 - val_mean_absolute_error: 0.1136\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.09301\n",
      "Epoch 416/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0800 - mean_absolute_error: 0.0800 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.09301\n",
      "Epoch 417/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0665 - mean_absolute_error: 0.0665 - val_loss: 0.1494 - val_mean_absolute_error: 0.1494\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.09301\n",
      "Epoch 418/500\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 0.0889 - mean_absolute_error: 0.0889 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.09301\n",
      "Epoch 419/500\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 0.0744 - mean_absolute_error: 0.0744 - val_loss: 0.1229 - val_mean_absolute_error: 0.1229\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.09301\n",
      "Epoch 420/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0903 - mean_absolute_error: 0.0903 - val_loss: 0.1033 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.09301\n",
      "Epoch 421/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0547 - mean_absolute_error: 0.0547 - val_loss: 0.1168 - val_mean_absolute_error: 0.1168\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.09301\n",
      "Epoch 422/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.1878 - val_mean_absolute_error: 0.1878\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.09301\n",
      "Epoch 423/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0961 - mean_absolute_error: 0.0961 - val_loss: 0.1941 - val_mean_absolute_error: 0.1941\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.09301\n",
      "Epoch 424/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.1378 - mean_absolute_error: 0.1378 - val_loss: 0.1410 - val_mean_absolute_error: 0.1410\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.09301\n",
      "Epoch 425/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0849 - mean_absolute_error: 0.0849 - val_loss: 0.1214 - val_mean_absolute_error: 0.1214\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.09301\n",
      "Epoch 426/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0715 - mean_absolute_error: 0.0715 - val_loss: 0.1091 - val_mean_absolute_error: 0.1091\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.09301\n",
      "Epoch 427/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0845 - mean_absolute_error: 0.0845 - val_loss: 0.1236 - val_mean_absolute_error: 0.1236\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.09301\n",
      "Epoch 428/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0732 - mean_absolute_error: 0.0732 - val_loss: 0.1033 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.09301\n",
      "Epoch 429/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0631 - mean_absolute_error: 0.0631 - val_loss: 0.1117 - val_mean_absolute_error: 0.1117\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.09301\n",
      "Epoch 430/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0617 - mean_absolute_error: 0.0617 - val_loss: 0.1407 - val_mean_absolute_error: 0.1407\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.09301\n",
      "Epoch 431/500\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.0925 - mean_absolute_error: 0.0925 - val_loss: 0.1130 - val_mean_absolute_error: 0.1130\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.09301\n",
      "Epoch 432/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0723 - mean_absolute_error: 0.0723 - val_loss: 0.1067 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.09301\n",
      "Epoch 433/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0880 - mean_absolute_error: 0.0880 - val_loss: 0.1644 - val_mean_absolute_error: 0.1644\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.09301\n",
      "Epoch 434/500\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0865 - mean_absolute_error: 0.0865 - val_loss: 0.1125 - val_mean_absolute_error: 0.1125\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.09301\n",
      "Epoch 435/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0709 - mean_absolute_error: 0.0709 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.09301\n",
      "Epoch 436/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0800 - mean_absolute_error: 0.0800 - val_loss: 0.1099 - val_mean_absolute_error: 0.1099\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.09301\n",
      "Epoch 437/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0636 - mean_absolute_error: 0.0636 - val_loss: 0.1193 - val_mean_absolute_error: 0.1193\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.09301\n",
      "Epoch 438/500\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 0.0877 - mean_absolute_error: 0.0877 - val_loss: 0.1048 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.09301\n",
      "Epoch 439/500\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.1206 - val_mean_absolute_error: 0.1206\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.09301\n",
      "Epoch 440/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0900 - mean_absolute_error: 0.0900 - val_loss: 0.1047 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.09301\n",
      "Epoch 441/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.1044 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.09301\n",
      "Epoch 442/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0765 - mean_absolute_error: 0.0765 - val_loss: 0.1390 - val_mean_absolute_error: 0.1390\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.09301\n",
      "Epoch 443/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0738 - mean_absolute_error: 0.0738 - val_loss: 0.1132 - val_mean_absolute_error: 0.1132\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.09301\n",
      "Epoch 444/500\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0806 - mean_absolute_error: 0.0806 - val_loss: 0.1042 - val_mean_absolute_error: 0.1042\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.09301\n",
      "Epoch 445/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.1079 - mean_absolute_error: 0.1079 - val_loss: 0.1367 - val_mean_absolute_error: 0.1367\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.09301\n",
      "Epoch 446/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.1657 - mean_absolute_error: 0.1657 - val_loss: 0.1823 - val_mean_absolute_error: 0.1823\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.09301\n",
      "Epoch 447/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1020 - mean_absolute_error: 0.1020 - val_loss: 0.1210 - val_mean_absolute_error: 0.1210\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.09301\n",
      "Epoch 448/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1127 - val_mean_absolute_error: 0.1127\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.09301\n",
      "Epoch 449/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0726 - mean_absolute_error: 0.0726 - val_loss: 0.1075 - val_mean_absolute_error: 0.1075\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.09301\n",
      "Epoch 450/500\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 0.0769 - mean_absolute_error: 0.0769 - val_loss: 0.1422 - val_mean_absolute_error: 0.1422\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.09301\n",
      "Epoch 451/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0874 - mean_absolute_error: 0.0874 - val_loss: 0.1021 - val_mean_absolute_error: 0.1021\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.09301\n",
      "Epoch 452/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.1444 - val_mean_absolute_error: 0.1444\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.09301\n",
      "Epoch 453/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.0831 - mean_absolute_error: 0.0831 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.09301\n",
      "Epoch 454/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.1553 - val_mean_absolute_error: 0.1553\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.09301\n",
      "Epoch 455/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0727 - mean_absolute_error: 0.0727 - val_loss: 0.1035 - val_mean_absolute_error: 0.1035\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.09301\n",
      "Epoch 456/500\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.0922 - mean_absolute_error: 0.0922 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.09301\n",
      "Epoch 457/500\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 0.0629 - mean_absolute_error: 0.0629 - val_loss: 0.1530 - val_mean_absolute_error: 0.1530\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.09301\n",
      "Epoch 458/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0914 - mean_absolute_error: 0.0914 - val_loss: 0.1295 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.09301\n",
      "Epoch 459/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0851 - mean_absolute_error: 0.0851 - val_loss: 0.1187 - val_mean_absolute_error: 0.1187\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.09301\n",
      "Epoch 460/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0650 - mean_absolute_error: 0.0650 - val_loss: 0.1676 - val_mean_absolute_error: 0.1676\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.09301\n",
      "Epoch 461/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1052 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.09301\n",
      "Epoch 462/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0798 - mean_absolute_error: 0.0798 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.09301\n",
      "Epoch 463/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0906 - mean_absolute_error: 0.0906 - val_loss: 0.1058 - val_mean_absolute_error: 0.1058\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.09301\n",
      "Epoch 464/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0774 - mean_absolute_error: 0.0774 - val_loss: 0.1076 - val_mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.09301\n",
      "Epoch 465/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0767 - mean_absolute_error: 0.0767 - val_loss: 0.2079 - val_mean_absolute_error: 0.2079\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.09301\n",
      "Epoch 466/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1528 - mean_absolute_error: 0.1528 - val_loss: 0.1999 - val_mean_absolute_error: 0.1999\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.09301\n",
      "Epoch 467/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.09301\n",
      "Epoch 468/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.1143 - mean_absolute_error: 0.1143 - val_loss: 0.1579 - val_mean_absolute_error: 0.1579\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.09301\n",
      "Epoch 469/500\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.1355 - mean_absolute_error: 0.1355 - val_loss: 0.2208 - val_mean_absolute_error: 0.2208\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.09301\n",
      "Epoch 470/500\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.09301\n",
      "Epoch 471/500\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.1241 - val_mean_absolute_error: 0.1241\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.09301\n",
      "Epoch 472/500\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 0.0884 - mean_absolute_error: 0.0884 - val_loss: 0.1052 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.09301\n",
      "Epoch 473/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0722 - mean_absolute_error: 0.0722 - val_loss: 0.1164 - val_mean_absolute_error: 0.1164\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.09301\n",
      "Epoch 474/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0825 - mean_absolute_error: 0.0825 - val_loss: 0.1029 - val_mean_absolute_error: 0.1029\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.09301\n",
      "Epoch 475/500\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.1203 - val_mean_absolute_error: 0.1203\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.09301\n",
      "Epoch 476/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0654 - mean_absolute_error: 0.0654 - val_loss: 0.1050 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.09301\n",
      "Epoch 477/500\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 0.0713 - mean_absolute_error: 0.0713 - val_loss: 0.1121 - val_mean_absolute_error: 0.1121\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.09301\n",
      "Epoch 478/500\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0840 - mean_absolute_error: 0.0840 - val_loss: 0.1401 - val_mean_absolute_error: 0.1401\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.09301\n",
      "Epoch 479/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.1326 - val_mean_absolute_error: 0.1326\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.09301\n",
      "Epoch 480/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0585 - mean_absolute_error: 0.0585 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.09301\n",
      "Epoch 481/500\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 0.0592 - mean_absolute_error: 0.0592 - val_loss: 0.1184 - val_mean_absolute_error: 0.1184\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.09301\n",
      "Epoch 482/500\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.1147 - val_mean_absolute_error: 0.1147\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.09301\n",
      "Epoch 483/500\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0680 - mean_absolute_error: 0.0680 - val_loss: 0.1136 - val_mean_absolute_error: 0.1136\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.09301\n",
      "Epoch 484/500\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.0703 - mean_absolute_error: 0.0703 - val_loss: 0.1235 - val_mean_absolute_error: 0.1235\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.09301\n",
      "Epoch 485/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.0721 - mean_absolute_error: 0.0721 - val_loss: 0.1037 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.09301\n",
      "Epoch 486/500\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0642 - mean_absolute_error: 0.0642 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.09301\n",
      "Epoch 487/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0738 - mean_absolute_error: 0.0738 - val_loss: 0.1351 - val_mean_absolute_error: 0.1351\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.09301\n",
      "Epoch 488/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.0774 - mean_absolute_error: 0.0774 - val_loss: 0.1022 - val_mean_absolute_error: 0.1022\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.09301\n",
      "Epoch 489/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.1132 - val_mean_absolute_error: 0.1132\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.09301\n",
      "Epoch 490/500\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 0.0733 - mean_absolute_error: 0.0733 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.09301\n",
      "Epoch 491/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1638 - mean_absolute_error: 0.1638 - val_loss: 0.1117 - val_mean_absolute_error: 0.1117\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.09301\n",
      "Epoch 492/500\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 0.1400 - mean_absolute_error: 0.1400 - val_loss: 0.1969 - val_mean_absolute_error: 0.1969\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.09301\n",
      "Epoch 493/500\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 0.1095 - mean_absolute_error: 0.1095 - val_loss: 0.1166 - val_mean_absolute_error: 0.1166\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.09301\n",
      "Epoch 494/500\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0764 - mean_absolute_error: 0.0764 - val_loss: 0.2026 - val_mean_absolute_error: 0.2026\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.09301\n",
      "Epoch 495/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.0903 - mean_absolute_error: 0.0903 - val_loss: 0.1104 - val_mean_absolute_error: 0.1104\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.09301\n",
      "Epoch 496/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0787 - mean_absolute_error: 0.0787 - val_loss: 0.1094 - val_mean_absolute_error: 0.1094\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.09301\n",
      "Epoch 497/500\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 0.0563 - mean_absolute_error: 0.0563 - val_loss: 0.1463 - val_mean_absolute_error: 0.1463\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.09301\n",
      "Epoch 498/500\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0935 - mean_absolute_error: 0.0935 - val_loss: 0.1160 - val_mean_absolute_error: 0.1160\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.09301\n",
      "Epoch 499/500\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.2018 - val_mean_absolute_error: 0.2018\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.09301\n",
      "Epoch 500/500\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.09301\n"
     ]
    }
   ],
   "source": [
    "history = NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "TQ9-SUuePKkF",
    "outputId": "a13d5a0d-5932-44f0-efd2-1ebda20cfbe2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCEcTgKBEEI4nAQCIYRwOAkEQgjhcP8Pm7LVnQY42m0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mean_absolute_error = history.history['mean_absolute_error']\n",
    "val_mean_absolute_error = history.history['val_mean_absolute_error']\n",
    "\n",
    "epochs = range(len(mean_absolute_error))\n",
    "\n",
    "plt.plot(epochs, mean_absolute_error, 'bo', label='Training error')\n",
    "plt.plot(epochs, val_mean_absolute_error, 'r', label='Val error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "8CajJHYZwLng",
    "outputId": "1145c71c-e9f2-4598-d716-d0c49f4fca9f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T9MqOUvJSyIVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mean_absolute_error = history.history['loss']\n",
    "val_mean_absolute_error = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(mean_absolute_error))\n",
    "\n",
    "plt.plot(epochs, mean_absolute_error, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_mean_absolute_error, 'r', label='Val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qLaHGR8HQ3n-"
   },
   "outputs": [],
   "source": [
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-248--0.09301.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0O8U-rAXRCHv"
   },
   "outputs": [],
   "source": [
    "# Make prediction on our test set\n",
    "predictions = NN_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mb0I6BJgREqT",
    "outputId": "9031072e-f6e9-4d94-c504-737e0b1c1ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A submission file has been made\n"
     ]
    }
   ],
   "source": [
    "def make_submission(prediction, sub_name, ID):\n",
    "  my_submission = pd.DataFrame({'Id':ID,'SalePrice':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')\n",
    "\n",
    "make_submission(predictions[:,0],'submission(NN)', test_ID)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model-building-1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
